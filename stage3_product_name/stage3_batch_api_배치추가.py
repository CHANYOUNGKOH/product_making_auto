"""
stage3_batch_api.py

Stage 3 Batch API ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (GUI)
- ê¸°ëŠ¥: Batch JSONL ìƒì„± -> ì—…ë¡œë“œ -> ì‹¤í–‰ -> ê²°ê³¼ ë³‘í•© -> [NEW] í†µí•© ë¦¬í¬íŠ¸ & íœ´ì§€í†µ
- [Fix] ë°°ì¹˜ ëª©ë¡ ë° íœ´ì§€í†µì— 'Effort' ì»¬ëŸ¼ ì¶”ê°€
"""

import os
import sys
import json
import threading
import subprocess
import re
from datetime import datetime
from dataclasses import asdict

import pandas as pd
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, Menu
from tkinter.scrolledtext import ScrolledText

from openai import OpenAI

# ========================================================
# ë©”ì¸ ëŸ°ì²˜ ì—°ë™ìš© JobManager & íŒŒì¼ëª… ìœ í‹¸ (Stage3: Text)
# ========================================================
def get_root_filename(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´(_T*_I* ë˜ëŠ” _t*_i*) ë° ê¸°íƒ€ ê¼¬ë¦¬í‘œë¥¼ ë–¼ê³  ì›ë³¸ëª…(Key)ë§Œ ì¶”ì¶œ
    ì˜ˆ: ìƒí’ˆ_T2_I0.xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T3_I1.xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T2_I0(ì—…ì™„).xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T2_I0_T3_I1.xlsx -> ìƒí’ˆ.xlsx (ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ ì œê±°)
    """
    name = os.path.basename(filename)
    base, ext = os.path.splitext(name)

    # 1. ë²„ì „ íŒ¨í„´ (_Tìˆ«ì_Iìˆ«ì ë˜ëŠ” _tìˆ«ì_iìˆ«ì) ë°˜ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    # íŒ¨í„´ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì œê±°
    while True:
        new_base = re.sub(r"_[Tt]\d+_[Ii]\d+", "", base, flags=re.IGNORECASE)
        if new_base == base:
            break
        base = new_base
    
    # 2. ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„), (ì™„ë£Œ) ë“±)
    base = re.sub(r"\([^)]*\)", "", base)
    
    # 3. ê¸°íƒ€ êµ¬í˜• ê¼¬ë¦¬í‘œ ì œê±° (í˜¸í™˜ì„± ìœ ì§€)
    suffixes = ["_stage1_mapping", "_stage1_img_mapping", "_stage2_analysis", "_stage3_done", "_with_images"]
    for s in suffixes:
        base = base.replace(s, "")
    
    # 4. ëì— ë‚¨ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    base = base.rstrip("_")

    return base + ext


def get_next_version_path(current_path: str, task_type: str = "text") -> str:
    """
    í˜„ì¬ íŒŒì¼ëª…ì„ ë¶„ì„í•´ì„œ ë‹¤ìŒ ë‹¨ê³„ì˜ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    íŒŒì¼ëª… í˜•ì‹: ì›ë³¸ëª…_T{ìˆ«ì}_I{ìˆ«ì}.xlsx
    - task_type='text'  â†’ T ë²„ì „ +1 (Stage1: T1, Stage2: T2, Stage3: T3, ...)
    - task_type='image' â†’ I ë²„ì „ +1
    
    ì£¼ì˜: íŒŒì¼ëª…ì— ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ íŒ¨í„´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    dir_name = os.path.dirname(current_path)
    base_name = os.path.basename(current_path)
    name_only, ext = os.path.splitext(base_name)

    # ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„))
    name_only_clean = re.sub(r"\([^)]*\)", "", name_only)
    
    # ë§ˆì§€ë§‰ _T*_I* íŒ¨í„´ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ, ì—¬ëŸ¬ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ ê²ƒë§Œ)
    all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
    
    if all_matches:
        # ë§ˆì§€ë§‰ ë§¤ì¹­ ì‚¬ìš©
        match = all_matches[-1]
        current_t = int(match.group(2))
        current_i = int(match.group(4))
        # ì›ë³¸ëª…ì€ ë§ˆì§€ë§‰ íŒ¨í„´ ì´ì „ê¹Œì§€
        original_name = name_only_clean[: match.start()].rstrip("_")
    else:
        # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì›ë³¸ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì œê±° í›„ ì‚¬ìš©
        original_name = name_only_clean
        # ê¸°ì¡´ ë²„ì „ íŒ¨í„´ ì œê±°
        while True:
            new_name = re.sub(r"_[Tt]\d+_[Ii]\d+", "", original_name, flags=re.IGNORECASE)
            if new_name == original_name:
                break
            original_name = new_name
        original_name = original_name.rstrip("_")
        current_t = 0
        current_i = 0

    if task_type == "text":
        new_t = current_t + 1
        new_i = current_i
    elif task_type == "image":
        new_t = current_t
        new_i = current_i + 1
    else:
        return current_path

    new_filename = f"{original_name}_T{new_t}_I{new_i}{ext}"
    return os.path.join(dir_name, new_filename)


class JobManager:
    DB_FILE = None

    @classmethod
    def find_db_path(cls):
        if cls.DB_FILE and os.path.exists(cls.DB_FILE):
            return cls.DB_FILE

        current_dir = os.path.dirname(os.path.abspath(__file__))
        search_dirs = [
            current_dir,
            os.path.abspath(os.path.join(current_dir, "..")),
            os.path.abspath(os.path.join(current_dir, "..", "..")),
        ]

        for d in search_dirs:
            target = os.path.join(d, "job_history.json")
            if os.path.exists(target):
                cls.DB_FILE = target
                print(f"[JobManager] DB Found: {target}")
                return target

        default_path = os.path.abspath(os.path.join(current_dir, "..", "job_history.json"))
        cls.DB_FILE = default_path
        return default_path

    @classmethod
    def load_jobs(cls):
        db_path = cls.find_db_path()
        if not os.path.exists(db_path):
            return {}
        try:
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    @classmethod
    def update_status(cls, filename, text_msg=None, img_msg=None):
        """ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ìƒíƒœ ì—…ë°ì´íŠ¸ (Stage1/2/3 ê³µìš©)."""
        db_path = cls.find_db_path()
        data = cls.load_jobs()
        now = datetime.now().strftime("%m-%d %H:%M")

        if filename not in data:
            data[filename] = {
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "text_status": "ëŒ€ê¸°",
                "text_time": "-",
                "image_status": "ëŒ€ê¸°",
                "image_time": "-",
                "memo": "",
            }

        if text_msg:
            data[filename]["text_status"] = text_msg
            data[filename]["text_time"] = now
        if img_msg:
            data[filename]["image_status"] = img_msg
            data[filename]["image_time"] = now

        data[filename]["last_update"] = now

        try:
            with open(db_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"[JobManager Error] {e}")


def safe_save_excel(df: pd.DataFrame, path: str) -> bool:
    """ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆì–´ ì €ì¥ì´ ì•ˆ ë  ë•Œ ì¬ì‹œë„ë¥¼ ìœ ë„í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            df.to_excel(path, index=False)
            return True
        except PermissionError:
            if not messagebox.askretrycancel(
                "ì €ì¥ ì‹¤íŒ¨",
                f"ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤!\n[{os.path.basename(path)}]\n\n"
                "íŒŒì¼ì„ ë‹«ê³  'ë‹¤ì‹œ ì‹œë„'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
            ):
                return False
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False


# [í•„ìˆ˜ ì˜ì¡´ì„±] stage3_core.py / stage3_run_history.py
try:
    from stage3_core import (
        safe_str,
        Stage3Settings,
        Stage3Request,
        build_stage3_request_from_row,
        MODEL_PRICING_USD_PER_MTOK,
        load_api_key_from_file,
        save_api_key_to_file,
    )
    from stage3_run_history import append_run_history
    _HISTORY_AVAILABLE = True
except ImportError as e:
    # ì˜ì¡´ì„± íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë”ë¯¸
    _HISTORY_AVAILABLE = False
    MODEL_PRICING_USD_PER_MTOK = {}
    def safe_str(x): return str(x)
    def load_api_key_from_file(x): return ""
    def save_api_key_to_file(x, y): pass
    def append_run_history(*args, **kwargs): 
        # ë”ë¯¸ í•¨ìˆ˜: íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë¬´ì‹œ
        pass

# === ê¸°ë³¸ ì„¤ì • ===
API_KEY_FILE = ".openai_api_key_stage3_batch"
BATCH_JOBS_FILE = os.path.join(os.path.dirname(__file__), "stage3_batch_jobs.json")

# --- UI ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ---
COLOR_BG = "#F8F9FA"
COLOR_WHITE = "#FFFFFF"
COLOR_PRIMARY = "#4A90E2"
COLOR_PRIMARY_HOVER = "#357ABD"
COLOR_SUCCESS = "#28A745"
COLOR_SUCCESS_HOVER = "#218838"
COLOR_DANGER = "#DC3545"
COLOR_DANGER_HOVER = "#C82333"
COLOR_TEXT = "#333333"
COLOR_HEADER = "#E9ECEF"

def get_seoul_now():
    try:
        from pytz import timezone
        return datetime.now(timezone("Asia/Seoul"))
    except:
        return datetime.now()

# ========================================================
# ë°°ì¹˜ ì¡ ê´€ë¦¬ (JSON DB)
# ========================================================
def load_batch_jobs():
    if not os.path.exists(BATCH_JOBS_FILE):
        return []
    try:
        with open(BATCH_JOBS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return []

def save_batch_jobs(jobs):
    try:
        with open(BATCH_JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"[Error] ì¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def upsert_batch_job(batch_id, **kwargs):
    jobs = load_batch_jobs()
    found = False
    now_str = datetime.now().isoformat()
    
    for j in jobs:
        if j["batch_id"] == batch_id:
            if kwargs.get("status") == "completed" and j.get("status") != "completed":
                if "completed_at" not in kwargs:
                    j["completed_at"] = now_str
            j.update(kwargs)
            j["updated_at"] = now_str
            found = True
            break
            
    if not found:
        new_job = {
            "batch_id": batch_id,
            "created_at": now_str,
            "updated_at": now_str,
            "completed_at": "",
            "archived": False,
            **kwargs
        }
        jobs.insert(0, new_job)
    save_batch_jobs(jobs)

def archive_batch_job(batch_ids, archive=True):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    for j in jobs:
        if j["batch_id"] in batch_ids:
            j["archived"] = archive
    save_batch_jobs(jobs)

def hard_delete_batch_job(batch_ids):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    jobs = [j for j in jobs if j["batch_id"] not in batch_ids]
    save_batch_jobs(jobs)

# ========================================================
# GUI Class
# ========================================================
class Stage3BatchGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Stage 3: Batch API Manager (Production Generator)")
        self.geometry("1250x950") # ë„ˆë¹„ ì‚´ì§ ì¦ê°€
        
        self.api_key_var = tk.StringVar()
        
        # íŒŒì¼ ë³€ìˆ˜
        self.src_file_var = tk.StringVar()
        self.skip_exist_var = tk.BooleanVar(value=True)
        
        # [ì¤‘ìš”] Stage 3 ì „ìš© ì„¤ì • ë³€ìˆ˜
        self.model_var = tk.StringVar(value="gpt-5-mini")
        self.effort_var = tk.StringVar(value="medium")
        self.market_var = tk.StringVar(value="ë„¤ì´ë²„ 50ì")
        self.max_len_var = tk.IntVar(value=50)
        self.num_cand_var = tk.IntVar(value=10)
        self.naming_strategy_var = tk.StringVar(value="í†µí•©í˜•")
        
        # íƒ­ 3 ë³€ìˆ˜
        self.batch_id_var = tk.StringVar()
        
        self._configure_styles()
        self._init_ui()
        self._load_key()

    def _configure_styles(self):
        style = ttk.Style()
        try: style.theme_use('clam')
        except: pass
        
        self.configure(background=COLOR_BG)
        
        style.configure("TFrame", background=COLOR_BG)
        style.configure("TLabel", background=COLOR_BG, foreground=COLOR_TEXT, font=("ë§‘ì€ ê³ ë”•", 10))
        style.configure("Header.TLabel", font=("ë§‘ì€ ê³ ë”•", 11, "bold"), foreground="#444")
        
        style.configure("TLabelframe", background=COLOR_BG, bordercolor="#D0D7DE")
        style.configure("TLabelframe.Label", background=COLOR_BG, foreground="#0056b3", font=("ë§‘ì€ ê³ ë”•", 10, "bold"))

        style.configure("TNotebook", background=COLOR_BG, borderwidth=0)
        style.configure("TNotebook.Tab", background="#E1E4E8", padding=[12, 5], font=("ë§‘ì€ ê³ ë”•", 10))
        style.map("TNotebook.Tab", background=[("selected", COLOR_WHITE)], foreground=[("selected", COLOR_PRIMARY)])
        
        style.configure("Treeview", background=COLOR_WHITE, fieldbackground=COLOR_WHITE, font=("ë§‘ì€ ê³ ë”•", 9), rowheight=28)
        style.configure("Treeview.Heading", background=COLOR_HEADER, foreground="#333", font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        style.map("Treeview", background=[('selected', '#CCE5FF')], foreground=[('selected', 'black')])

        style.configure("TButton", font=("ë§‘ì€ ê³ ë”•", 9), padding=5, borderwidth=1)
        style.configure("Primary.TButton", background=COLOR_PRIMARY, foreground="white", bordercolor=COLOR_PRIMARY)
        style.map("Primary.TButton", background=[("active", COLOR_PRIMARY_HOVER)])
        style.configure("Success.TButton", background=COLOR_SUCCESS, foreground="white", bordercolor=COLOR_SUCCESS)
        style.map("Success.TButton", background=[("active", COLOR_SUCCESS_HOVER)])
        style.configure("Danger.TButton", background=COLOR_DANGER, foreground="white", bordercolor=COLOR_DANGER)
        style.map("Danger.TButton", background=[("active", COLOR_DANGER_HOVER)])

    def _init_ui(self):
        main_container = ttk.Frame(self, padding=15)
        main_container.pack(fill='both', expand=True)

        # 1. ìƒë‹¨ API Key
        f_top = ttk.LabelFrame(main_container, text="ğŸ”‘ API ì„¤ì •", padding=10)
        f_top.pack(fill='x', pady=(0, 10))
        ttk.Label(f_top, text="Batch API Key:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(side='left')
        entry_key = ttk.Entry(f_top, textvariable=self.api_key_var, show="*", width=50, font=("Consolas", 10))
        entry_key.pack(side='left', padx=10)
        ttk.Button(f_top, text="ì €ì¥", command=self._save_key, style="Primary.TButton").pack(side='left')

        # 2. ë©”ì¸ íƒ­
        self.main_tabs = ttk.Notebook(main_container)
        self.main_tabs.pack(fill='both', expand=True, pady=5)
        
        self.tab_create = ttk.Frame(self.main_tabs)
        self.tab_manage = ttk.Frame(self.main_tabs) 
        self.tab_merge = ttk.Frame(self.main_tabs)
        
        self.main_tabs.add(self.tab_create, text=" 1. ë°°ì¹˜ ìƒì„± & ì—…ë¡œë“œ ")
        self.main_tabs.add(self.tab_manage, text=" 2. ë°°ì¹˜ ê´€ë¦¬ (ëª©ë¡/ë³‘í•©/ë¦¬í¬íŠ¸) ")
        self.main_tabs.add(self.tab_merge, text=" 3. ê°œë³„ ë³‘í•© (ìˆ˜ë™) ")
        
        self._init_tab_create()
        self._init_tab_manage()
        self._init_tab_merge()
        
        # 3. ë¡œê·¸
        f_log = ttk.LabelFrame(main_container, text="ğŸ“‹ ì‹œìŠ¤í…œ ë¡œê·¸", padding=10)
        f_log.pack(fill='both', expand=True, pady=(10, 0))
        self.log_widget = ScrolledText(f_log, height=15, state='disabled', font=("Consolas", 9), bg="#F1F3F5")
        self.log_widget.pack(fill='both', expand=True)

    def _load_key(self):
        loaded = load_api_key_from_file(API_KEY_FILE)
        if loaded: self.api_key_var.set(loaded)

    def _save_key(self):
        k = self.api_key_var.get().strip()
        if k:
            save_api_key_to_file(k, API_KEY_FILE)
            messagebox.showinfo("ì €ì¥", "API Key ì €ì¥ ì™„ë£Œ")

    def append_log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        self.log_widget.config(state='normal')
        self.log_widget.insert('end', f"[{ts}] {msg}\n")
        self.log_widget.see('end')
        self.log_widget.config(state='disabled')

    # ----------------------------------------------------
    # Tab 1: Create (ìƒì„±)
    # ----------------------------------------------------
    def _init_tab_create(self):
        container = ttk.Frame(self.tab_create, padding=20)
        container.pack(fill='both', expand=True)
        
        # Step 1: íŒŒì¼
        f_file = ttk.LabelFrame(container, text="1. ì‘ì—… ëŒ€ìƒ íŒŒì¼ (ST2_JSON í¬í•¨)", padding=15)
        f_file.pack(fill='x', pady=(0, 15))
        ttk.Entry(f_file, textvariable=self.src_file_var, font=("ë§‘ì€ ê³ ë”•", 10)).pack(side='left', fill='x', expand=True)
        ttk.Button(f_file, text="ğŸ“‚ íŒŒì¼ ì°¾ê¸°", command=self._select_src_file).pack(side='right', padx=5)
        
        # Step 2: Stage 3 ì˜µì…˜
        f_opt = ttk.LabelFrame(container, text="2. Stage 3 ìƒì„± ì˜µì…˜", padding=15)
        f_opt.pack(fill='x', pady=5)

        # ëª¨ë¸ & Effort
        fr1 = ttk.Frame(f_opt)
        fr1.pack(fill='x', pady=5)
        ttk.Label(fr1, text="ëª¨ë¸ (Model):", width=12).pack(side='left')
        models = list(MODEL_PRICING_USD_PER_MTOK.keys()) if MODEL_PRICING_USD_PER_MTOK else ["gpt-5-mini", "gpt-5", "gpt-5-nano"]
        cb_model = ttk.Combobox(fr1, textvariable=self.model_var, values=models, state="readonly", width=20)
        cb_model.pack(side='left', padx=5)
        
        ttk.Label(fr1, text="ì¶”ë¡  ê°•ë„:", width=10).pack(side='left', padx=(20, 5))
        ttk.Combobox(fr1, textvariable=self.effort_var, values=["none", "low", "medium", "high"], state="readonly", width=12).pack(side='left', padx=5)
        
        # ë§ˆì¼“ ì„¤ì •
        fr2 = ttk.Frame(f_opt)
        fr2.pack(fill='x', pady=5)
        ttk.Label(fr2, text="íƒ€ê²Ÿ ë§ˆì¼“:", width=12).pack(side='left')
        markets = ["ë„¤ì´ë²„ 50ì", "ì¿ íŒ¡ 100ì", "ì§€ë§ˆì¼“/ì˜¥ì…˜ 45ì", "ê¸°íƒ€"]
        cb_mk = ttk.Combobox(fr2, textvariable=self.market_var, values=markets, state="readonly", width=18)
        cb_mk.pack(side='left', padx=5)
        cb_mk.bind("<<ComboboxSelected>>", self._on_market_change)
        
        ttk.Label(fr2, text="ìµœëŒ€ ê¸€ì:", width=10).pack(side='left', padx=(20, 5))
        ttk.Spinbox(fr2, from_=10, to=200, textvariable=self.max_len_var, width=10).pack(side='left', padx=5)

        # ì¶œë ¥ ê°œìˆ˜ & ì „ëµ
        fr3 = ttk.Frame(f_opt)
        fr3.pack(fill='x', pady=5)
        ttk.Label(fr3, text="ì¶œë ¥ ê°œìˆ˜:", width=12).pack(side='left')
        ttk.Spinbox(fr3, from_=1, to=30, textvariable=self.num_cand_var, width=10).pack(side='left', padx=5)

        ttk.Label(fr3, text="ëª…ëª… ì „ëµ:", width=10).pack(side='left', padx=(25, 5))
        ttk.Combobox(fr3, textvariable=self.naming_strategy_var, values=["í†µí•©í˜•", "ì˜µì…˜í¬í•¨í˜•"], state="readonly", width=12).pack(side='left', padx=5)
        
        # ì²´í¬ë°•ìŠ¤
        f_row_chk = ttk.Frame(f_opt)
        f_row_chk.pack(fill='x', pady=10)
        ttk.Checkbutton(f_row_chk, text=" ì´ë¯¸ ST3_ê²°ê³¼ê°€ ìˆëŠ” í–‰ ê±´ë„ˆë›°ê¸°", variable=self.skip_exist_var).pack(side='left')
        
        # Step 3: ì‹¤í–‰
        f_step3 = ttk.LabelFrame(container, text="3. ì‹¤í–‰", padding=15)
        f_step3.pack(fill='x', pady=15)

        btn = ttk.Button(f_step3, text="ğŸš€ JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (Start Batch)", command=self._start_create_batch, style="Success.TButton")
        btn.pack(fill='x', ipady=8)
        ttk.Label(container, text="â€» ë°°ì¹˜ APIëŠ” ê²°ê³¼ ìˆ˜ì‹ ê¹Œì§€ ìµœëŒ€ 24ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. (ë¹„ìš© 50% ì ˆê°)", foreground="#666").pack()

    def _on_market_change(self, event=None):
        val = self.market_var.get()
        if "ë„¤ì´ë²„" in val: self.max_len_var.set(50)
        elif "ì¿ íŒ¡" in val: self.max_len_var.set(100)
        elif "ì§€ë§ˆì¼“" in val: self.max_len_var.set(45)

    def _select_src_file(self):
        p = filedialog.askopenfilename(
            title="Stage3 ì—‘ì…€ ì„ íƒ (T2 ë²„ì „ë§Œ ê°€ëŠ¥)",
            filetypes=[("Excel", "*.xlsx;*.xls")]
        )
        if p:
            # T2 í¬í•¨ ì—¬ë¶€ ê²€ì¦
            base_name = os.path.splitext(os.path.basename(p))[0]
            if not re.search(r"_T2_[Ii]\d+", base_name, re.IGNORECASE):
                messagebox.showerror(
                    "ì˜¤ë¥˜", 
                    f"ì´ ë„êµ¬ëŠ” T2 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                    f"ì„ íƒí•œ íŒŒì¼: {os.path.basename(p)}\n"
                    f"íŒŒì¼ëª…ì— '_T2_I*' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
                )
                return
            self.src_file_var.set(p)

    def _start_create_batch(self):
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # T2 í¬í•¨ ì—¬ë¶€ ê²€ì¦
        src = self.src_file_var.get().strip()
        base_name = os.path.splitext(os.path.basename(src))[0]
        if not re.search(r"_T2_[Ii]\d+", base_name, re.IGNORECASE):
            messagebox.showerror(
                "ì˜¤ë¥˜", 
                f"ì´ ë„êµ¬ëŠ” T2 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                f"ì„ íƒí•œ íŒŒì¼: {os.path.basename(src)}\n"
                f"íŒŒì¼ëª…ì— '_T2_I*' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
            )
            return
        
        t = threading.Thread(target=self._run_create_batch)
        t.daemon = True
        t.start()

    def _run_create_batch(self):
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        
        # Stage3SettingsëŠ” model_nameê³¼ reasoning_effortë¥¼ í¬í•¨í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë³„ë„ ê´€ë¦¬
        model_name = self.model_var.get().strip() or "gpt-5-mini"
        reasoning_effort = self.effort_var.get().strip() or "medium"
        
        settings = Stage3Settings(
            market=self.market_var.get(),
            max_len=self.max_len_var.get(),
            num_candidates=self.num_cand_var.get(),
            naming_strategy=self.naming_strategy_var.get()
        )
        
        try:
            client = OpenAI(api_key=key)
            self.append_log(f"ì—‘ì…€ ë¡œë“œ ì¤‘... {os.path.basename(src)}")
            df = pd.read_excel(src)
            
            if "ST2_JSON" not in df.columns:
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼(ST2_JSON)ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. Stage 2ë¥¼ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.")
            
            self.append_log(f"ì„¤ì •: {settings.market} / {settings.max_len}ì / {model_name}")

            jsonl_lines = []
            skipped_cnt = 0
            
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get() and "ST3_ê²°ê³¼ìƒí’ˆëª…" in df.columns:
                    val = str(row.get("ST3_ê²°ê³¼ìƒí’ˆëª…", "")).strip()
                    if val and val != "nan":
                        continue
                
                try:
                    req = build_stage3_request_from_row(row, settings)
                    prompt = req.prompt
                except Exception:
                    skipped_cnt += 1
                    continue

                body = {
                    "model": model_name,
                    "messages": [{"role": "user", "content": prompt}],
                }
                
                is_reasoning = any(x in model_name for x in ["gpt-5", "o1", "o3"])
                if is_reasoning and reasoning_effort != "none":
                    body["reasoning_effort"] = reasoning_effort
                elif not is_reasoning:
                    body["temperature"] = 0.7

                request_obj = {
                    "custom_id": f"row_{idx}",
                    "method": "POST",
                    "url": "/v1/chat/completions",
                    "body": body
                }
                
                jsonl_lines.append(json.dumps(request_obj, ensure_ascii=False))
            
            if not jsonl_lines:
                self.append_log("ìƒì„±í•  ìš”ì²­ ì—†ìŒ.")
                return

            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_stage3_batch_input.jsonl"
            with open(jsonl_path, "w", encoding="utf-8") as f:
                f.write("\n".join(jsonl_lines))
            
            self.append_log(f"JSONL ìƒì„± ì™„ë£Œ: {len(jsonl_lines)}ê±´ (ìŠ¤í‚µ {skipped_cnt}ê±´)")
            
            # íŒŒì¼ í¬ê¸° ë° ìš”ì²­ ìˆ˜ í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            info = {
                'num_requests': len(jsonl_lines),
                'file_size_mb': jsonl_size_mb
            }
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ìš”ì²­ ìˆ˜: {info['num_requests']}ê°œ")
            
            # ì²­í¬ ë¶„í•  ê¸°ì¤€ (OpenAI Batch API ì œí•œ: 200MB)
            MAX_FILE_SIZE_MB = 190
            MAX_REQUESTS_PER_BATCH = 500
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB or info['num_requests'] > MAX_REQUESTS_PER_BATCH:
                reason = []
                if jsonl_size_mb > MAX_FILE_SIZE_MB:
                    reason.append(f"íŒŒì¼ í¬ê¸° ({jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB)")
                if info['num_requests'] > MAX_REQUESTS_PER_BATCH:
                    reason.append(f"ìš”ì²­ ìˆ˜ ({info['num_requests']}ê°œ > {MAX_REQUESTS_PER_BATCH}ê°œ)")
                self.append_log(f"[INFO] {' ë° '.join(reason)}ë¡œ ì¸í•´ ë¶„í•  ì²˜ë¦¬í•©ë‹ˆë‹¤... (OpenAI ì œí•œ: 200MB)")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client_with_timeout = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch_ids = self._create_batch_chunks(
                    client=client_with_timeout,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    effort=reasoning_effort,
                    settings=settings,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=MAX_REQUESTS_PER_BATCH,
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}")
            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    reasoning_effort=reasoning_effort,
                    settings=settings,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # 3) ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src,
                    jsonl_path=jsonl_path,
                    model=model_name,
                    effort=reasoning_effort,
                    status=batch.status,
                    output_file_id=None,
                    market=settings.market,
                    strategy=settings.naming_strategy
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage3(Text) ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡: T3 (ì§„í–‰ì¤‘) (img ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                try:
                    root_name = get_root_filename(src)
                    JobManager.update_status(root_name, text_msg="T3 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T3 (ì§„í–‰ì¤‘)")
                except Exception:
                    # ëŸ°ì²˜ë‚˜ job_history.json ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ìš©íˆ ë¬´ì‹œ
                    pass
                messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {batch_id}")
            
            self._load_jobs_all()
            self._load_archive_list()

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            messagebox.showerror("ì—ëŸ¬", str(e))
    
    def _create_batch_from_jsonl(self, client, jsonl_path, excel_path, model_name, reasoning_effort, settings):
        """JSONL íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë°°ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        if not os.path.exists(jsonl_path):
            raise FileNotFoundError(f"ì…ë ¥ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")

        with open(jsonl_path, "rb") as f:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
            up_file = client.files.create(file=f, purpose="batch", timeout=600)

        batch = client.batches.create(
            input_file_id=up_file.id,
            endpoint="/v1/chat/completions",
            completion_window="24h"
        )
        return batch
    
    def _create_batch_chunks(self, client, jsonl_path, excel_path, model_name, effort, settings, max_size_mb=190, max_requests=500):
        """
        í° JSONL íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ batch_group_idë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        """
        import uuid
        
        # ë°°ì¹˜ ê·¸ë£¹ ID ìƒì„± (ê°™ì€ ì—‘ì…€ì—ì„œ ë¶„í• ëœ ë°°ì¹˜ë“¤ì„ ë¬¶ìŒ)
        batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
        
        # JSONL íŒŒì¼ ì½ê¸°
        requests = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    requests.append(json.loads(line))
        
        total_requests = len(requests)
        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (íŒŒì¼ í¬ê¸°ì™€ ìš”ì²­ ìˆ˜ ëª¨ë‘ ê³ ë ¤)
        original_file_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
        estimated_chunks_by_size = max(1, int(original_file_size_mb / max_size_mb) + 1)
        estimated_chunks_by_count = (total_requests + max_requests - 1) // max_requests
        estimated_total_chunks = max(estimated_chunks_by_size, estimated_chunks_by_count)
        
        self.append_log(f"[INFO] ì´ {total_requests}ê°œ ìš”ì²­ì„ ë¶„í• í•©ë‹ˆë‹¤... (ì˜ˆìƒ: ì•½ {estimated_total_chunks}ê°œ ì²­í¬, ê·¸ë£¹ ID: {batch_group_id})")
        
        batch_ids = []
        chunk_num = 0
        chunk_files_created = []  # ìƒì„±ëœ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì •ë¦¬ìš©)
        
        i = 0
        while i < total_requests:
            chunk_num += 1
            chunk_requests = []
            chunk_size_bytes = 0  # ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì •í™•íˆ ê³„ì‚°
            
            # ì²­í¬ ìƒì„± (í¬ê¸° ë˜ëŠ” ê°œìˆ˜ ì œí•œ)
            while i < total_requests and len(chunk_requests) < max_requests:
                req_json = json.dumps(requests[i], ensure_ascii=False)
                req_size_bytes = len(req_json.encode('utf-8')) + 1  # +1 for newline
                
                # ë‹¤ìŒ ìš”ì²­ì„ ì¶”ê°€í•˜ë©´ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ ë§ˆì§„ 5% í¬í•¨)
                if chunk_size_bytes + req_size_bytes > max_size_mb * 1024 * 1024 * 0.95:
                    break
                
                chunk_requests.append(requests[i])
                chunk_size_bytes += req_size_bytes
                i += 1
            
            if not chunk_requests:
                break
            
            # ì²­í¬ JSONL íŒŒì¼ ìƒì„±
            base, ext = os.path.splitext(jsonl_path)
            chunk_jsonl_path = f"{base}_chunk{chunk_num:03d}{ext}"
            chunk_files_created.append(chunk_jsonl_path)
            
            with open(chunk_jsonl_path, "w", encoding="utf-8") as f:
                for req in chunk_requests:
                    f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            chunk_size_mb = os.path.getsize(chunk_jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] ì²­í¬ {chunk_num}: {len(chunk_requests)}ê°œ ìš”ì²­, {chunk_size_mb:.2f} MB")
            
            # ë°°ì¹˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 3
            retry_count = 0
            batch_created = False
            
            while retry_count < max_retries and not batch_created:
                try:
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹œë„ ì¤‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                    batch = self._create_batch_from_jsonl(
                        client=client,
                        jsonl_path=chunk_jsonl_path,
                        excel_path=excel_path,
                        model_name=model_name,
                        reasoning_effort=effort,
                        settings=settings,
                    )
                    
                    batch_id = batch.id
                    batch_ids.append(batch_id)
                    self.append_log(f"âœ… ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_id}")
                    
                    batch_created = True
                    
                    # ì‘ì—… ì´ë ¥ ê¸°ë¡ (ê·¸ë£¹ ì •ë³´ í¬í•¨)
                    upsert_batch_job(
                        batch_id=batch_id,
                        src_excel=excel_path,
                        jsonl_path=chunk_jsonl_path,
                        model=model_name,
                        effort=effort,
                        status=batch.status,
                        output_file_id=None,
                        batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                        chunk_index=chunk_num,  # ì²­í¬ ë²ˆí˜¸
                        total_chunks=chunk_num,  # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì²­í¬ ìˆ˜ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨)
                        market=settings.market,
                        strategy=settings.naming_strategy
                    )
                except Exception as e:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                        self.append_log(f"âš ï¸ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                        self.append_log(f"[INFO] {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        import time
                        time.sleep(wait_time)
                    else:
                        self.append_log(f"âŒ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}")
                        import traceback
                        self.append_log(traceback.format_exc())
        
        # ëª¨ë“  ì²­í¬ ìƒì„± ì™„ë£Œ í›„, total_chunksë¥¼ ì‹¤ì œ ìƒì„±ëœ ë°°ì¹˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        actual_total_chunks = len(batch_ids)
        if actual_total_chunks > 0:
            self.append_log(f"[INFO] ì´ {actual_total_chunks}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ. ì‘ì—… ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
            jobs = load_batch_jobs()
            updated_count = 0
            for j in jobs:
                if j.get("batch_group_id") == batch_group_id:
                    j["total_chunks"] = actual_total_chunks
                    updated_count += 1
            if updated_count > 0:
                save_batch_jobs(jobs)
                self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {actual_total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.append_log(f"âš ï¸ ìƒì„±ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage3(Text) ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡: T3 (ì§„í–‰ì¤‘)
        try:
            root_name = get_root_filename(excel_path)
            JobManager.update_status(root_name, text_msg="T3 (ì§„í–‰ì¤‘)")
            self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T3 (ì§„í–‰ì¤‘)")
        except Exception:
            pass
        
        return batch_ids

    # ----------------------------------------------------
    # Tab 2: Manage (List & Trash)
    # ----------------------------------------------------
    def _init_tab_manage(self):
        container = ttk.Frame(self.tab_manage, padding=10)
        container.pack(fill='both', expand=True)

        sub_tabs = ttk.Notebook(container)
        sub_tabs.pack(fill='both', expand=True)
        
        self.sub_active = ttk.Frame(sub_tabs, padding=10)
        self.sub_archive = ttk.Frame(sub_tabs, padding=10)
        
        sub_tabs.add(self.sub_active, text=" â–¶ ì§„í–‰ì¤‘ / ì™„ë£Œ (Active) ")
        sub_tabs.add(self.sub_archive, text=" ğŸ—‘ íœ´ì§€í†µ (Archive) ")
        
        # --- Active Tab UI ---
        f_ctrl = ttk.Frame(self.sub_active)
        f_ctrl.pack(fill='x', pady=(0, 10))
        
        ttk.Button(f_ctrl, text="ğŸ”„ ì„ íƒ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ“¥ ì„ íƒ ì¼ê´„ ë³‘í•©", command=self._merge_selected, style="Primary.TButton").pack(side='left', padx=10)
        ttk.Button(f_ctrl, text="ğŸ“Š ì„ íƒ ì¼ê´„ í†µí•© ë¦¬í¬íŠ¸", command=self._report_selected_unified, style="Success.TButton").pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ—‘ íœ´ì§€í†µ ì´ë™", command=self._archive_selected, style="Danger.TButton").pack(side='right', padx=2)
        
        # [NEW] Effort ì»¬ëŸ¼ ë° ê·¸ë£¹ ì»¬ëŸ¼ ì¶”ê°€
        cols = ("batch_id", "status", "created", "completed", "model", "effort", "market", "counts", "group")
        self.tree_active = ttk.Treeview(self.sub_active, columns=cols, show='headings', height=18, selectmode='extended')
        
        self.tree_active.tag_configure('odd', background=COLOR_WHITE)
        self.tree_active.tag_configure('even', background='#F2F7FF')
        self.tree_active.tag_configure('group', background='#E8F4FD')
        
        self.tree_active.heading("batch_id", text="Batch ID")
        self.tree_active.heading("status", text="ìƒíƒœ")
        self.tree_active.heading("created", text="ìƒì„±ì¼ì‹œ")
        self.tree_active.heading("completed", text="ì™„ë£Œì¼ì‹œ")
        self.tree_active.heading("model", text="ëª¨ë¸")
        self.tree_active.heading("effort", text="Effort")
        self.tree_active.heading("market", text="ë§ˆì¼“")
        self.tree_active.heading("counts", text="ì™„ë£Œ/ì „ì²´")
        self.tree_active.heading("group", text="ê·¸ë£¹")
        
        self.tree_active.column("batch_id", width=180)
        self.tree_active.column("status", width=80, anchor="center")
        self.tree_active.column("created", width=120, anchor="center")
        self.tree_active.column("completed", width=120, anchor="center")
        self.tree_active.column("model", width=80, anchor="center")
        self.tree_active.column("effort", width=60, anchor="center")
        self.tree_active.column("market", width=80, anchor="center")
        self.tree_active.column("counts", width=80, anchor="center")
        self.tree_active.column("group", width=80, anchor="center")
        
        self.tree_active.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ìš°í´ë¦­ ë©”ë‰´
        self.menu_active = Menu(self, tearoff=0)
        self.menu_active.add_command(label="ìƒíƒœ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ê²°ê³¼ ë³‘í•©", command=self._merge_selected)
        self.menu_active.add_command(label="í†µí•© ë¦¬í¬íŠ¸ ìƒì„±", command=self._report_selected_unified)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="íœ´ì§€í†µìœ¼ë¡œ ì´ë™", command=self._archive_selected)
        
        self.tree_active.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_active, self.menu_active))
        self.tree_active.bind("<Double-1>", self._on_tree_double_click)

        # --- Archive Tab UI ---
        f_arch_ctrl = ttk.Frame(self.sub_archive)
        f_arch_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_arch_ctrl, text="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected, style="Primary.TButton").pack(side='left')
        ttk.Button(f_arch_ctrl, text="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected, style="Danger.TButton").pack(side='right')
        
        self.tree_arch = ttk.Treeview(self.sub_archive, columns=cols, show='headings', height=18, selectmode='extended')
        self.tree_arch.tag_configure('odd', background=COLOR_WHITE)
        self.tree_arch.tag_configure('even', background='#FFF2F2') 

        for col in cols: 
            self.tree_arch.heading(col, text=col.capitalize())
            self.tree_arch.column(col, anchor="center")
        self.tree_arch.column("batch_id", width=200, anchor="w")
        
        self.tree_arch.pack(fill='both', expand=True)
        
        self._load_jobs_all()
        self._load_archive_list()

    def _show_context_menu(self, event, tree, menu):
        item = tree.identify_row(event.y)
        if item:
            if item not in tree.selection():
                tree.selection_set(item)
            menu.post(event.x_root, event.y_root)

    def _get_selected_ids(self, tree):
        selection = tree.selection()
        ids = []
        for item in selection:
            vals = tree.item(item)['values']
            if vals: ids.append(vals[0])
        return ids

    def _load_jobs_all(self):
        if not hasattr(self, 'tree_active'): return
        for i in self.tree_active.get_children(): self.tree_active.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ì„ ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ê·¸ë£¹ {chunk_info}"
                tag = 'group' if idx % 2 == 0 else 'group'
                self.tree_active.insert("", "end", values=(
                    j["batch_id"], j.get("status"), 
                    c_at, f_at, 
                    j.get("model"), j.get("effort", "-"), j.get("market", "-"), cnt, group_display
                ), tags=(tag,))
                idx += 1
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_active.insert("", "end", values=(
                j["batch_id"], j.get("status"), 
                c_at, f_at, 
                j.get("model"), j.get("effort", "-"), j.get("market", "-"), cnt, "-"
            ), tags=(tag,))
            idx += 1

    def _load_archive_list(self):
        if not hasattr(self, 'tree_arch'): return
        for i in self.tree_arch.get_children(): self.tree_arch.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if not j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ê·¸ë£¹ {chunk_info}"
                tag = 'group' if idx % 2 == 0 else 'group'
                self.tree_arch.insert("", "end", values=(
                    j["batch_id"], j.get("status"), 
                    c_at, f_at, 
                    j.get("model"), j.get("effort", "-"), j.get("market", "-"), cnt, group_display
                ), tags=(tag,))
                idx += 1
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_arch.insert("", "end", values=(
                j["batch_id"], j.get("status"), 
                c_at, f_at, 
                j.get("model"), j.get("effort", "-"), j.get("market", "-"), cnt, "-"
            ), tags=(tag,))
            idx += 1

    # --- Batch Actions ---
    def _refresh_selected(self, tree):
        ids = self._get_selected_ids(tree)
        if not ids: return
        
        # API í‚¤ í™•ì¸
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.\nìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        jobs = load_batch_jobs()
        completed = [bid for bid in ids if next((x for x in jobs if x["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        
        if completed:
            if messagebox.askyesno("í™•ì¸", f"{len(completed)}ê±´ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\nì œì™¸í•˜ê³  ë¯¸ì™„ë£Œ ê±´ë§Œ ê°±ì‹ í• ê¹Œìš”?"):
                ids = [i for i in ids if i not in completed]
        
        if not ids:
            messagebox.showinfo("ì·¨ì†Œ", "ê°±ì‹ í•  ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        t = threading.Thread(target=self._run_refresh_ids, args=(ids,))
        t.daemon = True
        t.start()

    def _run_refresh_ids(self, ids):
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            self.after(0, lambda: messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."))
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            self.after(0, lambda: messagebox.showerror("ì˜¤ë¥˜", f"API Keyê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n{e}"))
            return
        
        self.append_log(f"ì„ íƒëœ {len(ids)}ê±´ ê°±ì‹  ì¤‘...")
        success_cnt = 0
        fail_cnt = 0
        
        for bid in ids:
            try:
                remote = client.batches.retrieve(bid)
                rc = None
                if remote.request_counts:
                    rc = {"total": remote.request_counts.total, "completed": remote.request_counts.completed, "failed": remote.request_counts.failed}
                upsert_batch_job(bid, status=remote.status, output_file_id=remote.output_file_id, request_counts=rc)
                self.append_log(f"âœ… {bid}: {remote.status}")
                success_cnt += 1
            except Exception as e:
                error_msg = str(e)
                # 401 ì˜¤ë¥˜ì¸ ê²½ìš° ë” ëª…í™•í•œ ë©”ì‹œì§€
                if "401" in error_msg or "authentication" in error_msg.lower():
                    self.append_log(f"âŒ {bid} ê°±ì‹  ì‹¤íŒ¨: API Key ì¸ì¦ ì˜¤ë¥˜ (401)")
                    self.append_log(f"   â†’ ìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ ì˜¬ë°”ë¥¸ API Keyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                else:
                    self.append_log(f"âŒ {bid} ê°±ì‹  ì‹¤íŒ¨: {error_msg}")
                fail_cnt += 1
        
        self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])
        if fail_cnt > 0:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}, ì‹¤íŒ¨: {fail_cnt})")
            if fail_cnt == len(ids):
                self.after(0, lambda: messagebox.showwarning("ê²½ê³ ", f"ëª¨ë“  ë°°ì¹˜ ê°±ì‹ ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\nAPI Keyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."))
        else:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}ê±´)")

    def _merge_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        
        # API í‚¤ í™•ì¸
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.\nìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        jobs = load_batch_jobs()
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        group_ids = set()
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if job:
                group_id = job.get("batch_group_id")
                if group_id:
                    group_ids.add(group_id)
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        all_target_ids = set(ids)
        for group_id in group_ids:
            if group_id:
                group_batches = [j for j in jobs if j.get("batch_group_id") == group_id and j.get("status") == "completed"]
                for j in group_batches:
                    all_target_ids.add(j["batch_id"])
        
        if len(all_target_ids) > len(ids):
            group_info = f"\n\nê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ {len(all_target_ids) - len(ids)}ê°œê°€ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤."
        else:
            group_info = ""
        
        # completed ë˜ëŠ” merged ìƒíƒœì¸ ë°°ì¹˜ ëª¨ë‘ ì„ íƒ ê°€ëŠ¥
        targets = [bid for bid in all_target_ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ë³‘í•©í•  ìˆ˜ ìˆëŠ” 'completed' ë˜ëŠ” 'merged' ìƒíƒœì˜ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        completed_cnt = sum(1 for bid in targets if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "completed")
        merged_cnt = len(targets) - completed_cnt
        
        msg = f"ì„ íƒí•œ {len(targets)}ê±´ì„ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ?{group_info}"
        if merged_cnt > 0:
            msg += f"\n\n({completed_cnt}ê±´: ë³‘í•© + íˆìŠ¤í† ë¦¬ ê¸°ë¡)\n({merged_cnt}ê±´: íˆìŠ¤í† ë¦¬ ê¸°ë¡ë§Œ)"
        
        if messagebox.askyesno("ë³‘í•©", msg):
            t = threading.Thread(target=self._run_merge_multi, args=(targets,))
            t.daemon = True
            t.start()

    def _run_merge_multi(self, ids):
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            self.after(0, lambda: messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."))
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            self.after(0, lambda: messagebox.showerror("ì˜¤ë¥˜", f"API Keyê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n{e}"))
            return
        
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ë¶„ë¥˜
        groups_to_merge = {}  # {group_id: [batch_ids]}
        ungrouped_batches = []  # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤
        
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not job:
                continue
            
            group_id = job.get("batch_group_id")
            if group_id:
                if group_id not in groups_to_merge:
                    groups_to_merge[group_id] = []
                groups_to_merge[group_id].append(bid)
            else:
                ungrouped_batches.append(bid)
        
        success_cnt = 0
        total_cost = 0.0
        
        # ê·¸ë£¹ë³„ ë³‘í•© ì²˜ë¦¬
        for group_id, batch_ids in groups_to_merge.items():
            self.append_log(f"--- ê·¸ë£¹ ë³‘í•© ì‹œì‘: {group_id} ({len(batch_ids)}ê°œ ë°°ì¹˜) ---")
            try:
                # ê·¸ë£¹ ë‚´ ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì›ë³¸ ì—‘ì…€ ê²½ë¡œ ì‚¬ìš©
                first_job = next((j for j in jobs if j["batch_id"] == batch_ids[0]), None)
                if not first_job:
                    continue
                
                src_path = first_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ì„ì‹œ JSONLì— ìˆ˜ì§‘
                all_output_lines = []
                model_name = first_job.get("model", "gpt-5-mini")
                total_group_cost = 0.0
                
                # ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ (chunk_indexê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ)
                def get_chunk_index(bid):
                    job = next((j for j in jobs if j["batch_id"] == bid), None)
                    if job:
                        idx = job.get("chunk_index")
                        return idx if idx is not None else 999999
                    return 999999
                
                batch_ids_sorted = sorted(batch_ids, key=get_chunk_index)
                
                for bid in batch_ids_sorted:
                    self.append_log(f"  [ê·¸ë£¹] ë°°ì¹˜ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    try:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if not local_job:
                            continue
                        
                        # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                        if local_job.get("status") == "merged":
                            self.append_log(f"  â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        # Batch ìƒíƒœ í™•ì¸
                        remote = client.batches.retrieve(bid)
                        if remote.status != "completed":
                            self.append_log(f"  âš ï¸ {bid}: ì•„ì§ completed ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                            upsert_batch_job(bid, status=remote.status, output_file_id=getattr(remote, "output_file_id", None))
                            continue
                        
                        output_file_id = getattr(remote, "output_file_id", None)
                        if not output_file_id:
                            self.append_log(f"  âš ï¸ {bid}: output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                        base_dir = os.path.dirname(src_path)
                        base_name, _ = os.path.splitext(os.path.basename(src_path))
                        out_jsonl = os.path.join(base_dir, f"{base_name}_stage3_batch_output_{bid}.jsonl")
                        
                        try:
                            content = client.files.content(output_file_id).content
                        except AttributeError:
                            file_content = client.files.content(output_file_id)
                            if hasattr(file_content, "read"):
                                content = file_content.read()
                            elif hasattr(file_content, "iter_bytes"):
                                chunks = []
                                for ch in file_content.iter_bytes():
                                    chunks.append(ch)
                                content = b"".join(chunks)
                            else:
                                content = file_content
                        
                        with open(out_jsonl, "wb") as f:
                            f.write(content)
                        
                        upsert_batch_job(bid, status="completed", output_file_id=output_file_id, output_jsonl=out_jsonl)
                        
                        # JSONL íŒŒì¼ ì½ì–´ì„œ ìˆ˜ì§‘
                        if os.path.exists(out_jsonl):
                            with open(out_jsonl, "r", encoding="utf-8") as f:
                                for line in f:
                                    line = line.strip()
                                    if line:
                                        all_output_lines.append(line)
                        
                    except Exception as e:
                        self.append_log(f"  âŒ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if not all_output_lines:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ë³‘í•©í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ì˜ ì „ì²´ ì²­í¬ ìˆ˜ í™•ì¸ ë° ê²€ì¦
                expected_total_chunks = first_job.get("total_chunks")
                if expected_total_chunks:
                    downloaded_batch_ids = []
                    for bid in batch_ids_sorted:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if local_job and local_job.get("status") == "completed":
                            out_jsonl = local_job.get("output_jsonl") or os.path.join(
                                os.path.dirname(src_path),
                                f"{os.path.splitext(os.path.basename(src_path))[0]}_stage3_batch_output_{bid}.jsonl"
                            )
                            if os.path.exists(out_jsonl):
                                downloaded_batch_ids.append(bid)
                    
                    if len(downloaded_batch_ids) < expected_total_chunks:
                        missing = expected_total_chunks - len(downloaded_batch_ids)
                        self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì˜ˆìƒ {expected_total_chunks}ê°œ ì¤‘ {len(downloaded_batch_ids)}ê°œë§Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({missing}ê°œ ëˆ„ë½ ê°€ëŠ¥)")
                
                # ì„ì‹œ í†µí•© JSONL íŒŒì¼ ìƒì„±
                base_dir = os.path.dirname(src_path)
                base_name, _ = os.path.splitext(os.path.basename(src_path))
                merged_jsonl = os.path.join(base_dir, f"{base_name}_stage3_batch_output_merged_{group_id}.jsonl")
                
                with open(merged_jsonl, "w", encoding="utf-8") as f:
                    for line in all_output_lines:
                        f.write(line + "\n")
                
                self.append_log(f"  [ê·¸ë£¹] í†µí•© JSONL ìƒì„±: {len(all_output_lines)}ê°œ ê²°ê³¼")
                
                # í†µí•© JSONLì„ ì—‘ì…€ì— ë³‘í•©
                results_map = {}
                batch_in_tok = 0
                batch_out_tok = 0
                
                with open(merged_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        
                        usage = data.get("response", {}).get("body", {}).get("usage", {})
                        batch_in_tok += usage.get("prompt_tokens", 0)
                        batch_out_tok += usage.get("completion_tokens", 0)
                        
                        try:
                            val = data["response"]["body"]["choices"][0]["message"]["content"].strip()
                            results_map[cid] = val
                        except: results_map[cid] = ""
                
                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    if "ST3_ê²°ê³¼ìƒí’ˆëª…" not in df.columns:
                        df["ST3_ê²°ê³¼ìƒí’ˆëª…"] = ""
                    df["ST3_ê²°ê³¼ìƒí’ˆëª…"] = df["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str)
                    cnt = 0
                    for cid, val in results_map.items():
                        try:
                            idx = int(cid.split("_")[1])
                            if 0 <= idx < len(df):
                                df.at[idx, "ST3_ê²°ê³¼ìƒí’ˆëª…"] = val
                                cnt += 1
                        except:
                            pass

                    # ì½”ì–´ ì™„ë£Œ íŒŒì¼(out_excel)ì„ ë¨¼ì € ì €ì¥
                    base, _ = os.path.splitext(src_path)
                    out_excel = f"{base}_stage3_batch_done.xlsx"
                    if not safe_save_excel(df, out_excel):
                        self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")

                    # Stage3 ìµœì¢… íŒŒì¼ëª…: *_T3_... í˜•ì‹ìœ¼ë¡œ ë²„ì „ ì—…
                    try:
                        final_out_path = get_next_version_path(src_path, task_type="text")
                        df_done = pd.read_excel(out_excel)
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ê³¼ ì—†ëŠ” í–‰ ë¶„ë¦¬
                        if "ST3_ê²°ê³¼ìƒí’ˆëª…" in df_done.columns:
                            df_with_st3 = df_done[df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].notna() & (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"] != '') & (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str) != 'nan')].copy()
                            df_no_st3 = df_done[(df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].isna()) | (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"] == '') | (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str) == 'nan')].copy()
                        else:
                            df_with_st3 = pd.DataFrame()
                            df_no_st3 = df_done.copy()
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ì—†ëŠ” í–‰ë“¤ì„ T3(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³„ë„ íŒŒì¼ ì €ì¥
                        no_st3_path = None
                        if len(df_no_st3) > 0:
                            base_dir = os.path.dirname(src_path)
                            base_name, ext = os.path.splitext(os.path.basename(src_path))
                            
                            name_only_clean = re.sub(r"\([^)]*\)", "", base_name)
                            all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
                            
                            if all_matches:
                                match = all_matches[-1]
                                original_name = name_only_clean[: match.start()].rstrip("_")
                                current_i = int(match.group(4))
                                new_filename = f"{original_name}_T3_I{current_i}(ì‹¤íŒ¨){ext}"
                            else:
                                new_filename = f"{base_name}_T3(ì‹¤íŒ¨)_I0{ext}"
                            
                            no_st3_path = os.path.join(base_dir, new_filename)
                            df_no_st3.to_excel(no_st3_path, index=False)
                            
                            self.append_log(f"  [ê·¸ë£¹] T3(ì‹¤íŒ¨) ë¶„ë¦¬ íŒŒì¼: {os.path.basename(no_st3_path)} ({len(df_no_st3)}ê°œ í–‰)")
                            self.append_log(f"         â€» ì´ íŒŒì¼ì€ T3 ì‘ì—…ì— ì‹¤íŒ¨í•œ í•­ëª©ì…ë‹ˆë‹¤.")
                            
                            try:
                                no_st3_root_name = get_root_filename(no_st3_path)
                                JobManager.update_status(no_st3_root_name, text_msg="T3(ì‹¤íŒ¨)")
                                self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸: {no_st3_root_name} -> T3(ì‹¤íŒ¨)")
                            except Exception as e:
                                self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                        
                        if len(df_with_st3) > 0:
                            df_done = df_with_st3
                        else:
                            self.append_log(f"  âš ï¸ ê·¸ë£¹ {group_id}: ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        if safe_save_excel(df_done, final_out_path):
                            out_path_for_history = final_out_path
                            if out_excel != final_out_path and os.path.exists(out_excel):
                                try:
                                    os.remove(out_excel)
                                    self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                                except Exception as e:
                                    self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                        else:
                            out_path_for_history = out_excel
                    except Exception as e:
                        self.append_log(f"[WARN] T3 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                        out_path_for_history = out_excel
                    
                    # ë¹„ìš© ê³„ì‚°
                    pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                    cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                    cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                    cost_total = cost_in + cost_out
                    total_group_cost += cost_total
                    total_cost += total_group_cost
                    
                    # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ë¥¼ merged ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                    for bid in batch_ids:
                        upsert_batch_job(
                            batch_id=bid,
                            out_excel=out_path_for_history,
                            status="merged",
                        )
                    
                    # ì‹¤í–‰ ì´ë ¥ ê¸°ë¡
                    try:
                        if first_job:
                            c_at_str = first_job.get("created_at", "")
                            if c_at_str:
                                c_at = datetime.fromisoformat(c_at_str)
                            else:
                                c_at = datetime.now()
                            finish_dt = datetime.now()
                            elapsed = (finish_dt - c_at).total_seconds()
                            
                            append_run_history(
                                stage="Stage 3 Batch (Grouped)",
                                model_name=model_name,
                                reasoning_effort=first_job.get("effort", "medium"),
                                src_file=src_path,
                                out_file=out_path_for_history,
                                total_rows=len(df),
                                api_rows=len(results_map),
                                elapsed_seconds=elapsed,
                                total_in_tok=batch_in_tok,
                                total_out_tok=batch_out_tok,
                                total_reasoning_tok=0,
                                input_cost_usd=cost_in,
                                output_cost_usd=cost_out,
                                total_cost_usd=cost_total,
                                start_dt=c_at,
                                finish_dt=finish_dt,
                                api_type="batch",
                                batch_id=f"{group_id} ({len(batch_ids)} batches)",
                                success_rows=cnt,
                                fail_rows=len(results_map)-cnt,
                            )
                    except Exception as hist_e:
                        self.append_log(f"[WARN] ê·¸ë£¹ {group_id} íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì‹¤íŒ¨: {hist_e}")
                    
                    try:
                        root_name = get_root_filename(src_path)
                        JobManager.update_status(root_name, text_msg="T3(ìƒì„±ì™„ë£Œ)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T3(ìƒì„±ì™„ë£Œ)")
                    except Exception as e:
                        self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")
                    
                    self.append_log(f"âœ… ê·¸ë£¹ {group_id} ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(out_path_for_history)}")
                    success_cnt += 1
                else:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                self.append_log(f"âŒ ê·¸ë£¹ {group_id} ë³‘í•© ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        for bid in ungrouped_batches:
            self.append_log(f"--- ë³‘í•© ì‹œì‘: {bid} ---")
            try:
                jobs = load_batch_jobs()
                local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                
                if not local_job:
                    self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì‘ì—… ì´ë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” íŒŒì¼ ì¬ë³‘í•©ì€ ê±´ë„ˆë›°ë˜, íˆìŠ¤í† ë¦¬ ê¸°ë¡ì€ ìˆ˜í–‰
                is_already_merged = local_job.get("status") == "merged"
                if is_already_merged:
                    self.append_log(f"â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. íˆìŠ¤í† ë¦¬ ê¸°ë¡ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                    # merged ìƒíƒœì¸ ê²½ìš° ê¸°ì¡´ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©
                    src_path = local_job.get("src_excel")
                    out_path_for_history = local_job.get("out_excel")
                    if not src_path or not out_path_for_history or not os.path.exists(out_path_for_history):
                        self.append_log(f"âš ï¸ {bid}: ë³‘í•©ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íˆìŠ¤í† ë¦¬ ê¸°ë¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    
                    # ê¸°ì¡´ íŒŒì¼ì—ì„œ í† í° ì •ë³´ ì¶”ì¶œ (JSONL íŒŒì¼ì´ ìˆìœ¼ë©´)
                    base, _ = os.path.splitext(src_path) if src_path else ("", "")
                    out_jsonl = f"{base}_stage3_batch_output.jsonl"
                    batch_in_tok = 0
                    batch_out_tok = 0
                    results_map = {}
                    
                    if os.path.exists(out_jsonl):
                        try:
                            with open(out_jsonl, "r", encoding="utf-8") as f:
                                for line in f:
                                    if not line.strip(): continue
                                    data = json.loads(line)
                                    usage = data.get("response", {}).get("body", {}).get("usage", {})
                                    batch_in_tok += usage.get("prompt_tokens", 0)
                                    batch_out_tok += usage.get("completion_tokens", 0)
                                    cid = data.get("custom_id")
                                    try:
                                        val = data["response"]["body"]["choices"][0]["message"]["content"].strip()
                                        results_map[cid] = val
                                    except: pass
                        except Exception as e:
                            self.append_log(f"[WARN] {bid}: JSONL íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                    
                    # ë¹„ìš© ê³„ì‚°
                    model_name = local_job.get("model", "gpt-5-mini")
                    pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                    cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                    cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                    cost_total = cost_in + cost_out
                    
                    # ì¶œë ¥ íŒŒì¼ì—ì„œ í–‰ ìˆ˜ í™•ì¸
                    try:
                        df_out = pd.read_excel(out_path_for_history)
                        total_rows = len(df_out)
                        api_rows = len(results_map) if results_map else total_rows
                        cnt = api_rows  # merged ìƒíƒœì—ì„œëŠ” ì„±ê³µ ê±´ìˆ˜ ì¶”ì •
                    except:
                        total_rows = 0
                        api_rows = 0
                        cnt = 0
                    
                    # íˆìŠ¤í† ë¦¬ ê¸°ë¡ë§Œ ìˆ˜í–‰
                    try:
                        c_at_str = local_job.get("created_at", "")
                        if c_at_str:
                            c_at = datetime.fromisoformat(c_at_str)
                        else:
                            c_at = datetime.now()
                        finish_dt = datetime.now()
                        elapsed = (finish_dt - c_at).total_seconds()
                        
                        # íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì „ì— íŒŒì¼ ê²½ë¡œ í™•ì¸
                        from stage3_run_history import RUN_LOG_PATH
                        self.append_log(f"[DEBUG] íˆìŠ¤í† ë¦¬ íŒŒì¼ ê²½ë¡œ: {RUN_LOG_PATH}")
                        
                        result = append_run_history(
                            stage="Stage 3 Batch",
                            model_name=model_name,
                            reasoning_effort=local_job.get("effort", "medium"),
                            src_file=src_path,
                            out_file=out_path_for_history,
                            total_rows=total_rows,
                            api_rows=api_rows,
                            elapsed_seconds=elapsed,
                            total_in_tok=batch_in_tok,
                            total_out_tok=batch_out_tok,
                            total_reasoning_tok=0,
                            input_cost_usd=cost_in,
                            output_cost_usd=cost_out,
                            total_cost_usd=cost_total,
                            start_dt=c_at,
                            finish_dt=finish_dt,
                            api_type="batch",
                            batch_id=bid,
                            success_rows=cnt,
                            fail_rows=api_rows - cnt if api_rows > 0 else 0,
                        )
                        if result:
                            # íŒŒì¼ì´ ì‹¤ì œë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            if os.path.exists(RUN_LOG_PATH):
                                self.append_log(f"[INFO] âœ… ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì™„ë£Œ: {RUN_LOG_PATH} (ë°°ì¹˜ ID: {bid})")
                            else:
                                self.append_log(f"[ERROR] âŒ ì‹¤í–‰ ì´ë ¥ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {RUN_LOG_PATH}")
                        else:
                            self.append_log(f"[INFO] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ê±´ë„ˆëœ€: ë°°ì¹˜ {bid}ëŠ” ì´ë¯¸ ê¸°ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    except Exception as hist_e:
                        import traceback
                        error_detail = traceback.format_exc()
                        self.append_log(f"[WARN] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨")
                        self.append_log(f"[WARN] ì˜¤ë¥˜ ìƒì„¸: {str(hist_e)}")
                        self.append_log(f"[WARN] {error_detail}")
                    
                    continue

                # Batch ìƒíƒœ ë° ê²°ê³¼ íŒŒì¼ ID ì¡°íšŒ (ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©)
                remote = client.batches.retrieve(bid)
                
                # ìµœì‹  ìƒíƒœ í™•ì¸ (ê°±ì‹  í›„ ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŒ)
                if remote.status != "completed":
                    self.append_log(f"âš ï¸ {bid}: ì•„ì§ completed ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    upsert_batch_job(bid, status=remote.status, output_file_id=getattr(remote, "output_file_id", None))
                    continue
                
                # output_file_id í™•ì¸
                output_file_id = getattr(remote, "output_file_id", None)
                if not output_file_id:
                    self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: output_file_id ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (status={remote.status})")
                    upsert_batch_job(bid, status=remote.status)
                    continue
                
                try:
                    content = client.files.content(output_file_id).content
                except AttributeError:
                    # ì‹ ë²„ì „ í´ë¼ì´ì–¸íŠ¸ ëŒ€ì‘
                    file_content = client.files.content(output_file_id)
                    if hasattr(file_content, "read"):
                        content = file_content.read()
                    elif hasattr(file_content, "iter_bytes"):
                        chunks = []
                        for ch in file_content.iter_bytes():
                            chunks.append(ch)
                        content = b"".join(chunks)
                    else:
                        content = file_content  # type: ignore
                
                if local_job and local_job.get("src_excel"):
                    src_path = local_job["src_excel"]
                    base, _ = os.path.splitext(src_path)
                    out_jsonl = f"{base}_stage3_batch_output.jsonl"
                    out_excel = f"{base}_stage3_batch_done.xlsx"
                else:
                    out_jsonl = f"output_{bid}.jsonl"
                    out_excel = f"output_{bid}.xlsx"
                    src_path = None

                with open(out_jsonl, "wb") as f:
                    f.write(content)
                
                results_map = {}
                batch_in_tok = 0
                batch_out_tok = 0
                
                with open(out_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        
                        usage = data.get("response", {}).get("body", {}).get("usage", {})
                        batch_in_tok += usage.get("prompt_tokens", 0)
                        batch_out_tok += usage.get("completion_tokens", 0)
                        
                        try:
                            val = data["response"]["body"]["choices"][0]["message"]["content"].strip()
                            results_map[cid] = val
                        except: results_map[cid] = ""
                
                # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                model_name = local_job.get("model", "gpt-5-mini") if local_job else "gpt-5-mini"
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                cost_total = cost_in + cost_out
                total_cost += cost_total

                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    if "ST3_ê²°ê³¼ìƒí’ˆëª…" not in df.columns:
                        df["ST3_ê²°ê³¼ìƒí’ˆëª…"] = ""
                    df["ST3_ê²°ê³¼ìƒí’ˆëª…"] = df["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str)
                    cnt = 0
                    for cid, val in results_map.items():
                        try:
                            idx = int(cid.split("_")[1])
                            if 0 <= idx < len(df):
                                df.at[idx, "ST3_ê²°ê³¼ìƒí’ˆëª…"] = val
                                cnt += 1
                        except:
                            pass

                    # ì½”ì–´ ì™„ë£Œ íŒŒì¼(out_excel)ì„ ë¨¼ì € ì €ì¥
                    if not safe_save_excel(df, out_excel):
                        self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")

                    # Stage3 ìµœì¢… íŒŒì¼ëª…: *_T3_... í˜•ì‹ìœ¼ë¡œ ë²„ì „ ì—…
                    try:
                        final_out_path = get_next_version_path(src_path, task_type="text")
                        df_done = pd.read_excel(out_excel)
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ê³¼ ì—†ëŠ” í–‰ ë¶„ë¦¬
                        if "ST3_ê²°ê³¼ìƒí’ˆëª…" in df_done.columns:
                            # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì¸ í–‰ ì°¾ê¸°
                            df_with_st3 = df_done[df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].notna() & (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"] != '') & (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str) != 'nan')].copy()
                            df_no_st3 = df_done[(df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].isna()) | (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"] == '') | (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str) == 'nan')].copy()
                        else:
                            # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  í–‰ì´ ST3_ê²°ê³¼ìƒí’ˆëª… ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
                            df_with_st3 = pd.DataFrame()
                            df_no_st3 = df_done.copy()
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ì—†ëŠ” í–‰ë“¤ì„ T3(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³„ë„ íŒŒì¼ ì €ì¥
                        no_st3_path = None
                        if len(df_no_st3) > 0:
                            base_dir = os.path.dirname(src_path)
                            base_name, ext = os.path.splitext(os.path.basename(src_path))
                            
                            # í˜„ì¬ íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: _T2_I0)
                            # T3(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³€ê²½
                            name_only_clean = re.sub(r"\([^)]*\)", "", base_name)  # ê¸°ì¡´ ê´„í˜¸ ì œê±°
                            all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
                            
                            if all_matches:
                                # ë§ˆì§€ë§‰ ë²„ì „ íŒ¨í„´ ì‚¬ìš©
                                match = all_matches[-1]
                                original_name = name_only_clean[: match.start()].rstrip("_")
                                current_i = int(match.group(4))
                                # T3(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ìƒì„±
                                new_filename = f"{original_name}_T3_I{current_i}(ì‹¤íŒ¨){ext}"
                            else:
                                # ë²„ì „ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ T3(ì‹¤íŒ¨)_I0ë¡œ ìƒì„±
                                new_filename = f"{base_name}_T3(ì‹¤íŒ¨)_I0{ext}"
                            
                            no_st3_path = os.path.join(base_dir, new_filename)
                            df_no_st3.to_excel(no_st3_path, index=False)
                            
                            self.append_log(f"  T3(ì‹¤íŒ¨) ë¶„ë¦¬ íŒŒì¼: {os.path.basename(no_st3_path)} ({len(df_no_st3)}ê°œ í–‰)")
                            self.append_log(f"  â€» ì´ íŒŒì¼ì€ T3 ì‘ì—…ì— ì‹¤íŒ¨í•œ í•­ëª©ì…ë‹ˆë‹¤.")
                            
                            # ë¶„ë¦¬ëœ íŒŒì¼ì˜ ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                            try:
                                no_st3_root_name = get_root_filename(no_st3_path)
                                JobManager.update_status(no_st3_root_name, text_msg="T3(ì‹¤íŒ¨)")
                                self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸: {no_st3_root_name} -> T3(ì‹¤íŒ¨)")
                            except Exception as e:
                                self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ë“¤ë§Œ ì €ì¥
                        if len(df_with_st3) > 0:
                            df_done = df_with_st3
                        else:
                            self.append_log(f"âš ï¸ {bid}: ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        if safe_save_excel(df_done, final_out_path):
                            out_path_for_history = final_out_path
                            # T3 ë²„ì „ íŒŒì¼ ì €ì¥ ì„±ê³µ ì‹œ, ì½”ì–´ê°€ ìƒì„±í•œ ì¤‘ê°„ íŒŒì¼(_stage3_batch_done) ì‚­ì œ
                            if out_excel != final_out_path and os.path.exists(out_excel):
                                try:
                                    os.remove(out_excel)
                                    self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                                except Exception as e:
                                    self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                        else:
                            out_path_for_history = out_excel
                    except Exception as e:
                        self.append_log(f"[WARN] T3 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                        out_path_for_history = out_excel

                    upsert_batch_job(bid, out_excel=out_path_for_history, status="merged")

                    # History ê¸°ë¡ (naive datetime ê¸°ì¤€)
                    try:
                        c_at_str = local_job.get("created_at", "")
                        if c_at_str:
                            c_at = datetime.fromisoformat(c_at_str)
                        else:
                            c_at = datetime.now()
                        finish_dt = datetime.now()
                        elapsed = (finish_dt - c_at).total_seconds()

                        # íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì „ì— íŒŒì¼ ê²½ë¡œ í™•ì¸
                        from stage3_run_history import RUN_LOG_PATH
                        self.append_log(f"[DEBUG] íˆìŠ¤í† ë¦¬ íŒŒì¼ ê²½ë¡œ: {RUN_LOG_PATH}")
                        
                        result = append_run_history(
                            stage="Stage 3 Batch",
                            model_name=model_name,
                            reasoning_effort=local_job.get("effort", "medium"),
                            src_file=src_path,
                            out_file=out_path_for_history,
                            total_rows=len(df),
                            api_rows=len(results_map),
                            elapsed_seconds=elapsed,
                            total_in_tok=batch_in_tok,
                            total_out_tok=batch_out_tok,
                            total_reasoning_tok=0,
                            input_cost_usd=cost_in,
                            output_cost_usd=cost_out,
                            total_cost_usd=cost_total,
                            start_dt=c_at,
                            finish_dt=finish_dt,
                            api_type="batch",
                            batch_id=bid,
                            success_rows=cnt,
                            fail_rows=len(results_map)-cnt,
                        )
                        if result:
                            # íŒŒì¼ì´ ì‹¤ì œë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            if os.path.exists(RUN_LOG_PATH):
                                self.append_log(f"[INFO] âœ… ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì™„ë£Œ: {RUN_LOG_PATH} (ë°°ì¹˜ ID: {bid})")
                            else:
                                self.append_log(f"[ERROR] âŒ ì‹¤í–‰ ì´ë ¥ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {RUN_LOG_PATH}")
                        else:
                            self.append_log(f"[INFO] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ê±´ë„ˆëœ€: ë°°ì¹˜ {bid}ëŠ” ì´ë¯¸ ê¸°ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    except Exception as hist_e:
                        # íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì‹¤íŒ¨í•´ë„ ë³‘í•©ì€ ì„±ê³µí•œ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
                        import traceback
                        error_detail = traceback.format_exc()
                        self.append_log(f"[WARN] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨ (ë³‘í•©ì€ ì •ìƒ ì™„ë£Œ)")
                        self.append_log(f"[WARN] ì˜¤ë¥˜ ìƒì„¸: {str(hist_e)}")
                        self.append_log(f"[WARN] {error_detail}")

                    # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage3(Text) ì™„ë£Œ ìƒíƒœ ê¸°ë¡: T3(ìƒì„±ì™„ë£Œ) (img ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                    try:
                        root_name = get_root_filename(src_path)
                        JobManager.update_status(root_name, text_msg="T3(ìƒì„±ì™„ë£Œ)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T3(ìƒì„±ì™„ë£Œ)")
                    except Exception as e:
                        self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")

                    self.append_log(f"âœ… ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(out_path_for_history)}")
                    success_cnt += 1
                else:
                    self.append_log(f"âš ï¸ ì›ë³¸ ì—†ìŒ. JSONLë§Œ ì €ì¥.")
            except Exception as e:
                self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: {e}")
        
        self.append_log(f"=== ì¼ê´„ ë³‘í•© ë (ì„±ê³µ: {success_cnt}, ë¹„ìš©: ${total_cost:.4f}) ===")
        self._load_jobs_all()
        messagebox.showinfo("ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©: ${total_cost:.4f}")

    def _report_selected_unified(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        targets = [bid for bid in ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "merged"]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ìƒíƒœê°€ 'merged'ì¸ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if messagebox.askyesno("ë¦¬í¬íŠ¸", f"ì„ íƒí•œ {len(targets)}ê±´ì˜ í†µí•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_report_unified, args=(targets,))
            t.daemon = True
            t.start()

    def _run_report_unified(self, ids):
        self.append_log(f"--- í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ({len(ids)}ê±´) ---")
        jobs = load_batch_jobs()
        all_reps = []
        for bid in ids:
            local_job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not local_job: continue
            src = local_job.get("src_excel")
            out = local_job.get("out_excel")
            if not src or not out or not os.path.exists(src) or not os.path.exists(out): 
                self.append_log(f"âš ï¸ íŒŒì¼ ëˆ„ë½: {bid}")
                continue
            try:
                df_in = pd.read_excel(src)
                df_out = pd.read_excel(out)
                
                # ì›ë³¸ ìƒí’ˆëª… ì»¬ëŸ¼ ì°¾ê¸° (ìš°ì„ ìˆœìœ„: ST1_ê²°ê³¼ìƒí’ˆëª… > ì›ë³¸ìƒí’ˆëª…)
                orig_col = None
                for col in ["ST1_ê²°ê³¼ìƒí’ˆëª…", "ì›ë³¸ìƒí’ˆëª…", "ê³µê¸‰ì‚¬ìƒí’ˆëª…"]:
                    if col in df_in.columns:
                        orig_col = col
                        break
                
                # Stage 3 ë¦¬í¬íŠ¸ ë¡œì§ (ì›ë³¸ê³¼ ë¹„êµ)
                for idx, row in df_in.iterrows():
                    # ì›ë³¸ ìƒí’ˆëª…
                    orig_name = safe_str(row.get(orig_col, "")) if orig_col else ""
                    
                    # ST3 ê²°ê³¼
                    st3 = ""
                    if idx < len(df_out):
                        st3 = safe_str(df_out.iloc[idx].get("ST3_ê²°ê³¼ìƒí’ˆëª…", ""))
                    
                    cands = [x for x in st3.split('\n') if x.strip()]
                    first_line = cands[0] if cands else "(ìƒì„±ì‹¤íŒ¨)"
                    
                    # ë¹„êµ ì •ë³´
                    is_same = (orig_name.strip() == first_line.strip()) if orig_name and first_line != "(ìƒì„±ì‹¤íŒ¨)" else False
                    length_diff = len(first_line) - len(orig_name) if orig_name else None
                    
                    all_reps.append({
                        "Batch_ID": bid,
                        "í–‰ë²ˆí˜¸": idx + 2,
                        "ìƒí’ˆì½”ë“œ": safe_str(row.get("ìƒí’ˆì½”ë“œ", "")),
                        "ì›ë³¸ìƒí’ˆëª…": orig_name,
                        "ST3_ì²«ì¤„": first_line,
                        "ë™ì¼ì—¬ë¶€": "âœ… ë™ì¼" if is_same else "âŒ ë‹¤ë¦„",
                        "ê¸¸ì´ì°¨ì´": length_diff if length_diff is not None else "-",
                        "ìƒì„±í›„ë³´ìˆ˜": len(cands),
                        "ST2_ê¸¸ì´": len(safe_str(row.get("ST2_JSON", "")))
                    })
            except Exception as e:
                self.append_log(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜ ({bid}): {e}")
                import traceback
                self.append_log(traceback.format_exc())

        if not all_reps:
            messagebox.showinfo("ì•Œë¦¼", "ë°ì´í„° ì—†ìŒ")
            return

        try:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_dir = os.path.dirname(os.path.abspath(__file__))
            report_path = os.path.join(save_dir, f"Stage3_Unified_Report_{ts}.xlsx")
            pd.DataFrame(all_reps).to_excel(report_path, index=False)
            self.append_log(f"ğŸ“Š ë¦¬í¬íŠ¸ ì™„ë£Œ: {os.path.basename(report_path)}")
            if messagebox.askyesno("ì™„ë£Œ", "íŒŒì¼ì„ ì—¬ì‹œê² ìŠµë‹ˆê¹Œ?"):
                os.startfile(report_path)
        except Exception as e:
            self.append_log(f"ì‹¤íŒ¨: {e}")
            messagebox.showerror("ì˜¤ë¥˜", str(e))

    def _archive_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if ids and messagebox.askyesno("ë³´ê´€", f"{len(ids)}ê±´ íœ´ì§€í†µ ì´ë™?"):
            archive_batch_job(ids, True)
            self._load_jobs_all()
            self._load_archive_list()

    def _restore_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ë³µêµ¬", f"{len(ids)}ê±´ ë³µêµ¬?"):
            archive_batch_job(ids, False)
            self._load_jobs_all()
            self._load_archive_list()

    def _hard_delete_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ì‚­ì œ", "ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            hard_delete_batch_job(ids)
            self._load_archive_list()

    def _on_tree_double_click(self, event):
        sel = self.tree_active.selection()
        if not sel: return
        bid = self.tree_active.item(sel[0])['values'][0]
        self.batch_id_var.set(bid)
        self.main_tabs.select(self.tab_merge)

    # ----------------------------------------------------
    # Tab 3: Merge (Manual)
    # ----------------------------------------------------
    def _init_tab_merge(self):
        container = ttk.Frame(self.tab_merge, padding=20)
        container.pack(fill='both', expand=True)
        f_in = ttk.LabelFrame(container, text="ê°œë³„ ì‘ì—…", padding=15)
        f_in.pack(fill='x')
        ttk.Label(f_in, text="Batch ID:").pack(side='left')
        ttk.Entry(f_in, textvariable=self.batch_id_var, width=45, font=("Consolas", 10)).pack(side='left', padx=10)
        
        f_btn = ttk.Frame(container)
        f_btn.pack(fill='x', pady=20)
        ttk.Button(f_btn, text="1. ê²°ê³¼ ë³‘í•©", command=self._start_merge, style="Primary.TButton").pack(fill='x', pady=5)
        ttk.Button(f_btn, text="2. ë‹¨ì¼ ë¦¬í¬íŠ¸", command=self._start_diff_report).pack(fill='x', pady=5)

    def _start_merge(self):
        # API í‚¤ í™•ì¸
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.\nìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        bid = self.batch_id_var.get().strip()
        if not bid:
            messagebox.showwarning("ì˜¤ë¥˜", "Batch IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        t = threading.Thread(target=self._run_merge)
        t.daemon = True
        t.start()

    def _run_merge(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_merge_multi([bid])

    def _start_diff_report(self):
        t = threading.Thread(target=self._run_diff_report)
        t.daemon = True
        t.start()

    def _run_diff_report(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_report_unified([bid])

if __name__ == "__main__":
    app = Stage3BatchGUI()
    app.mainloop()