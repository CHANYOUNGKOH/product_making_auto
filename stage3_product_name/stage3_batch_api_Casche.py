"""
stage3_batch_api_Casche.py

Stage 3 Batch API ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (GUI) - ìºì‹± ìµœì í™” ë²„ì „
- ê¸°ëŠ¥: Batch JSONL ìƒì„± -> ì—…ë¡œë“œ -> ì‹¤í–‰ -> ê²°ê³¼ ë³‘í•© -> [NEW] í†µí•© ë¦¬í¬íŠ¸ & íœ´ì§€í†µ
- [Fix] ë°°ì¹˜ ëª©ë¡ ë° íœ´ì§€í†µì— 'Effort' ì»¬ëŸ¼ ì¶”ê°€
- ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™”: OpenAI Prompt Caching ê°€ì´ë“œì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì¬êµ¬ì„±
  * ì •ì  ì½˜í…ì¸ (ì—­í• , ì œì•½, ê·œì¹™)ë¥¼ system í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * ë™ì  ì½˜í…ì¸ (ì„¤ì •, JSON ë°ì´í„°)ë¥¼ user í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * prompt_cache_key ì‚¬ìš©ìœ¼ë¡œ ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ (í† í° ë¹„ìš© ìµœëŒ€ 90% ì ˆê° ê°€ëŠ¥)
"""

import os
import sys
import json
import threading
import subprocess
import re
from datetime import datetime
from dataclasses import asdict

import pandas as pd
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, Menu
from tkinter.scrolledtext import ScrolledText

from openai import OpenAI

# ========================================================
# ë©”ì¸ ëŸ°ì²˜ ì—°ë™ìš© JobManager & íŒŒì¼ëª… ìœ í‹¸ (Stage3: Text)
# ========================================================
def get_root_filename(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´(_T*_I* ë˜ëŠ” _t*_i*, _I*(ì—…ì™„) í¬í•¨) ë° ê¸°íƒ€ ê¼¬ë¦¬í‘œë¥¼ ë–¼ê³  ì›ë³¸ëª…(Key)ë§Œ ì¶”ì¶œ
    ì˜ˆ: ìƒí’ˆ_T2_I0.xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T3_I1.xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T2_I0(ì—…ì™„).xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T2_I0_T3_I1.xlsx -> ìƒí’ˆ.xlsx (ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ ì œê±°)
    ì˜ˆ: ìƒí’ˆ_T2_I5(ì—…ì™„).xlsx -> ìƒí’ˆ.xlsx
    """
    name = os.path.basename(filename)
    base, ext = os.path.splitext(name)

    # 1. ë²„ì „ íŒ¨í„´ (_Tìˆ«ì_Iìˆ«ì(ê´„í˜¸)? ë˜ëŠ” _tìˆ«ì_iìˆ«ì(ê´„í˜¸)?) ë°˜ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    # íŒ¨í„´ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì œê±°, ê´„í˜¸ê°€ ë¶™ì€ ê²½ìš°ë„ í¬í•¨
    while True:
        new_base = re.sub(r"_[Tt]\d+_[Ii]\d+(\([^)]+\))?", "", base, flags=re.IGNORECASE)
        if new_base == base:
            break
        base = new_base
    
    # 2. ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„), (ì™„ë£Œ) ë“±) - ë²„ì „ íŒ¨í„´ì˜ ê´„í˜¸ëŠ” ì´ë¯¸ ì œê±°ë¨
    base = re.sub(r"\([^)]*\)", "", base)
    
    # 3. ê¸°íƒ€ êµ¬í˜• ê¼¬ë¦¬í‘œ ì œê±° (í˜¸í™˜ì„± ìœ ì§€)
    suffixes = ["_stage1_mapping", "_stage1_img_mapping", "_stage2_analysis", "_stage3_done", "_with_images"]
    for s in suffixes:
        base = base.replace(s, "")
    
    # 4. ëì— ë‚¨ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    base = base.rstrip("_")

    return base + ext

def get_excel_name_from_path(excel_path: str) -> str:
    """
    ì—‘ì…€ íŒŒì¼ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
    ì˜ˆ: C:/Users/.../ìƒí’ˆ_T1_I0.xlsx -> ìƒí’ˆ_T1_I0.xlsx
    """
    if not excel_path:
        return "-"
    return os.path.basename(excel_path)


def get_next_version_path(current_path: str, task_type: str = "text") -> str:
    """
    í˜„ì¬ íŒŒì¼ëª…ì„ ë¶„ì„í•´ì„œ ë‹¤ìŒ ë‹¨ê³„ì˜ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    íŒŒì¼ëª… í˜•ì‹: ì›ë³¸ëª…_T{ìˆ«ì}_I{ìˆ«ì}.xlsx ë˜ëŠ” ì›ë³¸ëª…_T{ìˆ«ì}_I{ìˆ«ì}(ê´„í˜¸).xlsx
    - task_type='text'  â†’ T ë²„ì „ +1 (Stage1: T1, Stage2: T2, Stage3: T3, ...)
    - task_type='image' â†’ I ë²„ì „ +1
    
    ì£¼ì˜: íŒŒì¼ëª…ì— ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ íŒ¨í„´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì˜ˆ: ìƒí’ˆ_T2_I5(ì—…ì™„).xlsx -> ìƒí’ˆ_T3_I5(ì—…ì™„).xlsx (text) ë˜ëŠ” ìƒí’ˆ_T2_I6(ì—…ì™„).xlsx (image)
    """
    dir_name = os.path.dirname(current_path)
    base_name = os.path.basename(current_path)
    name_only, ext = os.path.splitext(base_name)

    # ë§ˆì§€ë§‰ _T*_I*(ê´„í˜¸)? íŒ¨í„´ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ, ì—¬ëŸ¬ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ ê²ƒë§Œ)
    # ê´„í˜¸ê°€ ë¶™ì€ ê²½ìš°ë„ ì¸ì‹ (ì˜ˆ: _I5(ì—…ì™„))
    all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)(\([^)]+\))?", name_only, re.IGNORECASE))
    
    if all_matches:
        # ë§ˆì§€ë§‰ ë§¤ì¹­ ì‚¬ìš©
        match = all_matches[-1]
        current_t = int(match.group(2))
        current_i = int(match.group(4))
        i_suffix = match.group(5) or ""  # ê´„í˜¸ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ìœ ì§€ (ì˜ˆ: (ì—…ì™„))
        # ì›ë³¸ëª…ì€ ë§ˆì§€ë§‰ íŒ¨í„´ ì´ì „ê¹Œì§€
        original_name = name_only[: match.start()].rstrip("_")
    else:
        # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì›ë³¸ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì œê±° í›„ ì‚¬ìš©
        original_name = name_only
        # ê¸°ì¡´ ë²„ì „ íŒ¨í„´ ì œê±° (ê´„í˜¸ í¬í•¨)
        while True:
            new_name = re.sub(r"_[Tt]\d+_[Ii]\d+(\([^)]+\))?", "", original_name, flags=re.IGNORECASE)
            if new_name == original_name:
                break
            original_name = new_name
        original_name = original_name.rstrip("_")
        current_t = 0
        current_i = 0
        i_suffix = ""

    if task_type == "text":
        new_t = current_t + 1
        new_i = current_i
    elif task_type == "image":
        new_t = current_t
        new_i = current_i + 1
    else:
        return current_path

    # ê´„í˜¸ ë¶€ë¶„ ìœ ì§€ (ì˜ˆ: (ì—…ì™„))
    new_filename = f"{original_name}_T{new_t}_I{new_i}{i_suffix}{ext}"
    return os.path.join(dir_name, new_filename)


class JobManager:
    DB_FILE = None

    @classmethod
    def find_db_path(cls):
        if cls.DB_FILE and os.path.exists(cls.DB_FILE):
            return cls.DB_FILE

        current_dir = os.path.dirname(os.path.abspath(__file__))
        search_dirs = [
            current_dir,
            os.path.abspath(os.path.join(current_dir, "..")),
            os.path.abspath(os.path.join(current_dir, "..", "..")),
        ]

        for d in search_dirs:
            target = os.path.join(d, "job_history.json")
            if os.path.exists(target):
                cls.DB_FILE = target
                print(f"[JobManager] DB Found: {target}")
                return target

        default_path = os.path.abspath(os.path.join(current_dir, "..", "job_history.json"))
        cls.DB_FILE = default_path
        return default_path

    @classmethod
    def load_jobs(cls):
        db_path = cls.find_db_path()
        if not os.path.exists(db_path):
            return {}
        try:
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    @classmethod
    def update_status(cls, filename, text_msg=None, img_msg=None):
        """ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ìƒíƒœ ì—…ë°ì´íŠ¸ (Stage1/2/3 ê³µìš©)."""
        db_path = cls.find_db_path()
        data = cls.load_jobs()
        now = datetime.now().strftime("%m-%d %H:%M")

        if filename not in data:
            data[filename] = {
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "text_status": "ëŒ€ê¸°",
                "text_time": "-",
                "image_status": "ëŒ€ê¸°",
                "image_time": "-",
                "memo": "",
            }

        if text_msg:
            data[filename]["text_status"] = text_msg
            data[filename]["text_time"] = now
        if img_msg:
            data[filename]["image_status"] = img_msg
            data[filename]["image_time"] = now

        data[filename]["last_update"] = now

        try:
            with open(db_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"[JobManager Error] {e}")


def safe_save_excel(df: pd.DataFrame, path: str) -> bool:
    """ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆì–´ ì €ì¥ì´ ì•ˆ ë  ë•Œ ì¬ì‹œë„ë¥¼ ìœ ë„í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            df.to_excel(path, index=False)
            return True
        except PermissionError:
            if not messagebox.askretrycancel(
                "ì €ì¥ ì‹¤íŒ¨",
                f"ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤!\n[{os.path.basename(path)}]\n\n"
                "íŒŒì¼ì„ ë‹«ê³  'ë‹¤ì‹œ ì‹œë„'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
            ):
                return False
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False


# [í•„ìˆ˜ ì˜ì¡´ì„±] stage3_core_Casche.py / stage3_run_history.py
# ìºì‹± ìµœì í™” ë²„ì „ ì‚¬ìš© (stage3_core_Casche.py)
try:
    from stage3_core_Casche import (
        safe_str,
        Stage3Settings,
        Stage3Request,
        build_stage3_request_from_row,  # Row -> Request ê°ì²´(í”„ë¡¬í”„íŠ¸+ì„¤ì •) ë³€í™˜ (ìºì‹± ìµœì í™”)
    )
    CACHE_MODE_CORE = True
    from stage3_run_history import append_run_history
    _HISTORY_AVAILABLE = True
except ImportError:
    # ìºì‹± ë²„ì „ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ë²„ì „ ì‚¬ìš©
    try:
        from stage3_core import (
            safe_str,
            Stage3Settings,
            Stage3Request,
            build_stage3_request_from_row,
        )
        CACHE_MODE_CORE = False
        from stage3_run_history import append_run_history
        _HISTORY_AVAILABLE = True
    except ImportError as e:
        # ì˜ì¡´ì„± íŒŒì¼ ë¶€ì¬ ì‹œ ë¹„ìƒìš© ë”ë¯¸
        CACHE_MODE_CORE = False
        _HISTORY_AVAILABLE = False
        MODEL_PRICING_USD_PER_MTOK = {}
        def safe_str(x): return str(x) if x is not None else ""
        def load_api_key_from_file(x): return ""
        def save_api_key_to_file(x, y): pass
        def append_run_history(*args, **kwargs): 
            # ë”ë¯¸ í•¨ìˆ˜: íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë¬´ì‹œ
            pass

# === ê¸°ë³¸ ì„¤ì • ===
API_KEY_FILE = ".openai_api_key_stage3_batch"
BATCH_JOBS_FILE = os.path.join(os.path.dirname(__file__), "stage3_batch_jobs.json")

# Stage 3ìš© Batch ëª¨ë¸/ê°€ê²© (gpt-5 ê³„ì—´ë§Œ ì‚¬ìš©)
MODEL_PRICING_USD_PER_MTOK = {
    "gpt-5": {
        "input": 1.25,
        "output": 10.0,
    },
    "gpt-5-mini": {
        "input": 0.25,
        "output": 2.00,
    },
    "gpt-5-nano": {
        "input": 0.05,
        "output": 0.40,
    },
}

# === API Key ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ===
def load_api_key_from_file(path: str = API_KEY_FILE) -> str:
    """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ì½ëŠ”ë‹¤."""
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return f.read().strip()
        except Exception:
            return ""
    return ""

def save_api_key_to_file(key: str, path: str = API_KEY_FILE) -> None:
    """API í‚¤ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì €ì¥í•œë‹¤."""
    try:
        with open(path, "w", encoding="utf-8") as f:
            f.write(key.strip())
    except Exception as e:
        print(f"[WARN] API í‚¤ ì €ì¥ ì‹¤íŒ¨: {e}")

# --- UI ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ---
COLOR_BG = "#F8F9FA"
COLOR_WHITE = "#FFFFFF"
COLOR_PRIMARY = "#4A90E2"
COLOR_PRIMARY_HOVER = "#357ABD"
COLOR_SUCCESS = "#28A745"
COLOR_SUCCESS_HOVER = "#218838"
COLOR_DANGER = "#DC3545"
COLOR_DANGER_HOVER = "#C82333"
COLOR_TEXT = "#333333"
COLOR_HEADER = "#E9ECEF"

def get_seoul_now():
    try:
        from pytz import timezone
        return datetime.now(timezone("Asia/Seoul"))
    except:
        return datetime.now()

# ========================================================
# ë°°ì¹˜ ì¡ ê´€ë¦¬ (JSON DB)
# ========================================================
def load_batch_jobs():
    if not os.path.exists(BATCH_JOBS_FILE):
        return []
    try:
        with open(BATCH_JOBS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return []

def save_batch_jobs(jobs):
    try:
        with open(BATCH_JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"[Error] ì¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def upsert_batch_job(batch_id, **kwargs):
    jobs = load_batch_jobs()
    found = False
    now_str = datetime.now().isoformat()
    
    for j in jobs:
        if j["batch_id"] == batch_id:
            if kwargs.get("status") == "completed" and j.get("status") != "completed":
                if "completed_at" not in kwargs:
                    j["completed_at"] = now_str
            j.update(kwargs)
            j["updated_at"] = now_str
            found = True
            break
            
    if not found:
        new_job = {
            "batch_id": batch_id,
            "created_at": now_str,
            "updated_at": now_str,
            "completed_at": "",
            "archived": False,
            **kwargs
        }
        jobs.insert(0, new_job)
    save_batch_jobs(jobs)

def archive_batch_job(batch_ids, archive=True):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    for j in jobs:
        if j["batch_id"] in batch_ids:
            j["archived"] = archive
    save_batch_jobs(jobs)

def hard_delete_batch_job(batch_ids):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    jobs = [j for j in jobs if j["batch_id"] not in batch_ids]
    save_batch_jobs(jobs)

# ========================================================
# GUI Class
# ========================================================
class Stage3BatchGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Stage 3: Batch API Manager (Production Generator) ğŸš€ ìºì‹± ìµœì í™” ë²„ì „")
        self.geometry("1250x1000") # ë†’ì´ ì•½ê°„ ì¦ê°€í•˜ì—¬ ë¡œê·¸ ì°½ ê³µê°„ í™•ë³´
        
        self.api_key_var = tk.StringVar()
        
        # íŒŒì¼ ë³€ìˆ˜
        self.src_file_var = tk.StringVar()
        self.skip_exist_var = tk.BooleanVar(value=True)
        
        # [ì¤‘ìš”] Stage 3 ì „ìš© ì„¤ì • ë³€ìˆ˜
        self.model_var = tk.StringVar(value="gpt-5-mini")
        self.effort_var = tk.StringVar(value="medium")
        self.market_var = tk.StringVar(value="ë„¤ì´ë²„ 50ì")
        self.max_len_var = tk.IntVar(value=50)
        self.num_cand_var = tk.IntVar(value=10)
        self.naming_strategy_var = tk.StringVar(value="í†µí•©í˜•")
        
        # íƒ­ 3 ë³€ìˆ˜
        self.batch_id_var = tk.StringVar()
        
        self._configure_styles()
        self._init_ui()
        self._load_key()

    def _configure_styles(self):
        style = ttk.Style()
        try: style.theme_use('clam')
        except: pass
        
        self.configure(background=COLOR_BG)
        
        style.configure("TFrame", background=COLOR_BG)
        style.configure("TLabel", background=COLOR_BG, foreground=COLOR_TEXT, font=("ë§‘ì€ ê³ ë”•", 10))
        style.configure("Header.TLabel", font=("ë§‘ì€ ê³ ë”•", 11, "bold"), foreground="#444")
        
        style.configure("TLabelframe", background=COLOR_BG, bordercolor="#D0D7DE")
        style.configure("TLabelframe.Label", background=COLOR_BG, foreground="#0056b3", font=("ë§‘ì€ ê³ ë”•", 10, "bold"))

        style.configure("TNotebook", background=COLOR_BG, borderwidth=0)
        style.configure("TNotebook.Tab", background="#E1E4E8", padding=[12, 5], font=("ë§‘ì€ ê³ ë”•", 10))
        style.map("TNotebook.Tab", background=[("selected", COLOR_WHITE)], foreground=[("selected", COLOR_PRIMARY)])
        
        style.configure("Treeview", background=COLOR_WHITE, fieldbackground=COLOR_WHITE, font=("ë§‘ì€ ê³ ë”•", 9), rowheight=28)
        style.configure("Treeview.Heading", background=COLOR_HEADER, foreground="#333", font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        style.map("Treeview", background=[('selected', '#CCE5FF')], foreground=[('selected', 'black')])

        style.configure("TButton", font=("ë§‘ì€ ê³ ë”•", 9), padding=5, borderwidth=1)
        style.configure("Primary.TButton", background=COLOR_PRIMARY, foreground="white", bordercolor=COLOR_PRIMARY)
        style.map("Primary.TButton", background=[("active", COLOR_PRIMARY_HOVER)])
        style.configure("Success.TButton", background=COLOR_SUCCESS, foreground="white", bordercolor=COLOR_SUCCESS)
        style.map("Success.TButton", background=[("active", COLOR_SUCCESS_HOVER)])
        style.configure("Danger.TButton", background=COLOR_DANGER, foreground="white", bordercolor=COLOR_DANGER)
        style.map("Danger.TButton", background=[("active", COLOR_DANGER_HOVER)])

    def _init_ui(self):
        main_container = ttk.Frame(self, padding=15)
        main_container.pack(fill='both', expand=True)
        
        # Gridë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ìœ¨ ì œì–´ (ë¡œê·¸ ì°½ì´ ë” í° ê³µê°„ ì°¨ì§€)
        main_container.grid_rowconfigure(1, weight=1)  # ë©”ì¸ íƒ­ í–‰ (ì‘ì€ ë¹„ì¤‘)
        main_container.grid_rowconfigure(2, weight=2)  # ë¡œê·¸ ì°½ í–‰ (ë” í° ë¹„ì¤‘, 2ë°°)
        main_container.grid_columnconfigure(0, weight=1)

        # 1. ìƒë‹¨ API Key
        f_top = ttk.LabelFrame(main_container, text="ğŸ”‘ API ì„¤ì •", padding=10)
        f_top.grid(row=0, column=0, sticky='ew', pady=(0, 10))
        ttk.Label(f_top, text="Batch API Key:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(side='left')
        entry_key = ttk.Entry(f_top, textvariable=self.api_key_var, show="*", width=50, font=("Consolas", 10))
        entry_key.pack(side='left', padx=10)
        ttk.Button(f_top, text="ì €ì¥", command=self._save_key, style="Primary.TButton").pack(side='left')

        # 2. ë©”ì¸ íƒ­ (ë¹„ìœ¨ ì¡°ì •: ì‘ì€ ê³µê°„)
        self.main_tabs = ttk.Notebook(main_container)
        self.main_tabs.grid(row=1, column=0, sticky='nsew', pady=5)  # grid ì‚¬ìš©, weight=1
        
        self.tab_create = ttk.Frame(self.main_tabs)
        self.tab_manage = ttk.Frame(self.main_tabs) 
        self.tab_merge = ttk.Frame(self.main_tabs)
        
        self.main_tabs.add(self.tab_create, text=" 1. ë°°ì¹˜ ìƒì„± & ì—…ë¡œë“œ ")
        self.main_tabs.add(self.tab_manage, text=" 2. ë°°ì¹˜ ê´€ë¦¬ (ëª©ë¡/ë³‘í•©/ë¦¬í¬íŠ¸) ")
        self.main_tabs.add(self.tab_merge, text=" 3. ê°œë³„ ë³‘í•© (ìˆ˜ë™) ")
        
        self._init_tab_create()
        self._init_tab_manage()
        self._init_tab_merge()
        
        # 3. ë¡œê·¸ (ë” í° ê³µê°„ í• ë‹¹: weight=2)
        f_log = ttk.LabelFrame(main_container, text="ğŸ“‹ ì‹œìŠ¤í…œ ë¡œê·¸", padding=10)
        f_log.grid(row=2, column=0, sticky='nsew', pady=(10, 0))  # grid ì‚¬ìš©, weight=2ë¡œ ë” í° ê³µê°„
        self.log_widget = ScrolledText(f_log, height=25, state='disabled', font=("Consolas", 9), bg="#F1F3F5")  # heightë¥¼ 15ì—ì„œ 25ë¡œ ì¦ê°€
        self.log_widget.pack(fill='both', expand=True)

    def _load_key(self):
        loaded = load_api_key_from_file(API_KEY_FILE)
        if loaded: self.api_key_var.set(loaded)

    def _save_key(self):
        k = self.api_key_var.get().strip()
        if k:
            save_api_key_to_file(k, API_KEY_FILE)
            messagebox.showinfo("ì €ì¥", "API Key ì €ì¥ ì™„ë£Œ")

    def append_log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        self.log_widget.config(state='normal')
        self.log_widget.insert('end', f"[{ts}] {msg}\n")
        self.log_widget.see('end')
        self.log_widget.config(state='disabled')

    # ----------------------------------------------------
    # Tab 1: Create (ìƒì„±)
    # ----------------------------------------------------
    def _init_tab_create(self):
        container = ttk.Frame(self.tab_create, padding=20)
        container.pack(fill='both', expand=True)
        
        # Step 1: íŒŒì¼
        f_file = ttk.LabelFrame(container, text="1. ì‘ì—… ëŒ€ìƒ íŒŒì¼ (ST2_JSON í¬í•¨)", padding=15)
        f_file.pack(fill='x', pady=(0, 15))
        ttk.Entry(f_file, textvariable=self.src_file_var, font=("ë§‘ì€ ê³ ë”•", 10)).pack(side='left', fill='x', expand=True)
        ttk.Button(f_file, text="ğŸ“‚ íŒŒì¼ ì°¾ê¸°", command=self._select_src_file).pack(side='right', padx=5)
        
        # Step 2: Stage 3 ì˜µì…˜
        f_opt = ttk.LabelFrame(container, text="2. Stage 3 ìƒì„± ì˜µì…˜", padding=15)
        f_opt.pack(fill='x', pady=5)

        # ëª¨ë¸ & Effort
        fr1 = ttk.Frame(f_opt)
        fr1.pack(fill='x', pady=5)
        ttk.Label(fr1, text="ëª¨ë¸ (Model):", width=12).pack(side='left')
        models = list(MODEL_PRICING_USD_PER_MTOK.keys()) if MODEL_PRICING_USD_PER_MTOK else ["gpt-5-mini", "gpt-5", "gpt-5-nano"]
        cb_model = ttk.Combobox(fr1, textvariable=self.model_var, values=models, state="readonly", width=20)
        cb_model.pack(side='left', padx=5)
        
        ttk.Label(fr1, text="ì¶”ë¡  ê°•ë„:", width=10).pack(side='left', padx=(20, 5))
        ttk.Combobox(fr1, textvariable=self.effort_var, values=["none", "low", "medium", "high"], state="readonly", width=12).pack(side='left', padx=5)
        
        # ë§ˆì¼“ ì„¤ì •
        fr2 = ttk.Frame(f_opt)
        fr2.pack(fill='x', pady=5)
        ttk.Label(fr2, text="íƒ€ê²Ÿ ë§ˆì¼“:", width=12).pack(side='left')
        markets = ["ë„¤ì´ë²„ 50ì", "ì¿ íŒ¡ 100ì", "ì§€ë§ˆì¼“/ì˜¥ì…˜ 45ì", "ê¸°íƒ€"]
        cb_mk = ttk.Combobox(fr2, textvariable=self.market_var, values=markets, state="readonly", width=18)
        cb_mk.pack(side='left', padx=5)
        cb_mk.bind("<<ComboboxSelected>>", self._on_market_change)
        
        ttk.Label(fr2, text="ìµœëŒ€ ê¸€ì:", width=10).pack(side='left', padx=(20, 5))
        ttk.Spinbox(fr2, from_=10, to=200, textvariable=self.max_len_var, width=10).pack(side='left', padx=5)

        # ì¶œë ¥ ê°œìˆ˜ & ì „ëµ
        fr3 = ttk.Frame(f_opt)
        fr3.pack(fill='x', pady=5)
        ttk.Label(fr3, text="ì¶œë ¥ ê°œìˆ˜:", width=12).pack(side='left')
        ttk.Spinbox(fr3, from_=1, to=30, textvariable=self.num_cand_var, width=10).pack(side='left', padx=5)

        ttk.Label(fr3, text="ëª…ëª… ì „ëµ:", width=10).pack(side='left', padx=(25, 5))
        ttk.Combobox(fr3, textvariable=self.naming_strategy_var, values=["í†µí•©í˜•", "ì˜µì…˜í¬í•¨í˜•"], state="readonly", width=12).pack(side='left', padx=5)
        
        # ì²´í¬ë°•ìŠ¤
        f_row_chk = ttk.Frame(f_opt)
        f_row_chk.pack(fill='x', pady=10)
        ttk.Checkbutton(f_row_chk, text=" ì´ë¯¸ ST3_ê²°ê³¼ê°€ ìˆëŠ” í–‰ ê±´ë„ˆë›°ê¸°", variable=self.skip_exist_var).pack(side='left')
        
        # Step 3: ì‹¤í–‰
        f_step3 = ttk.LabelFrame(container, text="3. ì‹¤í–‰", padding=15)
        f_step3.pack(fill='x', pady=15)

        btn = ttk.Button(f_step3, text="ğŸš€ JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (Start Batch)", command=self._start_create_batch, style="Success.TButton")
        btn.pack(fill='x', ipady=8)
        ttk.Label(container, text="â€» ë°°ì¹˜ APIëŠ” ê²°ê³¼ ìˆ˜ì‹ ê¹Œì§€ ìµœëŒ€ 24ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. (ë¹„ìš© 50% ì ˆê°)", foreground="#666").pack()

    def _on_market_change(self, event=None):
        val = self.market_var.get()
        if "ë„¤ì´ë²„" in val: self.max_len_var.set(50)
        elif "ì¿ íŒ¡" in val: self.max_len_var.set(100)
        elif "ì§€ë§ˆì¼“" in val: self.max_len_var.set(45)

    def _select_src_file(self):
        p = filedialog.askopenfilename(
            title="Stage3 ì—‘ì…€ ì„ íƒ (T2 ë²„ì „ë§Œ ê°€ëŠ¥)",
            filetypes=[("Excel", "*.xlsx;*.xls")]
        )
        if p:
            # T2 í¬í•¨ ì—¬ë¶€ ê²€ì¦
            base_name = os.path.splitext(os.path.basename(p))[0]
            if not re.search(r"_T2_[Ii]\d+", base_name, re.IGNORECASE):
                messagebox.showerror(
                    "ì˜¤ë¥˜", 
                    f"ì´ ë„êµ¬ëŠ” T2 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                    f"ì„ íƒí•œ íŒŒì¼: {os.path.basename(p)}\n"
                    f"íŒŒì¼ëª…ì— '_T2_I*' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
                )
                return
            self.src_file_var.set(p)

    def _start_create_batch(self):
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # T2 í¬í•¨ ì—¬ë¶€ ê²€ì¦
        src = self.src_file_var.get().strip()
        base_name = os.path.splitext(os.path.basename(src))[0]
        if not re.search(r"_T2_[Ii]\d+", base_name, re.IGNORECASE):
            messagebox.showerror(
                "ì˜¤ë¥˜", 
                f"ì´ ë„êµ¬ëŠ” T2 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                f"ì„ íƒí•œ íŒŒì¼: {os.path.basename(src)}\n"
                f"íŒŒì¼ëª…ì— '_T2_I*' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
            )
            return
        
        t = threading.Thread(target=self._run_create_batch)
        t.daemon = True
        t.start()

    def _run_create_batch(self):
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        jsonl_path = None  # ì—ëŸ¬ ì²˜ë¦¬ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë¯¸ë¦¬ ì„ ì–¸
        
        # Stage3SettingsëŠ” model_nameê³¼ reasoning_effortë¥¼ í¬í•¨í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë³„ë„ ê´€ë¦¬
        model_name = self.model_var.get().strip() or "gpt-5-mini"
        reasoning_effort = self.effort_var.get().strip() or "medium"
        
        settings = Stage3Settings(
            market=self.market_var.get(),
            max_len=self.max_len_var.get(),
            num_candidates=self.num_cand_var.get(),
            naming_strategy=self.naming_strategy_var.get()
        )
        
        try:
            client = OpenAI(api_key=key)
            self.append_log(f"ì—‘ì…€ ë¡œë“œ ì¤‘... {os.path.basename(src)}")
            df = pd.read_excel(src)
            
            if "ST2_JSON" not in df.columns:
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼(ST2_JSON)ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. Stage 2ë¥¼ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.")
            
            self.append_log(f"ì„¤ì •: {settings.market} / {settings.max_len}ì / {model_name}")
            
            # ìºì‹± ëª¨ë“œ í™•ì¸ ë° ë¡œê·¸
            if CACHE_MODE_CORE:
                self.append_log(f"[INFO] ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ëª¨ë“œ í™œì„±í™” (stage3_core_Casche.py)")
                # System prompt í† í° ìˆ˜ í™•ì¸ (ì„ íƒì )
                try:
                    import tiktoken
                    from stage3_core_Casche import STAGE3_SYSTEM_PROMPT
                    enc = tiktoken.encoding_for_model('gpt-4o')
                    token_count = len(enc.encode(STAGE3_SYSTEM_PROMPT))
                    status = "âœ… ì¶©ë¶„" if token_count > 1024 else "âš ï¸ ë¶€ì¡±"
                    self.append_log(f"[INFO] System prompt í† í° ìˆ˜: {token_count} í† í° ({status}, ê¸°ì¤€: 1024 í† í°)")
                except ImportError:
                    # tiktokenì´ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ì¡°ìš©íˆ ê±´ë„ˆëœ€ (ì„ íƒì  ê¸°ëŠ¥)
                    pass
                except Exception as e:
                    # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ë””ë²„ê¹…ìš©ìœ¼ë¡œë§Œ ë¡œê·¸ ì¶œë ¥
                    self.append_log(f"[DEBUG] System prompt í† í° ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
            else:
                self.append_log(f"[INFO] âš ï¸ ì¼ë°˜ ëª¨ë“œ (stage3_core.py) - ìºì‹± ìµœì í™” ë¯¸ì ìš©")

            # ë¨¼ì € ì „ì²´ ëŒ€ìƒ ìš”ì²­ ìˆ˜ë¥¼ ê³„ì‚° (ë²„í‚· ìˆ˜ ê²°ì •ìš©)
            target_rows = 0
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get() and "ST3_ê²°ê³¼ìƒí’ˆëª…" in df.columns:
                    val = str(row.get("ST3_ê²°ê³¼ìƒí’ˆëª…", "")).strip()
                    if val and val != "nan":
                        continue
                # ST2_JSON í™•ì¸
                st2_json = safe_str(row.get("ST2_JSON", ""))
                if not st2_json or st2_json.strip().lower() in ("", "nan", "none", "null"):
                    continue
                target_rows += 1

            # ë²„í‚· ìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚° (ëª¨ë“  ìš”ì²­ì— ë™ì¼í•˜ê²Œ ì ìš©)
            if CACHE_MODE_CORE and target_rows > 0:
                # [í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ì „ëµ - ìˆ˜ì •ë¨]
                # 
                # [ë¬¸ì œ ë¶„ì„ (ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼)]
                # - ë²„í‚· ë¶„ì‚° ì‹œë‚˜ë¦¬ì˜¤: 486ê°œ ìš”ì²­ì„ 49ê°œ ë²„í‚·(b00~b48)ìœ¼ë¡œ ë¶„ì‚°
                # - ê²°ê³¼: ìºì‹œ íˆíŠ¸ìœ¨ 10.08% (49ê±´/486ê±´), ëŒ€ë¶€ë¶„ì˜ ë²„í‚·ì€ íˆíŠ¸ 0ê±´
                # - ì›ì¸: prompt_cache_keyê°€ ë¶„ì‚°ë˜ë©´ ìºì‹œ ë¼ìš°íŒ…/ì €ì¥ í’€ë„ ë¶„ì‚°ë˜ì–´
                #   Batch ì²˜ë¦¬ íŠ¹ì„±ìƒ(ë³‘ë ¬/ë¶„ì‚° ì‹¤í–‰) ê°™ì€ í‚¤ë¼ë¦¬ ì—°ì† ì¬ì‚¬ìš©ì´ ì˜ ì•ˆ ê±¸ë¦¼
                #
                # [í•´ê²°ì±…: í‚¤ ê³ ì •]
                # - prompt_cache_keyë¥¼ í•˜ë‚˜ë¡œ ê³ ì •: "stage3_v1" (b00~b48ë¡œ ìª¼ê°œì§€ ì•Šê¸°)
                # - íš¨ê³¼: ëª¨ë“  ìš”ì²­ì´ ê°™ì€ ìºì‹œ í’€ì„ ê³µìœ í•˜ì—¬ íˆíŠ¸ìœ¨ ëŒ€í­ í–¥ìƒ ì˜ˆìƒ
                # - Batch APIëŠ” 24ì‹œê°„ì— ê±¸ì³ ì²˜ë¦¬ë˜ë¯€ë¡œ overflow ìš°ë ¤ëŠ” ë‚®ìŒ
                # - ì‹¤ì œ í…ŒìŠ¤íŠ¸ì—ì„œ ê°™ì€ í‚¤ë¥¼ 10~17ë²ˆì”© ì“°ëŠ” ê²½ìš°ë„ ìˆì—ˆì§€ë§Œ íˆíŠ¸ìœ¨ì´ ë‚®ì•˜ë˜ ì´ìœ ëŠ”
                #   í‚¤ ë¶„ì‚° + ë°°ì¹˜ ë‚´ë¶€ ìŠ¤ì¼€ì¤„ë§/ë³‘ë ¬ ì²˜ë¦¬ ë•Œë¬¸
                #
                # [ì°¸ê³ : OpenAI ê³µì‹ ë¬¸ì„œ]
                # - ì¼ë°˜ API(ë™ê¸° ìš”ì²­): ê°™ì€ prefix + prompt_cache_key ì¡°í•©ì´ ë¶„ë‹¹ ì•½ 15ê±´ì„ ì´ˆê³¼í•˜ë©´
                #   ì¼ë¶€ê°€ ì¶”ê°€ ë¨¸ì‹ ìœ¼ë¡œ overflowë˜ì–´ ìºì‹œ íš¨ìœ¨ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
                #   (ì°¸ê³ : https://platform.openai.com/docs/guides/prompt-caching)
                # - Batch API: ê³µì‹ ë¬¸ì„œì— prompt_cache_key ë²„í‚· ë¶„ë°° ê¸°ì¤€ì´ ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŒ
                #   â†’ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‚¤ ê³ ì • ì „ëµ ì±„íƒ
                #
                # [êµ¬í˜„]
                # - PROMPT_CACHE_BUCKETS = 1ë¡œ ê³ ì •í•˜ì—¬ ëª¨ë“  ìš”ì²­ì´ ë™ì¼í•œ prompt_cache_key ì‚¬ìš©
                # - ê°€ëŠ¥í•˜ë©´ ê°™ì€ í‚¤ë¼ë¦¬ ìš”ì²­ì„ ë¬¶ì–´ì„œ(ì—°ì†ë˜ê²Œ) ë°°ì¹˜ë¡œ ë„£ëŠ” ê²ƒì´ ì´ìƒì ì´ì§€ë§Œ,
                #   Batch APIëŠ” ìì²´ì ìœ¼ë¡œ ìµœì í™”í•˜ë¯€ë¡œ ë‹¨ìˆœíˆ í‚¤ë¥¼ ê³ ì •í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ íš¨ê³¼ì 
                PROMPT_CACHE_BUCKETS = 1
                
                self.append_log(f"[INFO] í”„ë¡¬í”„íŠ¸ ìºì‹±: í‚¤ ê³ ì • ì „ëµ ì‚¬ìš© (ëª¨ë“  ìš”ì²­ì´ 'stage3_v1' í‚¤ ê³µìœ )")
                self.append_log(f"[INFO] ì˜ˆìƒ ìš”ì²­ ìˆ˜: {target_rows}ê°œ, ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ ì˜ˆìƒ")
            else:
                PROMPT_CACHE_BUCKETS = 1

            jsonl_lines = []
            skipped_cnt = 0
            seen_custom_ids = set()
            duplicate_count = 0
            
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get() and "ST3_ê²°ê³¼ìƒí’ˆëª…" in df.columns:
                    val = str(row.get("ST3_ê²°ê³¼ìƒí’ˆëª…", "")).strip()
                    if val and val != "nan":
                        continue
                
                try:
                    req = build_stage3_request_from_row(row, settings)
                except Exception:
                    skipped_cnt += 1
                    continue

                # ìºì‹± ìµœì í™”: system/user í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬
                if CACHE_MODE_CORE:
                    system_prompt = safe_str(getattr(req, "system_prompt", ""))
                    user_prompt = safe_str(getattr(req, "user_prompt", ""))
                    
                    if not system_prompt or not user_prompt:
                        skipped_cnt += 1
                        continue
                    
                    # System ë©”ì‹œì§€ (í…ìŠ¤íŠ¸ë§Œ, ì •ì )
                    system_content = [{"type": "input_text", "text": system_prompt}]
                    
                    # User ë©”ì‹œì§€ (í…ìŠ¤íŠ¸ë§Œ, ë™ì )
                    user_content = [{"type": "input_text", "text": user_prompt}]
                    
                    body = {
                        "model": model_name,
                        "input": [
                            {
                                "role": "system",
                                "content": system_content,
                            },
                            {
                                "role": "user",
                                "content": user_content,
                            }
                        ],
                    }
                else:
                    # ì¼ë°˜ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ ìœ ì§€
                    prompt = safe_str(getattr(req, "prompt", ""))
                    if not prompt:
                        skipped_cnt += 1
                        continue
                    
                    body = {
                        "model": model_name,
                        "messages": [{"role": "user", "content": prompt}],
                    }
                
                # reasoning.effort (Responses API)
                is_reasoning = any(x in model_name for x in ["gpt-5", "o1", "o3"])
                if is_reasoning and reasoning_effort != "none":
                    if CACHE_MODE_CORE:
                        body["reasoning"] = {"effort": reasoning_effort}
                    else:
                        body["reasoning_effort"] = reasoning_effort
                elif not is_reasoning:
                    if not CACHE_MODE_CORE:
                        body["temperature"] = 0.7

                custom_id = f"row-{idx}"
                
                # ì¤‘ë³µ custom_id ì²´í¬
                if custom_id in seen_custom_ids:
                    duplicate_count += 1
                    continue
                seen_custom_ids.add(custom_id)

                # Prompt Caching ìµœì í™” (ìºì‹± ëª¨ë“œì¼ ë•Œë§Œ)
                if CACHE_MODE_CORE:
                    # prompt_cache_key: í‚¤ ê³ ì • ì „ëµ (ëª¨ë“  ìš”ì²­ì´ ë™ì¼í•œ í‚¤ ì‚¬ìš©)
                    # ë²„í‚· ë¶„ì‚° ëŒ€ì‹  í‚¤ë¥¼ í•˜ë‚˜ë¡œ ê³ ì •í•˜ì—¬ ìºì‹œ íˆíŠ¸ìœ¨ ìµœëŒ€í™”
                    body["prompt_cache_key"] = "stage3_v1"
                    
                    # prompt_cache_retention: ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                    # Extended retention ì§€ì› ëª¨ë¸: gpt-5.1, gpt-5.1-codex, gpt-5.1-codex-mini, gpt-5.1-chat-latest, gpt-5, gpt-5-codex, gpt-4.1
                    # gpt-5-mini, gpt-5-nanoëŠ” prompt_cache_retention íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
                    if model_name in ["gpt-5.1", "gpt-5.1-codex", "gpt-5.1-codex-mini", "gpt-5.1-chat-latest", "gpt-5", "gpt-5-codex", "gpt-4.1"]:
                        body["prompt_cache_retention"] = "extended"  # 24ì‹œê°„ retention
                    elif model_name not in ["gpt-5-mini", "gpt-5-nano"]:
                        # ê¸°íƒ€ ëª¨ë¸ì€ in-memory ì‚¬ìš© (5~10ë¶„ inactivity, ìµœëŒ€ 1ì‹œê°„)
                        body["prompt_cache_retention"] = "in_memory"
                    
                    # Responses API ì‚¬ìš© (system/user role)
                    url = "/v1/responses"
                else:
                    # ì¼ë°˜ ëª¨ë“œ: Chat Completions API ì‚¬ìš©
                    url = "/v1/chat/completions"

                request_obj = {
                    "custom_id": custom_id,
                    "method": "POST",
                    "url": url,
                    "body": body
                }
                
                jsonl_lines.append(json.dumps(request_obj, ensure_ascii=False))
            
            if duplicate_count > 0:
                self.append_log(f"[WARN] âš ï¸ ì¤‘ë³µ ìš”ì²­ {duplicate_count}ê°œê°€ ê°ì§€ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if not jsonl_lines:
                self.append_log("ìƒì„±í•  ìš”ì²­ ì—†ìŒ.")
                return

            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_stage3_batch_input.jsonl"
            with open(jsonl_path, "w", encoding="utf-8") as f:
                f.write("\n".join(jsonl_lines))
            
            self.append_log(f"JSONL ìƒì„± ì™„ë£Œ: {len(jsonl_lines)}ê±´ (ìŠ¤í‚µ {skipped_cnt}ê±´)")
            self.append_log(f"[INFO] JSONL íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {jsonl_path}")
            
            # íŒŒì¼ í¬ê¸° ë° ìš”ì²­ ìˆ˜ í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            info = {
                'num_requests': len(jsonl_lines),
                'file_size_mb': jsonl_size_mb
            }
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ìš”ì²­ ìˆ˜: {info['num_requests']}ê°œ")
            
            # ì²­í¬ ë¶„í•  ê¸°ì¤€ (OpenAI Batch API ì œí•œ: 200MB)
            # ì‹¤ì œ ë¶„í• ì€ íŒŒì¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œë§Œ ìˆ˜í–‰ë˜ë¯€ë¡œ, íŒŒì¼ í¬ê¸°ë§Œ ì²´í¬
            MAX_FILE_SIZE_MB = 190
            
            # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (íŒŒì¼ í¬ê¸° ê¸°ì¤€)
            estimated_chunks = max(1, int(jsonl_size_mb / MAX_FILE_SIZE_MB) + 1) if jsonl_size_mb > MAX_FILE_SIZE_MB else 1
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸° ({jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB)ë¡œ ì¸í•´ ë¶„í•  ì²˜ë¦¬í•©ë‹ˆë‹¤... (OpenAI ì œí•œ: 200MB)")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client_with_timeout = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch_ids = self._create_batch_chunks(
                    client=client_with_timeout,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    effort=reasoning_effort,
                    settings=settings,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=999999,  # ìš”ì²­ ìˆ˜ ì œí•œì€ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í° ê°’ìœ¼ë¡œ ì„¤ì •
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}")
            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    reasoning_effort=reasoning_effort,
                    settings=settings,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # 3) ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src,
                    jsonl_path=jsonl_path,
                    model=model_name,
                    effort=reasoning_effort,
                    status=batch.status,
                    output_file_id=None,
                    market=settings.market,
                    strategy=settings.naming_strategy
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage3(Text) ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡: T3 (ì§„í–‰ì¤‘) (img ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                try:
                    root_name = get_root_filename(src)
                    JobManager.update_status(root_name, text_msg="T3 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T3 (ì§„í–‰ì¤‘)")
                except Exception:
                    # ëŸ°ì²˜ë‚˜ job_history.json ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ìš©íˆ ë¬´ì‹œ
                    pass
                messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {batch_id}")
            
            self._load_jobs_all()
            self._load_archive_list()

        except Exception as e:
            error_str = str(e)
            error_lower = error_str.lower()
            
            # ê²°ì œ í•œë„ ì´ˆê³¼ ì—ëŸ¬ ê°ì§€
            if "billing_hard_limit_reached" in error_lower or "billing" in error_lower and "limit" in error_lower:
                self.append_log(f"âŒ [ê²°ì œ í•œë„ ì´ˆê³¼] OpenAI ê³„ì •ì˜ ê²°ì œ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                if jsonl_path and os.path.exists(jsonl_path):
                    self.append_log(f"   ğŸ’¾ ìƒì„±ëœ JSONL íŒŒì¼: {jsonl_path}")
                    self.append_log(f"      â†’ ê²°ì œ í•œë„ í•´ê²° í›„ ì´ íŒŒì¼ì„ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                self.append_log(f"   â†’ í•´ê²° ë°©ë²•:")
                self.append_log(f"      1. OpenAI ëŒ€ì‹œë³´ë“œ(https://platform.openai.com/account/billing)ì—ì„œ ê²°ì œ í•œë„ í™•ì¸")
                self.append_log(f"      2. ê²°ì œ í•œë„ ì¦ê°€ ë˜ëŠ” ê²°ì œ ì •ë³´ ì—…ë°ì´íŠ¸")
                self.append_log(f"      3. ë˜ëŠ” ë” ì‘ì€ ë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬")
                self.append_log(f"   ì›ë³¸ ì—ëŸ¬: {error_str}")
                
                msg = "OpenAI ê³„ì •ì˜ ê²°ì œ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.\n\n"
                if jsonl_path and os.path.exists(jsonl_path):
                    msg += f"âœ… JSONL íŒŒì¼ì€ ì´ë¯¸ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{os.path.basename(jsonl_path)}\n"
                    msg += "   ê²°ì œ í•œë„ í•´ê²° í›„ ì¬ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n"
                msg += "í•´ê²° ë°©ë²•:\n"
                msg += "1. OpenAI ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ì œ í•œë„ í™•ì¸\n"
                msg += "   (https://platform.openai.com/account/billing)\n"
                msg += "2. ê²°ì œ í•œë„ ì¦ê°€ ë˜ëŠ” ê²°ì œ ì •ë³´ ì—…ë°ì´íŠ¸\n"
                msg += "3. ë” ì‘ì€ ë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬\n\n"
                msg += f"ìƒì„¸ ì—ëŸ¬: {error_str[:200]}"
                
                messagebox.showerror("ê²°ì œ í•œë„ ì´ˆê³¼", msg)
            else:
                self.append_log(f"âŒ ì—ëŸ¬: {error_str}")
                if jsonl_path and os.path.exists(jsonl_path):
                    self.append_log(f"   ğŸ’¾ ìƒì„±ëœ JSONL íŒŒì¼: {jsonl_path}")
                messagebox.showerror("ì—ëŸ¬", error_str)
    
    def _create_batch_from_jsonl(self, client, jsonl_path, excel_path, model_name, reasoning_effort, settings):
        """JSONL íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë°°ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        if not os.path.exists(jsonl_path):
            raise FileNotFoundError(f"ì…ë ¥ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")

        with open(jsonl_path, "rb") as f:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
            up_file = client.files.create(file=f, purpose="batch", timeout=600)

        # ìºì‹± ëª¨ë“œì— ë”°ë¼ endpoint ê²°ì •
        endpoint = "/v1/responses" if CACHE_MODE_CORE else "/v1/chat/completions"

        batch = client.batches.create(
            input_file_id=up_file.id,
            endpoint=endpoint,
            completion_window="24h"
        )
        return batch
    
    def _create_batch_chunks(self, client, jsonl_path, excel_path, model_name, effort, settings, max_size_mb=180, max_requests=999999):
        """
        í° JSONL íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ batch_group_idë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        """
        import uuid
        
        # ë°°ì¹˜ ê·¸ë£¹ ID ìƒì„± (ê°™ì€ ì—‘ì…€ì—ì„œ ë¶„í• ëœ ë°°ì¹˜ë“¤ì„ ë¬¶ìŒ)
        batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
        
        # JSONL íŒŒì¼ ì½ê¸°
        requests = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    requests.append(json.loads(line))
        
        total_requests = len(requests)
        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (ìš©ëŸ‰ ê¸°ì¤€ë§Œ ì‚¬ìš©, ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
        original_file_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
        estimated_total_chunks = max(1, int(original_file_size_mb / max_size_mb) + 1)
        
        # ì‹¤ì œë¡œ ë¶„í• ì´ í•„ìš”í•œì§€ í™•ì¸ (íŒŒì¼ í¬ê¸°ê°€ ì œí•œë³´ë‹¤ ì‘ìœ¼ë©´ 1ê°œ ì²­í¬ë¡œ ì²˜ë¦¬)
        if original_file_size_mb <= max_size_mb:
            # íŒŒì¼ í¬ê¸°ê°€ ì œí•œë³´ë‹¤ ì‘ìœ¼ë©´ ë¶„í•  ë¶ˆí•„ìš”, ë‹¨ì¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            self.append_log(f"[INFO] íŒŒì¼ í¬ê¸° ({original_file_size_mb:.2f}MB â‰¤ {max_size_mb}MB)ë¡œ ë‹¨ì¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ìš”ì²­ ìˆ˜: {total_requests}ê°œ)")
            estimated_total_chunks = 1
        else:
            self.append_log(f"[INFO] ì´ {total_requests}ê°œ ìš”ì²­ì„ ë¶„í• í•©ë‹ˆë‹¤... (ì˜ˆìƒ: ì•½ {estimated_total_chunks}ê°œ ì²­í¬, ê·¸ë£¹ ID: {batch_group_id})")
        
        # base ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜ (ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹œ ì‚¬ìš©)
        base, ext = os.path.splitext(jsonl_path)
        
        batch_ids = []
        chunk_num = 0
        chunk_files_created = []  # ìƒì„±ëœ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì •ë¦¬ìš©)
        failed_chunk_files = []  # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì¬ì‹œë„ìš©)
        
        i = 0
        while i < total_requests:
            chunk_num += 1
            chunk_requests = []
            chunk_size_bytes = 0  # ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì •í™•íˆ ê³„ì‚°
            
            # ì²­í¬ ìƒì„± (ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ , ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
            while i < total_requests:
                req_json = json.dumps(requests[i], ensure_ascii=False)
                req_size_bytes = len(req_json.encode('utf-8')) + 1  # +1 for newline
                
                # ë‹¤ìŒ ìš”ì²­ì„ ì¶”ê°€í•˜ë©´ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ ë§ˆì§„ 5% í¬í•¨)
                if chunk_size_bytes + req_size_bytes > max_size_mb * 1024 * 1024 * 0.95:
                    break
                
                chunk_requests.append(requests[i])
                chunk_size_bytes += req_size_bytes
                i += 1
            
            if not chunk_requests:
                break
            
            # ì²­í¬ JSONL íŒŒì¼ ìƒì„±
            chunk_jsonl_path = f"{base}_chunk{chunk_num:03d}{ext}"
            chunk_files_created.append(chunk_jsonl_path)
            
            with open(chunk_jsonl_path, "w", encoding="utf-8") as f:
                for req in chunk_requests:
                    f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            chunk_size_mb = os.path.getsize(chunk_jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] ì²­í¬ {chunk_num}: {len(chunk_requests)}ê°œ ìš”ì²­, {chunk_size_mb:.2f} MB")
            
            # ë°°ì¹˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 3
            retry_count = 0
            batch_created = False
            
            while retry_count < max_retries and not batch_created:
                try:
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹œë„ ì¤‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                    batch = self._create_batch_from_jsonl(
                        client=client,
                        jsonl_path=chunk_jsonl_path,
                        excel_path=excel_path,
                        model_name=model_name,
                        reasoning_effort=effort,
                        settings=settings,
                    )
                    
                    batch_id = batch.id
                    batch_ids.append(batch_id)
                    self.append_log(f"âœ… ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_id}")
                    
                    batch_created = True
                    
                    # ì‘ì—… ì´ë ¥ ê¸°ë¡ (ê·¸ë£¹ ì •ë³´ í¬í•¨)
                    upsert_batch_job(
                        batch_id=batch_id,
                        src_excel=excel_path,
                        jsonl_path=chunk_jsonl_path,
                        model=model_name,
                        effort=effort,
                        status=batch.status,
                        output_file_id=None,
                        batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                        chunk_index=chunk_num,  # ì²­í¬ ë²ˆí˜¸
                        total_chunks=chunk_num,  # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì²­í¬ ìˆ˜ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨)
                        market=settings.market,
                        strategy=settings.naming_strategy
                    )
                except Exception as e:
                    error_str = str(e).lower()
                    is_token_limit_error = "enqueued token limit" in error_str or "token limit reached" in error_str
                    
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                        self.append_log(f"âš ï¸ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                        if is_token_limit_error:
                            self.append_log(f"[WARN] í† í° ì œí•œ ì˜¤ë¥˜ ê°ì§€. ë” ê¸´ ëŒ€ê¸° ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            wait_time = max(wait_time, 30)  # í† í° ì œí•œ ì˜¤ë¥˜ëŠ” ìµœì†Œ 30ì´ˆ ëŒ€ê¸°
                        self.append_log(f"[INFO] {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        import time
                        time.sleep(wait_time)
                    else:
                        self.append_log(f"âŒ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}")
                        if is_token_limit_error:
                            self.append_log(f"[INFO] í† í° ì œí•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì¼ë¶€ ë°°ì¹˜ê°€ ì™„ë£Œëœ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                            self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼: {chunk_jsonl_path}")
                            self.append_log(f"[INFO] ë‚˜ì¤‘ì— '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì´ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        import traceback
                        self.append_log(traceback.format_exc())
                        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìœ ì§€ (ìˆ˜ë™ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                        failed_chunk_files.append({
                            "chunk_num": chunk_num,
                            "chunk_file": chunk_jsonl_path,
                            "error": str(e),
                            "is_token_limit": is_token_limit_error,
                            "excel_path": excel_path,
                            "model_name": model_name,
                            "effort": effort,
                            "batch_group_id": batch_group_id,
                            "settings": asdict(settings) if hasattr(settings, '__dict__') else (settings if isinstance(settings, dict) else {}),
                        })
                        # í•˜ì§€ë§Œ ë°°ì¹˜ IDëŠ” ì¶”ê°€ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ batch_idsì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ
        
        # ëª¨ë“  ì²­í¬ ìƒì„± ì™„ë£Œ í›„, total_chunksë¥¼ ì‹¤ì œ ìƒì„±ëœ ë°°ì¹˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        actual_total_chunks = len(batch_ids)
        if actual_total_chunks > 0:
            self.append_log(f"[INFO] ì´ {actual_total_chunks}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ. ì‘ì—… ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
            jobs = load_batch_jobs()
            updated_count = 0
            for j in jobs:
                if j.get("batch_group_id") == batch_group_id:
                    j["total_chunks"] = actual_total_chunks
                    updated_count += 1
            if updated_count > 0:
                save_batch_jobs(jobs)
                self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {actual_total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.append_log(f"âš ï¸ ìƒì„±ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì„±ê³µí•œ ë°°ì¹˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²­í¬ íŒŒì¼ ì •ë¦¬ ì˜µì…˜ ì œê³µ (í˜„ì¬ëŠ” ìœ ì§€)
        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìˆ˜ë™ ì¬ì‹œë„ë¥¼ ìœ„í•´ ìœ ì§€
        if actual_total_chunks < chunk_num:
            failed_chunks = chunk_num - actual_total_chunks
            self.append_log(f"âš ï¸ {failed_chunks}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²­í¬ íŒŒì¼ì€ ìœ ì§€ë©ë‹ˆë‹¤.")
            
            # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ì„ ëª…í™•íˆ í‘œì‹œ
            if failed_chunk_files:
                self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡:")
                for failed_info in failed_chunk_files:
                    self.append_log(f"  - ì²­í¬ {failed_info['chunk_num']}: {os.path.basename(failed_info['chunk_file'])}")
                    if failed_info['is_token_limit']:
                        self.append_log(f"    â†’ í† í° ì œí•œ ì˜¤ë¥˜. ì¼ë¶€ ë°°ì¹˜ ì™„ë£Œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                
                # ì‹¤íŒ¨ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                failed_info_path = f"{base}_failed_chunks.json"
                try:
                    with open(failed_info_path, "w", encoding="utf-8") as f:
                        json.dump(failed_chunk_files, f, ensure_ascii=False, indent=2)
                    self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ ì €ì¥: {os.path.basename(failed_info_path)}")
                    self.append_log(f"[INFO] ë‚˜ì¤‘ì— ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤íŒ¨í•œ ì²­í¬ë§Œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    # GUIì— ìë™ìœ¼ë¡œ ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ì•Œë¦¼
                    self.after(0, lambda: self._handle_failed_chunks(failed_info_path, failed_chunk_files))
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage3(Text) ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡: T3 (ì§„í–‰ì¤‘)
        try:
            root_name = get_root_filename(excel_path)
            JobManager.update_status(root_name, text_msg="T3 (ì§„í–‰ì¤‘)")
            self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T3 (ì§„í–‰ì¤‘)")
        except Exception:
            pass
        
        return batch_ids

    # ----------------------------------------------------
    # Tab 2: Manage (List & Trash)
    # ----------------------------------------------------
    def _init_tab_manage(self):
        container = ttk.Frame(self.tab_manage, padding=10)
        container.pack(fill='both', expand=True)

        sub_tabs = ttk.Notebook(container)
        sub_tabs.pack(fill='both', expand=True)
        
        self.sub_active = ttk.Frame(sub_tabs, padding=10)
        self.sub_archive = ttk.Frame(sub_tabs, padding=10)
        
        sub_tabs.add(self.sub_active, text=" â–¶ ì§„í–‰ì¤‘ / ì™„ë£Œ (Active) ")
        sub_tabs.add(self.sub_archive, text=" ğŸ—‘ íœ´ì§€í†µ (Archive) ")
        
        # --- Active Tab UI ---
        f_ctrl = ttk.Frame(self.sub_active)
        f_ctrl.pack(fill='x', pady=(0, 10))
        
        ttk.Button(f_ctrl, text="ğŸ”„ ì„ íƒ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ“¥ ì„ íƒ ì¼ê´„ ë³‘í•©", command=self._merge_selected, style="Primary.TButton").pack(side='left', padx=10)
        ttk.Button(f_ctrl, text="ğŸ“Š ì„ íƒ ì¼ê´„ í†µí•© ë¦¬í¬íŠ¸", command=self._report_selected_unified, style="Success.TButton").pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ—‘ íœ´ì§€í†µ ì´ë™", command=self._archive_selected, style="Danger.TButton").pack(side='right', padx=2)
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ë²„íŠ¼ ì¶”ê°€
        f_group_ctrl = ttk.Frame(self.sub_active)
        f_group_ctrl.pack(fill='x', pady=(0, 5))
        ttk.Label(f_group_ctrl, text="ğŸ’¡ ê·¸ë£¹ í—¤ë”ë¥¼ ë”ë¸”í´ë¦­í•˜ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°ê°€ ë©ë‹ˆë‹¤.", 
                 font=("ë§‘ì€ ê³ ë”•", 8), foreground="#666").pack(side='left', padx=5)
        ttk.Button(f_group_ctrl, text="ğŸ“‚ ëª¨ë“  ê·¸ë£¹ í¼ì¹˜ê¸°", command=lambda: self._expand_all_groups(self.tree_active)).pack(side='right', padx=2)
        ttk.Button(f_group_ctrl, text="ğŸ“ ëª¨ë“  ê·¸ë£¹ ì ‘ê¸°", command=lambda: self._collapse_all_groups(self.tree_active)).pack(side='right', padx=2)
        
        # [NEW] Effort ì»¬ëŸ¼ ë° ê·¸ë£¹ ì»¬ëŸ¼ ì¶”ê°€, ì—‘ì…€ëª…ê³¼ ë©”ëª¨ ì»¬ëŸ¼ ì¶”ê°€
        cols = ("batch_id", "excel_name", "memo", "status", "created", "completed", "model", "effort", "market", "counts", "group")
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš© (íŠ¸ë¦¬ ì•„ì´ì½˜ + ì»¬ëŸ¼ í—¤ë”)
        self.tree_active = ttk.Treeview(self.sub_active, columns=cols, show='tree headings', height=18, selectmode='extended')
        
        self.tree_active.tag_configure('odd', background=COLOR_WHITE)
        self.tree_active.tag_configure('even', background='#F2F7FF')
        self.tree_active.tag_configure('group', background='#E8F4FD')  # ê·¸ë£¹ ë°°ì¹˜ ê°•ì¡°
        self.tree_active.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))  # ê·¸ë£¹ í—¤ë” ê°•ì¡°
        
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_active.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_active.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_active.heading("memo", text="ë©”ëª¨")
        self.tree_active.heading("status", text="ìƒíƒœ")
        self.tree_active.heading("created", text="ìƒì„±ì¼")
        self.tree_active.heading("completed", text="ì™„ë£Œì¼")
        self.tree_active.heading("model", text="ëª¨ë¸")
        self.tree_active.heading("effort", text="Effort")
        self.tree_active.heading("market", text="ë§ˆì¼“")
        self.tree_active.heading("counts", text="ìš”ì²­ìˆ˜")
        self.tree_active.heading("group", text="ê·¸ë£¹")
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •: ê·¸ë£¹ëª…ì´ ê¸¸ì–´ì„œ íŠ¸ë¦¬ ì»¬ëŸ¼ í™•ëŒ€
        self.tree_active.column("#0", width=350, anchor="w")  # íŠ¸ë¦¬ ì»¬ëŸ¼ (ê·¸ë£¹ëª… í‘œì‹œ)
        self.tree_active.column("batch_id", width=180)
        self.tree_active.column("excel_name", width=200, anchor="w")  # ì—‘ì…€ íŒŒì¼ëª…
        self.tree_active.column("memo", width=150, anchor="w")  # ë©”ëª¨
        self.tree_active.column("status", width=80, anchor="center")
        self.tree_active.column("created", width=120, anchor="center")
        self.tree_active.column("completed", width=120, anchor="center")
        self.tree_active.column("model", width=80, anchor="center")
        self.tree_active.column("effort", width=60, anchor="center")
        self.tree_active.column("market", width=80, anchor="center")
        self.tree_active.column("counts", width=80, anchor="center")
        self.tree_active.column("group", width=80, anchor="center")
        
        self.tree_active.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ìš°í´ë¦­ ë©”ë‰´
        self.menu_active = Menu(self, tearoff=0)
        self.menu_active.add_command(label="ìƒíƒœ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ê²°ê³¼ ë³‘í•©", command=self._merge_selected)
        self.menu_active.add_command(label="í†µí•© ë¦¬í¬íŠ¸ ìƒì„±", command=self._report_selected_unified)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="íœ´ì§€í†µìœ¼ë¡œ ì´ë™", command=self._archive_selected)
        
        self.tree_active.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_active, self.menu_active))
        self.tree_active.bind("<Double-1>", self._on_tree_double_click)

        # --- Archive Tab UI ---
        f_arch_ctrl = ttk.Frame(self.sub_archive)
        f_arch_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_arch_ctrl, text="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected, style="Primary.TButton").pack(side='left')
        ttk.Button(f_arch_ctrl, text="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected, style="Danger.TButton").pack(side='right')
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ë²„íŠ¼ ì¶”ê°€
        f_arch_group_ctrl = ttk.Frame(self.sub_archive)
        f_arch_group_ctrl.pack(fill='x', pady=(0, 5))
        ttk.Label(f_arch_group_ctrl, text="ğŸ’¡ ê·¸ë£¹ í—¤ë”ë¥¼ ë”ë¸”í´ë¦­í•˜ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°ê°€ ë©ë‹ˆë‹¤.", 
                 font=("ë§‘ì€ ê³ ë”•", 8), foreground="#666").pack(side='left', padx=5)
        ttk.Button(f_arch_group_ctrl, text="ğŸ“‚ ëª¨ë“  ê·¸ë£¹ í¼ì¹˜ê¸°", command=lambda: self._expand_all_groups(self.tree_arch)).pack(side='right', padx=2)
        ttk.Button(f_arch_group_ctrl, text="ğŸ“ ëª¨ë“  ê·¸ë£¹ ì ‘ê¸°", command=lambda: self._collapse_all_groups(self.tree_arch)).pack(side='right', padx=2)
        
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš©
        self.tree_arch = ttk.Treeview(self.sub_archive, columns=cols, show='tree headings', height=18, selectmode='extended')
        self.tree_arch.tag_configure('odd', background=COLOR_WHITE)
        self.tree_arch.tag_configure('even', background='#FFF2F2')
        self.tree_arch.tag_configure('group', background='#FFE8E8')  # ê·¸ë£¹ ë°°ì¹˜ ê°•ì¡°
        self.tree_arch.tag_configure('group_header', background='#FFCDD2', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))  # ê·¸ë£¹ í—¤ë” ê°•ì¡° 

        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_arch.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_arch.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_arch.heading("memo", text="ë©”ëª¨")
        self.tree_arch.heading("status", text="ìƒíƒœ")
        self.tree_arch.heading("created", text="ìƒì„±ì¼")
        self.tree_arch.heading("completed", text="ì™„ë£Œì¼")
        self.tree_arch.heading("model", text="ëª¨ë¸")
        self.tree_arch.heading("effort", text="Effort")
        self.tree_arch.heading("market", text="ë§ˆì¼“")
        self.tree_arch.heading("counts", text="ìš”ì²­ìˆ˜")
        self.tree_arch.heading("group", text="ê·¸ë£¹")
        
        self.tree_arch.column("#0", width=350, anchor="w")  # íŠ¸ë¦¬ ì»¬ëŸ¼ (ê·¸ë£¹ëª… í‘œì‹œ)
        self.tree_arch.column("batch_id", width=200, anchor="w")
        self.tree_arch.column("excel_name", width=200, anchor="w")  # ì—‘ì…€ íŒŒì¼ëª…
        self.tree_arch.column("memo", width=150, anchor="w")  # ë©”ëª¨
        self.tree_arch.column("status", width=80, anchor="center")
        self.tree_arch.column("created", width=120, anchor="center")
        self.tree_arch.column("completed", width=120, anchor="center")
        self.tree_arch.column("model", width=80, anchor="center")
        self.tree_arch.column("effort", width=60, anchor="center")
        self.tree_arch.column("market", width=80, anchor="center")
        self.tree_arch.column("counts", width=80, anchor="center")
        self.tree_arch.column("group", width=80, anchor="center")
        
        self.tree_arch.pack(fill='both', expand=True)
        
        # Archive ìš°í´ë¦­ ë©”ë‰´
        self.menu_arch = Menu(self, tearoff=0)
        self.menu_arch.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_arch))
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected)
        self.menu_arch.add_command(label="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected)
        self.tree_arch.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_arch, self.menu_arch))
        
        self._load_jobs_all()
        self._load_archive_list()

    def _show_context_menu(self, event, tree, menu):
        item = tree.identify_row(event.y)
        if item:
            if item not in tree.selection():
                tree.selection_set(item)
            menu.post(event.x_root, event.y_root)

    def _expand_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ í¼ì¹©ë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=True)
    
    def _collapse_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ ì ‘ìŠµë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=False)
    
    def _get_selected_ids(self, tree):
        """
        ì„ íƒëœ í•­ëª©ì—ì„œ ë°°ì¹˜ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê·¸ë£¹ í—¤ë”ë¥¼ ì„ íƒí•˜ë©´ ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        selection = tree.selection()
        ids = []
        
        for item in selection:
            vals = tree.item(item)['values']
            batch_id = vals[0] if vals else ""
            
            # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
            if not batch_id:
                # ê·¸ë£¹ í—¤ë”ì˜ ìì‹ ë…¸ë“œë“¤(ë°°ì¹˜ë“¤)ì„ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
                children = tree.get_children(item)
                for child in children:
                    child_vals = tree.item(child)['values']
                    if child_vals and child_vals[0]:
                        ids.append(child_vals[0])
            else:
                # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°
                ids.append(batch_id)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(ids))
    
    def _edit_memo(self, tree):
        """ì„ íƒëœ ë°°ì¹˜ì˜ ë©”ëª¨ë¥¼ í¸ì§‘í•©ë‹ˆë‹¤."""
        selection = tree.selection()
        if not selection:
            messagebox.showwarning("ê²½ê³ ", "ë©”ëª¨ë¥¼ í¸ì§‘í•  ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # ì²« ë²ˆì§¸ ì„ íƒëœ í•­ëª©ì˜ ë°°ì¹˜ ID ê°€ì ¸ì˜¤ê¸°
        item = selection[0]
        vals = tree.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        if not batch_id:
            messagebox.showinfo("ì•ˆë‚´", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # í˜„ì¬ ë©”ëª¨ ê°€ì ¸ì˜¤ê¸°
        jobs = load_batch_jobs()
        current_memo = ""
        for j in jobs:
            if j["batch_id"] == batch_id:
                current_memo = j.get("memo", "") or ""
                break
        
        # ë©”ëª¨ í¸ì§‘ ë‹¤ì´ì–¼ë¡œê·¸
        dialog = tk.Toplevel(self)
        dialog.title("ë©”ëª¨ í¸ì§‘")
        dialog.geometry("500x200")
        dialog.transient(self)
        dialog.grab_set()
        
        # ë°°ì¹˜ ID í‘œì‹œ
        tk.Label(dialog, text=f"ë°°ì¹˜ ID: {batch_id[:30]}...", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(10, 5))
        
        # ë©”ëª¨ ì…ë ¥ í•„ë“œ
        tk.Label(dialog, text="ë©”ëª¨:", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(5, 0))
        memo_entry = tk.Text(dialog, height=5, width=60, font=("ë§‘ì€ ê³ ë”•", 9))
        memo_entry.pack(fill="both", expand=True, padx=10, pady=5)
        memo_entry.insert("1.0", current_memo)
        memo_entry.focus()
        
        # ë²„íŠ¼
        btn_frame = tk.Frame(dialog)
        btn_frame.pack(fill="x", padx=10, pady=10)
        
        def save_memo():
            new_memo = memo_entry.get("1.0", "end-1c").strip()
            upsert_batch_job(batch_id, memo=new_memo)
            self.append_log(f"[INFO] ë°°ì¹˜ {batch_id[:20]}... ë©”ëª¨ ì—…ë°ì´íŠ¸: {new_memo[:50]}...")
            self._load_jobs_all()
            self._load_archive_list()
            dialog.destroy()
            messagebox.showinfo("ì™„ë£Œ", "ë©”ëª¨ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        tk.Button(btn_frame, text="ì €ì¥", command=save_memo, bg="#4CAF50", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)
        tk.Button(btn_frame, text="ì·¨ì†Œ", command=dialog.destroy, bg="#f44336", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)

    def _load_jobs_all(self):
        if not hasattr(self, 'tree_active'): return
        for i in self.tree_active.get_children(): self.tree_active.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ì„ ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0]
            created_at = first_job.get("created_at", "")
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = dt.strftime("%m-%d %H:%M")
                except:
                    date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
            else:
                date_str = "-"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
            group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
            excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
            memo = first_job.get("memo", "") or "-"
            group_node = self.tree_active.insert("", "end", 
                text=group_header_text,
                values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), first_job.get("market", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                memo = j.get("memo", "") or "-"
                tag = 'group'
                self.tree_active.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), j.get("market", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_active.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), j.get("market", "-"), cnt, "-"
                ), 
                tags=(tag,))
            idx += 1

    def _load_archive_list(self):
        if not hasattr(self, 'tree_arch'): return
        for i in self.tree_arch.get_children(): self.tree_arch.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if not j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs)) if group_jobs else len(group_jobs)
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0]
            created_at = first_job.get("created_at", "")
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = dt.strftime("%m-%d %H:%M")
                except:
                    date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
            else:
                date_str = "-"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
            group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
            excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
            memo = first_job.get("memo", "") or "-"
            group_node = self.tree_arch.insert("", "end", 
                text=group_header_text,
                values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), first_job.get("market", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                memo = j.get("memo", "") or "-"
                tag = 'group'
                self.tree_arch.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), j.get("market", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_arch.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), j.get("market", "-"), cnt, "-"
                ), 
                tags=(tag,))
            idx += 1

    # --- Batch Actions ---
    def _refresh_selected(self, tree):
        ids = self._get_selected_ids(tree)
        if not ids: return
        
        # API í‚¤ í™•ì¸
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.\nìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        jobs = load_batch_jobs()
        completed = [bid for bid in ids if next((x for x in jobs if x["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        
        if completed:
            if messagebox.askyesno("í™•ì¸", f"{len(completed)}ê±´ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\nì œì™¸í•˜ê³  ë¯¸ì™„ë£Œ ê±´ë§Œ ê°±ì‹ í• ê¹Œìš”?"):
                ids = [i for i in ids if i not in completed]
        
        if not ids:
            messagebox.showinfo("ì·¨ì†Œ", "ê°±ì‹ í•  ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        t = threading.Thread(target=self._run_refresh_ids, args=(ids,))
        t.daemon = True
        t.start()

    def _run_refresh_ids(self, ids):
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            self.after(0, lambda: messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."))
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            self.after(0, lambda: messagebox.showerror("ì˜¤ë¥˜", f"API Keyê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n{e}"))
            return
        
        self.append_log(f"ì„ íƒëœ {len(ids)}ê±´ ê°±ì‹  ì¤‘...")
        success_cnt = 0
        fail_cnt = 0
        
        for bid in ids:
            try:
                remote = client.batches.retrieve(bid)
                rc = None
                if remote.request_counts:
                    rc = {"total": remote.request_counts.total, "completed": remote.request_counts.completed, "failed": remote.request_counts.failed}
                
                # expired ìƒíƒœë„ ê°±ì‹  ê°€ëŠ¥ (output_file_id í™•ì¸ì„ ìœ„í•´)
                output_file_id = getattr(remote, "output_file_id", None)
                upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id, request_counts=rc)
                
                if remote.status == "expired" and output_file_id:
                    self.append_log(f"â„¹ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ì§€ë§Œ output_file_idê°€ ìˆìŠµë‹ˆë‹¤. (ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")
                self.append_log(f"âœ… {bid}: {remote.status}")
                success_cnt += 1
            except Exception as e:
                error_msg = str(e)
                # 401 ì˜¤ë¥˜ì¸ ê²½ìš° ë” ëª…í™•í•œ ë©”ì‹œì§€
                if "401" in error_msg or "authentication" in error_msg.lower():
                    self.append_log(f"âŒ {bid} ê°±ì‹  ì‹¤íŒ¨: API Key ì¸ì¦ ì˜¤ë¥˜ (401)")
                    self.append_log(f"   â†’ ìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ ì˜¬ë°”ë¥¸ API Keyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                else:
                    self.append_log(f"âŒ {bid} ê°±ì‹  ì‹¤íŒ¨: {error_msg}")
                fail_cnt += 1
        
        self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])
        if fail_cnt > 0:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}, ì‹¤íŒ¨: {fail_cnt})")
            if fail_cnt == len(ids):
                self.after(0, lambda: messagebox.showwarning("ê²½ê³ ", f"ëª¨ë“  ë°°ì¹˜ ê°±ì‹ ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\nAPI Keyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."))
        else:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}ê±´)")

    def _merge_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        
        # API í‚¤ í™•ì¸
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.\nìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        jobs = load_batch_jobs()
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        group_ids = set()
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if job:
                group_id = job.get("batch_group_id")
                if group_id:
                    group_ids.add(group_id)
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        all_target_ids = set(ids)
        group_info = ""  # ì´ˆê¸°í™”
        
        for group_id in group_ids:
            if group_id:
                # completed, expired ë˜ëŠ” merged ìƒíƒœì¸ ë°°ì¹˜ í¬í•¨ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
                group_batches = [j for j in jobs if j.get("batch_group_id") == group_id and j.get("status") in ["completed", "expired"]]
                for j in group_batches:
                    all_target_ids.add(j["batch_id"])
        
        # ê·¸ë£¹ ì •ë³´ ë©”ì‹œì§€ ìƒì„±
        if len(all_target_ids) > len(ids):
            group_info = f"\n\nê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ {len(all_target_ids) - len(ids)}ê°œê°€ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤."
        
        # completed, expired ë˜ëŠ” merged ìƒíƒœì¸ ë°°ì¹˜ ëª¨ë‘ ì„ íƒ ê°€ëŠ¥ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
        targets = [bid for bid in all_target_ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") in ["completed", "expired", "merged"]]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ë³‘í•©í•  ìˆ˜ ìˆëŠ” 'completed', 'expired' ë˜ëŠ” 'merged' ìƒíƒœì˜ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        completed_cnt = sum(1 for bid in targets if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "completed")
        merged_cnt = len(targets) - completed_cnt
        
        msg = f"ì„ íƒí•œ {len(targets)}ê±´ì„ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ?{group_info}"
        if merged_cnt > 0:
            msg += f"\n\n({completed_cnt}ê±´: ë³‘í•© + íˆìŠ¤í† ë¦¬ ê¸°ë¡)\n({merged_cnt}ê±´: íˆìŠ¤í† ë¦¬ ê¸°ë¡ë§Œ)"
        
        if messagebox.askyesno("ë³‘í•©", msg):
            t = threading.Thread(target=self._run_merge_multi, args=(targets,))
            t.daemon = True
            t.start()

    def _run_merge_multi(self, ids):
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            self.after(0, lambda: messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."))
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            self.after(0, lambda: messagebox.showerror("ì˜¤ë¥˜", f"API Keyê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n{e}"))
            return
        
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ë¶„ë¥˜
        groups_to_merge = {}  # {group_id: [batch_ids]}
        ungrouped_batches = []  # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤
        
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not job:
                continue
            
            group_id = job.get("batch_group_id")
            if group_id:
                if group_id not in groups_to_merge:
                    groups_to_merge[group_id] = []
                groups_to_merge[group_id].append(bid)
            else:
                ungrouped_batches.append(bid)
        
        success_cnt = 0
        total_cost = 0.0
        
        # ê·¸ë£¹ë³„ ë³‘í•© ì²˜ë¦¬
        for group_id, batch_ids in groups_to_merge.items():
            self.append_log(f"--- ê·¸ë£¹ ë³‘í•© ì‹œì‘: {group_id} ({len(batch_ids)}ê°œ ë°°ì¹˜) ---")
            try:
                # ê·¸ë£¹ ë‚´ ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì›ë³¸ ì—‘ì…€ ê²½ë¡œ ì‚¬ìš©
                first_job = next((j for j in jobs if j["batch_id"] == batch_ids[0]), None)
                if not first_job:
                    continue
                
                src_path = first_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ì„ì‹œ JSONLì— ìˆ˜ì§‘
                all_output_lines = []
                model_name = first_job.get("model", "gpt-5-mini")
                total_group_cost = 0.0
                
                # ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ (chunk_indexê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ)
                def get_chunk_index(bid):
                    job = next((j for j in jobs if j["batch_id"] == bid), None)
                    if job:
                        idx = job.get("chunk_index")
                        return idx if idx is not None else 999999
                    return 999999
                
                batch_ids_sorted = sorted(batch_ids, key=get_chunk_index)
                
                for bid in batch_ids_sorted:
                    self.append_log(f"  [ê·¸ë£¹] ë°°ì¹˜ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    try:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if not local_job:
                            continue
                        
                        # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ë„ output íŒŒì¼ì´ ìˆìœ¼ë©´ ì¬ë³‘í•© ê°€ëŠ¥
                        is_already_merged = local_job.get("status") == "merged"
                        if is_already_merged:
                            # ë¡œì»¬ì— output JSONL íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                            base_dir = os.path.dirname(src_path)
                            base_name, _ = os.path.splitext(os.path.basename(src_path))
                            out_jsonl = os.path.join(base_dir, f"{base_name}_stage3_batch_output_{bid}.jsonl")
                            
                            if os.path.exists(out_jsonl):
                                # ë¡œì»¬ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì—†ì´ ë°”ë¡œ ì‚¬ìš©
                                self.append_log(f"  â„¹ï¸ {bid}: ì´ë¯¸ ë³‘í•©ëœ ì‘ì—…ì´ì§€ë§Œ ë¡œì»¬ output íŒŒì¼ì´ ìˆì–´ ì¬ë³‘í•©í•©ë‹ˆë‹¤.")
                                # ì•„ë˜ ë‹¤ìš´ë¡œë“œ ë¡œì§ì„ ê±´ë„ˆë›°ê³  JSONL ì½ê¸°ë¡œ ì´ë™
                            else:
                                # ë¡œì»¬ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì›ê²©ì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œë„
                                self.append_log(f"  â„¹ï¸ {bid}: ì´ë¯¸ ë³‘í•©ëœ ì‘ì—…ì´ì§€ë§Œ output íŒŒì¼ì„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
                        
                        # Batch ìƒíƒœ í™•ì¸
                        remote = client.batches.retrieve(bid)
                        output_file_id = getattr(remote, "output_file_id", None)
                        
                        # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                        if remote.status == "completed":
                            if not output_file_id:
                                self.append_log(f"  âš ï¸ {bid}: ì™„ë£Œ ìƒíƒœì§€ë§Œ output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                                upsert_batch_job(bid, status=remote.status, output_file_id=None)
                                continue
                        elif remote.status == "expired":
                            if not output_file_id:
                                self.append_log(f"  âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                                upsert_batch_job(bid, status=remote.status, output_file_id=None)
                                continue
                            else:
                                self.append_log(f"  â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                        else:
                            self.append_log(f"  âš ï¸ {bid}: ì•„ì§ completed ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                            upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id)
                            continue
                        
                        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (ë¡œì»¬ íŒŒì¼ì´ ì—†ì„ ë•Œë§Œ)
                        base_dir = os.path.dirname(src_path)
                        base_name, _ = os.path.splitext(os.path.basename(src_path))
                        out_jsonl = os.path.join(base_dir, f"{base_name}_stage3_batch_output_{bid}.jsonl")
                        
                        if not os.path.exists(out_jsonl):
                            # ë¡œì»¬ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
                            try:
                                content = client.files.content(output_file_id).content
                            except AttributeError:
                                file_content = client.files.content(output_file_id)
                                if hasattr(file_content, "read"):
                                    content = file_content.read()
                                elif hasattr(file_content, "iter_bytes"):
                                    chunks = []
                                    for ch in file_content.iter_bytes():
                                        chunks.append(ch)
                                    content = b"".join(chunks)
                                else:
                                    content = file_content
                            
                            with open(out_jsonl, "wb") as f:
                                f.write(content)
                            
                            upsert_batch_job(bid, status="completed", output_file_id=output_file_id, output_jsonl=out_jsonl)
                        else:
                            # ë¡œì»¬ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê¸°
                            self.append_log(f"  [ê·¸ë£¹] ë¡œì»¬ output íŒŒì¼ ì‚¬ìš©: {os.path.basename(out_jsonl)}")
                            if not is_already_merged:
                                upsert_batch_job(bid, status="completed", output_file_id=output_file_id, output_jsonl=out_jsonl)
                        
                        # JSONL íŒŒì¼ ì½ì–´ì„œ ìˆ˜ì§‘
                        if os.path.exists(out_jsonl):
                            with open(out_jsonl, "r", encoding="utf-8") as f:
                                for line in f:
                                    line = line.strip()
                                    if line:
                                        all_output_lines.append(line)
                        
                    except Exception as e:
                        self.append_log(f"  âŒ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if not all_output_lines:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ë³‘í•©í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ì˜ ì „ì²´ ì²­í¬ ìˆ˜ í™•ì¸ ë° ê²€ì¦
                expected_total_chunks = first_job.get("total_chunks")
                if expected_total_chunks:
                    # ì‹¤ì œë¡œ ê²°ê³¼ë¥¼ ë‹¤ìš´ë¡œë“œí•œ ë°°ì¹˜ ìˆ˜ ê³„ì‚° (completed ë˜ëŠ” expired ìƒíƒœ í¬í•¨)
                    downloaded_batch_ids = []
                    for bid in batch_ids_sorted:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if local_job and local_job.get("status") in ["completed", "expired"]:
                            out_jsonl = local_job.get("output_jsonl") or os.path.join(
                                os.path.dirname(src_path),
                                f"{os.path.splitext(os.path.basename(src_path))[0]}_stage3_batch_output_{bid}.jsonl"
                            )
                            if os.path.exists(out_jsonl):
                                downloaded_batch_ids.append(bid)
                    
                    if len(downloaded_batch_ids) < expected_total_chunks:
                        missing = expected_total_chunks - len(downloaded_batch_ids)
                        self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì˜ˆìƒ {expected_total_chunks}ê°œ ì¤‘ {len(downloaded_batch_ids)}ê°œë§Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({missing}ê°œ ëˆ„ë½ ê°€ëŠ¥)")
                
                # ì„ì‹œ í†µí•© JSONL íŒŒì¼ ìƒì„±
                base_dir = os.path.dirname(src_path)
                base_name, _ = os.path.splitext(os.path.basename(src_path))
                merged_jsonl = os.path.join(base_dir, f"{base_name}_stage3_batch_output_merged_{group_id}.jsonl")
                
                with open(merged_jsonl, "w", encoding="utf-8") as f:
                    for line in all_output_lines:
                        f.write(line + "\n")
                
                self.append_log(f"  [ê·¸ë£¹] í†µí•© JSONL ìƒì„±: {len(all_output_lines)}ê°œ ê²°ê³¼")
                
                # í†µí•© JSONLì„ ì—‘ì…€ì— ë³‘í•©
                results_map = {}
                total_group_in = 0
                total_group_out = 0
                total_group_cached = 0
                total_group_requests = 0
                total_group_cache_hits = 0
                
                with open(merged_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        try:
                            data = json.loads(line)
                        except json.JSONDecodeError as e:
                            self.append_log(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                            continue
                        
                        cid = data.get("custom_id")
                        if not cid:
                            continue
                        
                        # /v1/responses API í˜•ì‹ ì²˜ë¦¬
                        # responseê°€ ìˆìœ¼ë©´ bodyì—ì„œ, ì—†ìœ¼ë©´ errorì—ì„œ í† í° ì •ë³´ í™•ì¸
                        response = data.get("response")
                        error = data.get("error")
                        
                        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (response ë˜ëŠ” error ëª¨ë‘ì—ì„œ)
                        usage = {}
                        if response:
                            response_body = response.get("body", {}) if isinstance(response, dict) else {}
                            usage = response_body.get("usage", {})
                        elif error:
                            # ì—ëŸ¬ ì‘ë‹µì—ë„ usageê°€ ìˆì„ ìˆ˜ ìˆìŒ
                            if isinstance(error, dict):
                                usage = error.get("usage", {})
                        
                        input_tokens = usage.get("input_tokens", 0) or usage.get("prompt_tokens", 0)  # í˜¸í™˜ì„±
                        output_tokens = usage.get("output_tokens", 0) or usage.get("completion_tokens", 0)  # í˜¸í™˜ì„±
                        total_group_in += input_tokens
                        total_group_out += output_tokens
                        
                        # ìºì‹± í†µê³„ ìˆ˜ì§‘
                        input_tokens_details = usage.get("input_tokens_details", {})
                        cached_tokens = input_tokens_details.get("cached_tokens", 0)
                        total_group_cached += cached_tokens
                        total_group_requests += 1
                        if cached_tokens > 0:
                            total_group_cache_hits += 1
                        
                        # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—ëŸ¬ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
                        if response and not error:
                            try:
                                # ìƒˆë¡œìš´ API ì‘ë‹µ í˜•ì‹: response.body.output ë°°ì—´ ì‚¬ìš©
                                response_body = response.get("body", {}) if isinstance(response, dict) else {}
                                output_array = response_body.get("output", [])
                                
                                # output ë°°ì—´ì—ì„œ type="message"ì¸ í•­ëª© ì°¾ê¸°
                                text_content = ""
                                for item in output_array:
                                    if item.get("type") == "message":
                                        content_array = item.get("content", [])
                                        for content_item in content_array:
                                            if content_item.get("type") == "output_text":
                                                text_content = content_item.get("text", "").strip()
                                                break
                                        if text_content:
                                            break
                                
                                if text_content:
                                    results_map[cid] = text_content
                                else:
                                    # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜: choices ì‚¬ìš© (fallback)
                                    val = response_body.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                                    if val:
                                        results_map[cid] = val
                                    else:
                                        results_map[cid] = ""
                            except Exception as e:
                                self.append_log(f"[WARN] ê²°ê³¼ ì¶”ì¶œ ì‹¤íŒ¨ (custom_id: {cid}): {e}")
                                results_map[cid] = ""
                        elif error:
                            # ì—ëŸ¬ ì¼€ì´ìŠ¤: ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                            results_map[cid] = ""
                        else:
                            # responseë„ errorë„ ì—†ëŠ” ê²½ìš°
                            results_map[cid] = ""
                
                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    if "ST3_ê²°ê³¼ìƒí’ˆëª…" not in df.columns:
                        df["ST3_ê²°ê³¼ìƒí’ˆëª…"] = ""
                    df["ST3_ê²°ê³¼ìƒí’ˆëª…"] = df["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str)
                    cnt = 0
                    for cid, val in results_map.items():
                        try:
                            # custom_id í˜•ì‹: "row-0", "row-1" ë“±
                            if cid.startswith("row-"):
                                idx = int(cid.split("-")[1])
                            else:
                                # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜: "row_0" ë“±
                                idx = int(cid.split("_")[1])
                            if 0 <= idx < len(df):
                                df.at[idx, "ST3_ê²°ê³¼ìƒí’ˆëª…"] = val
                                cnt += 1
                        except Exception as e:
                            self.append_log(f"[WARN] custom_id íŒŒì‹± ì‹¤íŒ¨: {cid} - {e}")
                            pass

                    # ì½”ì–´ ì™„ë£Œ íŒŒì¼(out_excel)ì„ ë¨¼ì € ì €ì¥
                    base, _ = os.path.splitext(src_path)
                    out_excel = f"{base}_stage3_batch_done.xlsx"
                    if not safe_save_excel(df, out_excel):
                        self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")

                    # Stage3 ìµœì¢… íŒŒì¼ëª…: *_T3_... í˜•ì‹ìœ¼ë¡œ ë²„ì „ ì—…
                    try:
                        final_out_path = get_next_version_path(src_path, task_type="text")
                        df_done = pd.read_excel(out_excel)
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ê³¼ ì—†ëŠ” í–‰ ë¶„ë¦¬
                        if "ST3_ê²°ê³¼ìƒí’ˆëª…" in df_done.columns:
                            df_with_st3 = df_done[df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].notna() & (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"] != '') & (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str) != 'nan')].copy()
                            df_no_st3 = df_done[(df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].isna()) | (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"] == '') | (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str) == 'nan')].copy()
                        else:
                            df_with_st3 = pd.DataFrame()
                            df_no_st3 = df_done.copy()
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ì—†ëŠ” í–‰ë“¤ì„ T3(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³„ë„ íŒŒì¼ ì €ì¥
                        no_st3_path = None
                        if len(df_no_st3) > 0:
                            base_dir = os.path.dirname(src_path)
                            base_name, ext = os.path.splitext(os.path.basename(src_path))
                            
                            name_only_clean = re.sub(r"\([^)]*\)", "", base_name)
                            all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
                            
                            if all_matches:
                                match = all_matches[-1]
                                original_name = name_only_clean[: match.start()].rstrip("_")
                                current_i = int(match.group(4))
                                new_filename = f"{original_name}_T3_I{current_i}(ì‹¤íŒ¨){ext}"
                            else:
                                new_filename = f"{base_name}_T3(ì‹¤íŒ¨)_I0{ext}"
                            
                            no_st3_path = os.path.join(base_dir, new_filename)
                            df_no_st3.to_excel(no_st3_path, index=False)
                            
                            self.append_log(f"  [ê·¸ë£¹] T3(ì‹¤íŒ¨) ë¶„ë¦¬ íŒŒì¼: {os.path.basename(no_st3_path)} ({len(df_no_st3)}ê°œ í–‰)")
                            self.append_log(f"         â€» ì´ íŒŒì¼ì€ T3 ì‘ì—…ì— ì‹¤íŒ¨í•œ í•­ëª©ì…ë‹ˆë‹¤.")
                            
                            try:
                                no_st3_root_name = get_root_filename(no_st3_path)
                                JobManager.update_status(no_st3_root_name, text_msg="T3(ì‹¤íŒ¨)")
                                self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸: {no_st3_root_name} -> T3(ì‹¤íŒ¨)")
                            except Exception as e:
                                self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                        
                        if len(df_with_st3) > 0:
                            df_done = df_with_st3
                        else:
                            self.append_log(f"  âš ï¸ ê·¸ë£¹ {group_id}: ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        if safe_save_excel(df_done, final_out_path):
                            out_path_for_history = final_out_path
                            if out_excel != final_out_path and os.path.exists(out_excel):
                                try:
                                    os.remove(out_excel)
                                    self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                                except Exception as e:
                                    self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                        else:
                            out_path_for_history = out_excel
                    except Exception as e:
                        self.append_log(f"[WARN] T3 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                        out_path_for_history = out_excel
                    
                    # ë””ë²„ê¹…: í† í° ìˆ˜ì§‘ ìƒíƒœ í™•ì¸
                    if total_group_requests == 0:
                        self.append_log(f"âš ï¸ [ë””ë²„ê¹…] ê·¸ë£¹ {group_id}: JSONLì—ì„œ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    elif total_group_in == 0 and total_group_out == 0:
                        self.append_log(f"âš ï¸ [ë””ë²„ê¹…] ê·¸ë£¹ {group_id}: í† í° ì •ë³´ê°€ ëª¨ë‘ 0ì…ë‹ˆë‹¤. usage í•„ë“œ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                        # ì²« ë²ˆì§¸ ë¼ì¸ ìƒ˜í”Œ ì¶œë ¥
                        try:
                            with open(merged_jsonl, "r", encoding="utf-8") as f:
                                first_line = f.readline()
                                if first_line.strip():
                                    sample = json.loads(first_line)
                                    self.append_log(f"  [ìƒ˜í”Œ] ì²« ë²ˆì§¸ ë¼ì¸ êµ¬ì¡°: response={bool(sample.get('response'))}, error={bool(sample.get('error'))}, usage={bool(sample.get('response', {}).get('body', {}).get('usage'))}")
                        except Exception as e:
                            self.append_log(f"  [ìƒ˜í”Œ í™•ì¸ ì‹¤íŒ¨]: {e}")
                    
                    # ê·¸ë£¹ ì „ì²´ ìºì‹± í†µê³„ ì¶œë ¥
                    group_cache_hit_rate = (total_group_cache_hits / total_group_requests * 100) if total_group_requests > 0 else 0
                    group_cache_savings_pct = (total_group_cached / total_group_in * 100) if total_group_in > 0 else 0
                    pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                    group_cache_savings = (total_group_cached / 1_000_000) * pricing["input"] * 0.5
                    
                    # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                    cost_in = (total_group_in / 1_000_000) * pricing["input"] * 0.5
                    cost_out = (total_group_out / 1_000_000) * pricing["output"] * 0.5
                    cost_total = cost_in + cost_out
                    total_group_cost = cost_total
                    total_cost += total_group_cost
                    
                    # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ë¥¼ merged ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                    for bid in batch_ids:
                        upsert_batch_job(
                            batch_id=bid,
                            out_excel=out_path_for_history,
                            status="merged",
                        )
                    
                    # ì‹¤í–‰ ì´ë ¥ ê¸°ë¡
                    try:
                        if first_job:
                            c_at_str = first_job.get("created_at", "")
                            if c_at_str:
                                try:
                                    # ISO í˜•ì‹ íŒŒì‹± (Zë¥¼ +00:00ìœ¼ë¡œ ë³€í™˜)
                                    c_at = datetime.fromisoformat(c_at_str.replace('Z', '+00:00'))
                                    # ì‹œê°„ëŒ€ ì •ë³´ê°€ ìˆìœ¼ë©´ naiveë¡œ ë³€í™˜ (datetime.now()ì™€ ì¼ì¹˜)
                                    if c_at.tzinfo is not None:
                                        c_at = c_at.replace(tzinfo=None)
                                except:
                                    c_at = datetime.now()
                            else:
                                c_at = datetime.now()
                            finish_dt = datetime.now()
                            elapsed = (finish_dt - c_at).total_seconds()
                            
                            append_run_history(
                                stage="Stage 3 Batch (Grouped)",
                                model_name=model_name,
                                reasoning_effort=first_job.get("effort", "medium"),
                                src_file=src_path,
                                out_file=out_path_for_history,
                                total_rows=len(df),
                                api_rows=len(results_map),
                                elapsed_seconds=elapsed,
                                total_in_tok=total_group_in,
                                total_out_tok=total_group_out,
                                total_reasoning_tok=0,
                                input_cost_usd=cost_in,
                                output_cost_usd=cost_out,
                                total_cost_usd=cost_total,
                                start_dt=c_at,
                                finish_dt=finish_dt,
                                api_type="batch",
                                batch_id=f"{group_id} ({len(batch_ids)} batches)",
                                success_rows=cnt,
                                fail_rows=len(results_map)-cnt,
                            )
                    except Exception as hist_e:
                        self.append_log(f"[WARN] ê·¸ë£¹ {group_id} íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì‹¤íŒ¨: {hist_e}")
                    
                    try:
                        root_name = get_root_filename(src_path)
                        JobManager.update_status(root_name, text_msg="T3(ìƒì„±ì™„ë£Œ)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T3(ìƒì„±ì™„ë£Œ)")
                    except Exception as e:
                        self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")
                    
                    # ê·¸ë£¹ ì „ì²´ ìºì‹± í†µê³„ ì¶œë ¥
                    group_cache_hit_rate = (total_group_cache_hits / total_group_requests * 100) if total_group_requests > 0 else 0
                    group_cache_savings_pct = (total_group_cached / total_group_in * 100) if total_group_in > 0 else 0
                    group_cache_savings = (total_group_cached / 1_000_000) * pricing["input"] * 0.5
                    
                    self.append_log(f"âœ… ê·¸ë£¹ {group_id} ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(out_path_for_history)}")
                    self.append_log(f"  [ê·¸ë£¹ ìºì‹± í†µê³„] ìš”ì²­ {total_group_requests:,}ê±´, íˆíŠ¸ {total_group_cache_hits:,}ê±´ ({group_cache_hit_rate:.1f}%), ìºì‹œ í† í° {total_group_cached:,} ({group_cache_savings_pct:.1f}%)")
                    if group_cache_savings > 0:
                        self.append_log(f"  [ê·¸ë£¹ ë¹„ìš©ì ˆê°] ìºì‹±ìœ¼ë¡œ ì´ ${group_cache_savings:.4f} ì ˆê°")
                    success_cnt += 1
                else:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                self.append_log(f"âŒ ê·¸ë£¹ {group_id} ë³‘í•© ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        for bid in ungrouped_batches:
            self.append_log(f"--- ë³‘í•© ì‹œì‘: {bid} ---")
            try:
                jobs = load_batch_jobs()
                local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                
                if not local_job:
                    self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì‘ì—… ì´ë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ë„ output íŒŒì¼ì´ ìˆìœ¼ë©´ ì¬ë³‘í•© ê°€ëŠ¥
                is_already_merged = local_job.get("status") == "merged"
                if is_already_merged:
                    # ë¡œì»¬ì— output JSONL íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                    src_path = local_job.get("src_excel")
                    if src_path:
                        base, _ = os.path.splitext(src_path)
                        out_jsonl = f"{base}_stage3_batch_output.jsonl"
                        
                        if os.path.exists(out_jsonl):
                            # ë¡œì»¬ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¬ë³‘í•© ì§„í–‰
                            self.append_log(f"â„¹ï¸ {bid}: ì´ë¯¸ ë³‘í•©ëœ ì‘ì—…ì´ì§€ë§Œ ë¡œì»¬ output íŒŒì¼ì´ ìˆì–´ ì¬ë³‘í•©í•©ë‹ˆë‹¤.")
                            is_already_merged = False  # ì¬ë³‘í•© í”Œë˜ê·¸ í•´ì œ
                        else:
                            # ë¡œì»¬ íŒŒì¼ì´ ì—†ìœ¼ë©´ íˆìŠ¤í† ë¦¬ ê¸°ë¡ë§Œ ìˆ˜í–‰
                            self.append_log(f"â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. íˆìŠ¤í† ë¦¬ ê¸°ë¡ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                            out_path_for_history = local_job.get("out_excel")
                            if not src_path or not out_path_for_history or not os.path.exists(out_path_for_history):
                                self.append_log(f"âš ï¸ {bid}: ë³‘í•©ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íˆìŠ¤í† ë¦¬ ê¸°ë¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                                continue
                    else:
                        # src_excelì´ ì—†ìœ¼ë©´ ì¬ë³‘í•© ë¶ˆê°€
                        self.append_log(f"âš ï¸ {bid}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ì–´ ì¬ë³‘í•©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                
                if is_already_merged:
                    # íˆìŠ¤í† ë¦¬ ê¸°ë¡ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
                    
                    # ê¸°ì¡´ íŒŒì¼ì—ì„œ í† í° ì •ë³´ ì¶”ì¶œ (JSONL íŒŒì¼ì´ ìˆìœ¼ë©´)
                    base, _ = os.path.splitext(src_path) if src_path else ("", "")
                    out_jsonl = f"{base}_stage3_batch_output.jsonl"
                    batch_in_tok = 0
                    batch_out_tok = 0
                    batch_cached_tok = 0
                    batch_total_requests = 0
                    batch_cache_hits = 0
                    results_map = {}
                    
                    if os.path.exists(out_jsonl):
                        try:
                            with open(out_jsonl, "r", encoding="utf-8") as f:
                                for line in f:
                                    if not line.strip(): continue
                                    data = json.loads(line)
                                    # /v1/responses API í˜•ì‹ ì²˜ë¦¬
                                    response_body = data.get("response", {}).get("body", {})
                                    usage = response_body.get("usage", {})
                                    input_tokens = usage.get("input_tokens", 0) or usage.get("prompt_tokens", 0)  # í˜¸í™˜ì„±
                                    output_tokens = usage.get("output_tokens", 0) or usage.get("completion_tokens", 0)  # í˜¸í™˜ì„±
                                    batch_in_tok += input_tokens
                                    batch_out_tok += output_tokens
                                    
                                    # ìºì‹± í†µê³„ ìˆ˜ì§‘
                                    input_tokens_details = usage.get("input_tokens_details", {})
                                    cached_tokens = input_tokens_details.get("cached_tokens", 0)
                                    batch_cached_tok += cached_tokens
                                    batch_total_requests += 1
                                    if cached_tokens > 0:
                                        batch_cache_hits += 1
                                    cid = data.get("custom_id")
                                    try:
                                        # ìƒˆë¡œìš´ API ì‘ë‹µ í˜•ì‹: response.body.output ë°°ì—´ ì‚¬ìš©
                                        body = response_body
                                        output_array = body.get("output", [])
                                        
                                        text_content = ""
                                        for item in output_array:
                                            if item.get("type") == "message":
                                                content_array = item.get("content", [])
                                                for content_item in content_array:
                                                    if content_item.get("type") == "output_text":
                                                        text_content = content_item.get("text", "").strip()
                                                        break
                                                if text_content:
                                                    break
                                        
                                        if text_content:
                                            results_map[cid] = text_content
                                        else:
                                            # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜: choices ì‚¬ìš© (fallback)
                                            val = body.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                                            if val:
                                                results_map[cid] = val
                                    except Exception as e:
                                        self.append_log(f"[WARN] {bid}: ê²°ê³¼ ì¶”ì¶œ ì‹¤íŒ¨ (custom_id: {cid}): {e}")
                                        pass
                        except Exception as e:
                            self.append_log(f"[WARN] {bid}: JSONL íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                    
                    # ìºì‹± í†µê³„ ì¶œë ¥
                    cache_hit_rate = (batch_cache_hits / batch_total_requests * 100) if batch_total_requests > 0 else 0
                    cache_savings_pct = (batch_cached_tok / batch_in_tok * 100) if batch_in_tok > 0 else 0
                    self.append_log(f"  [ìºì‹±] {bid}: ìš”ì²­ {batch_total_requests}ê±´, íˆíŠ¸ {batch_cache_hits}ê±´ ({cache_hit_rate:.1f}%), ìºì‹œ í† í° {batch_cached_tok:,} ({cache_savings_pct:.1f}%)")
                    
                    # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                    model_name = local_job.get("model", "gpt-5-mini")
                    pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                    cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                    cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                    cost_total = cost_in + cost_out
                    
                    # ìºì‹œë¡œ ì ˆê°ëœ ë¹„ìš© ê³„ì‚°
                    cache_savings = (batch_cached_tok / 1_000_000) * pricing["input"] * 0.5
                    if cache_savings > 0:
                        self.append_log(f"  [ë¹„ìš©ì ˆê°] {bid}: ìºì‹±ìœ¼ë¡œ ${cache_savings:.4f} ì ˆê°")
                    
                    # ì¶œë ¥ íŒŒì¼ì—ì„œ í–‰ ìˆ˜ í™•ì¸
                    try:
                        df_out = pd.read_excel(out_path_for_history)
                        total_rows = len(df_out)
                        api_rows = len(results_map) if results_map else total_rows
                        cnt = api_rows  # merged ìƒíƒœì—ì„œëŠ” ì„±ê³µ ê±´ìˆ˜ ì¶”ì •
                    except:
                        total_rows = 0
                        api_rows = 0
                        cnt = 0
                    
                    # íˆìŠ¤í† ë¦¬ ê¸°ë¡ë§Œ ìˆ˜í–‰
                    try:
                        c_at_str = local_job.get("created_at", "")
                        if c_at_str:
                            try:
                                # ISO í˜•ì‹ íŒŒì‹± (Zë¥¼ +00:00ìœ¼ë¡œ ë³€í™˜)
                                c_at = datetime.fromisoformat(c_at_str.replace('Z', '+00:00'))
                                # ì‹œê°„ëŒ€ ì •ë³´ê°€ ìˆìœ¼ë©´ naiveë¡œ ë³€í™˜ (datetime.now()ì™€ ì¼ì¹˜)
                                if c_at.tzinfo is not None:
                                    c_at = c_at.replace(tzinfo=None)
                            except:
                                c_at = datetime.now()
                        else:
                            c_at = datetime.now()
                        finish_dt = datetime.now()
                        elapsed = (finish_dt - c_at).total_seconds()
                        
                        # íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì „ì— íŒŒì¼ ê²½ë¡œ í™•ì¸
                        from stage3_run_history import RUN_LOG_PATH
                        self.append_log(f"[DEBUG] íˆìŠ¤í† ë¦¬ íŒŒì¼ ê²½ë¡œ: {RUN_LOG_PATH}")
                        
                        result = append_run_history(
                            stage="Stage 3 Batch",
                            model_name=model_name,
                            reasoning_effort=local_job.get("effort", "medium"),
                            src_file=src_path,
                            out_file=out_path_for_history,
                            total_rows=total_rows,
                            api_rows=api_rows,
                            elapsed_seconds=elapsed,
                            total_in_tok=batch_in_tok,
                            total_out_tok=batch_out_tok,
                            total_reasoning_tok=0,
                            input_cost_usd=cost_in,
                            output_cost_usd=cost_out,
                            total_cost_usd=cost_total,
                            start_dt=c_at,
                            finish_dt=finish_dt,
                            api_type="batch",
                            batch_id=bid,
                            success_rows=cnt,
                            fail_rows=api_rows - cnt if api_rows > 0 else 0,
                        )
                        if result:
                            # íŒŒì¼ì´ ì‹¤ì œë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            if os.path.exists(RUN_LOG_PATH):
                                self.append_log(f"[INFO] âœ… ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì™„ë£Œ: {RUN_LOG_PATH} (ë°°ì¹˜ ID: {bid})")
                            else:
                                self.append_log(f"[ERROR] âŒ ì‹¤í–‰ ì´ë ¥ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {RUN_LOG_PATH}")
                        else:
                            self.append_log(f"[INFO] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ê±´ë„ˆëœ€: ë°°ì¹˜ {bid}ëŠ” ì´ë¯¸ ê¸°ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    except Exception as hist_e:
                        import traceback
                        error_detail = traceback.format_exc()
                        self.append_log(f"[WARN] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨")
                        self.append_log(f"[WARN] ì˜¤ë¥˜ ìƒì„¸: {str(hist_e)}")
                        self.append_log(f"[WARN] {error_detail}")
                    
                    continue

                # Batch ìƒíƒœ ë° ê²°ê³¼ íŒŒì¼ ID ì¡°íšŒ (ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©)
                remote = client.batches.retrieve(bid)
                output_file_id = getattr(remote, "output_file_id", None)
                
                # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                if remote.status == "completed":
                    if not output_file_id:
                        self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì™„ë£Œ ìƒíƒœì§€ë§Œ output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        upsert_batch_job(bid, status=remote.status, output_file_id=None)
                        continue
                elif remote.status == "expired":
                    if not output_file_id:
                        self.append_log(f"âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                        upsert_batch_job(bid, status=remote.status, output_file_id=None)
                        continue
                    else:
                        self.append_log(f"â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                else:
                    self.append_log(f"âš ï¸ {bid}: ì•„ì§ completed ë˜ëŠ” expired ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id)
                    continue
                
                if local_job and local_job.get("src_excel"):
                    src_path = local_job["src_excel"]
                    base, _ = os.path.splitext(src_path)
                    out_jsonl = f"{base}_stage3_batch_output.jsonl"
                    out_excel = f"{base}_stage3_batch_done.xlsx"
                else:
                    out_jsonl = f"output_{bid}.jsonl"
                    out_excel = f"output_{bid}.xlsx"
                    src_path = None

                # ë¡œì»¬ íŒŒì¼ì´ ì—†ì„ ë•Œë§Œ ë‹¤ìš´ë¡œë“œ
                if not os.path.exists(out_jsonl):
                    try:
                        content = client.files.content(output_file_id).content
                    except AttributeError:
                        # ì‹ ë²„ì „ í´ë¼ì´ì–¸íŠ¸ ëŒ€ì‘
                        file_content = client.files.content(output_file_id)
                        if hasattr(file_content, "read"):
                            content = file_content.read()
                        elif hasattr(file_content, "iter_bytes"):
                            chunks = []
                            for ch in file_content.iter_bytes():
                                chunks.append(ch)
                            content = b"".join(chunks)
                        else:
                            content = file_content  # type: ignore
                    
                    with open(out_jsonl, "wb") as f:
                        f.write(content)
                    self.append_log(f"  [ë‹¤ìš´ë¡œë“œ ì™„ë£Œ] {os.path.basename(out_jsonl)}")
                else:
                    self.append_log(f"  [ë¡œì»¬ íŒŒì¼ ì‚¬ìš©] {os.path.basename(out_jsonl)} (ë‹¤ìš´ë¡œë“œ ê±´ë„ˆëœ€)")
                
                results_map = {}
                batch_in_tok = 0
                batch_out_tok = 0
                
                with open(out_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        
                        usage = data.get("response", {}).get("body", {}).get("usage", {})
                        batch_in_tok += usage.get("prompt_tokens", 0)
                        batch_out_tok += usage.get("completion_tokens", 0)
                        
                        try:
                            # ìƒˆë¡œìš´ API ì‘ë‹µ í˜•ì‹: response.body.output ë°°ì—´ ì‚¬ìš©
                            body = data.get("response", {}).get("body", {})
                            output_array = body.get("output", [])
                            
                            # output ë°°ì—´ì—ì„œ type="message"ì¸ í•­ëª© ì°¾ê¸°
                            text_content = ""
                            for item in output_array:
                                if item.get("type") == "message":
                                    content_array = item.get("content", [])
                                    for content_item in content_array:
                                        if content_item.get("type") == "output_text":
                                            text_content = content_item.get("text", "").strip()
                                            break
                                    if text_content:
                                        break
                            
                            if text_content:
                                results_map[cid] = text_content
                            else:
                                # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜: choices ì‚¬ìš© (fallback)
                                val = body.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                                if val:
                                    results_map[cid] = val
                                else:
                                    results_map[cid] = ""
                        except Exception as e:
                            self.append_log(f"[WARN] ê²°ê³¼ ì¶”ì¶œ ì‹¤íŒ¨ (custom_id: {cid}): {e}")
                            results_map[cid] = ""
                
                # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                model_name = local_job.get("model", "gpt-5-mini") if local_job else "gpt-5-mini"
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                cost_total = cost_in + cost_out
                total_cost += cost_total

                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    if "ST3_ê²°ê³¼ìƒí’ˆëª…" not in df.columns:
                        df["ST3_ê²°ê³¼ìƒí’ˆëª…"] = ""
                    df["ST3_ê²°ê³¼ìƒí’ˆëª…"] = df["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str)
                    cnt = 0
                    for cid, val in results_map.items():
                        try:
                            # custom_id í˜•ì‹: "row-0", "row-1" ë“±
                            if cid.startswith("row-"):
                                idx = int(cid.split("-")[1])
                            else:
                                # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜: "row_0" ë“±
                                idx = int(cid.split("_")[1])
                            if 0 <= idx < len(df):
                                df.at[idx, "ST3_ê²°ê³¼ìƒí’ˆëª…"] = val
                                cnt += 1
                        except Exception as e:
                            self.append_log(f"[WARN] custom_id íŒŒì‹± ì‹¤íŒ¨: {cid} - {e}")
                            pass

                    # ì½”ì–´ ì™„ë£Œ íŒŒì¼(out_excel)ì„ ë¨¼ì € ì €ì¥
                    if not safe_save_excel(df, out_excel):
                        self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")

                    # Stage3 ìµœì¢… íŒŒì¼ëª…: *_T3_... í˜•ì‹ìœ¼ë¡œ ë²„ì „ ì—…
                    try:
                        final_out_path = get_next_version_path(src_path, task_type="text")
                        df_done = pd.read_excel(out_excel)
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ê³¼ ì—†ëŠ” í–‰ ë¶„ë¦¬
                        if "ST3_ê²°ê³¼ìƒí’ˆëª…" in df_done.columns:
                            # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì¸ í–‰ ì°¾ê¸°
                            df_with_st3 = df_done[df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].notna() & (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"] != '') & (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str) != 'nan')].copy()
                            df_no_st3 = df_done[(df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].isna()) | (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"] == '') | (df_done["ST3_ê²°ê³¼ìƒí’ˆëª…"].astype(str) == 'nan')].copy()
                        else:
                            # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  í–‰ì´ ST3_ê²°ê³¼ìƒí’ˆëª… ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
                            df_with_st3 = pd.DataFrame()
                            df_no_st3 = df_done.copy()
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ì—†ëŠ” í–‰ë“¤ì„ T3(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³„ë„ íŒŒì¼ ì €ì¥
                        no_st3_path = None
                        if len(df_no_st3) > 0:
                            base_dir = os.path.dirname(src_path)
                            base_name, ext = os.path.splitext(os.path.basename(src_path))
                            
                            # í˜„ì¬ íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: _T2_I0)
                            # T3(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³€ê²½
                            name_only_clean = re.sub(r"\([^)]*\)", "", base_name)  # ê¸°ì¡´ ê´„í˜¸ ì œê±°
                            all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
                            
                            if all_matches:
                                # ë§ˆì§€ë§‰ ë²„ì „ íŒ¨í„´ ì‚¬ìš©
                                match = all_matches[-1]
                                original_name = name_only_clean[: match.start()].rstrip("_")
                                current_i = int(match.group(4))
                                # T3(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ìƒì„±
                                new_filename = f"{original_name}_T3_I{current_i}(ì‹¤íŒ¨){ext}"
                            else:
                                # ë²„ì „ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ T3(ì‹¤íŒ¨)_I0ë¡œ ìƒì„±
                                new_filename = f"{base_name}_T3(ì‹¤íŒ¨)_I0{ext}"
                            
                            no_st3_path = os.path.join(base_dir, new_filename)
                            df_no_st3.to_excel(no_st3_path, index=False)
                            
                            self.append_log(f"  T3(ì‹¤íŒ¨) ë¶„ë¦¬ íŒŒì¼: {os.path.basename(no_st3_path)} ({len(df_no_st3)}ê°œ í–‰)")
                            self.append_log(f"  â€» ì´ íŒŒì¼ì€ T3 ì‘ì—…ì— ì‹¤íŒ¨í•œ í•­ëª©ì…ë‹ˆë‹¤.")
                            
                            # ë¶„ë¦¬ëœ íŒŒì¼ì˜ ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                            try:
                                no_st3_root_name = get_root_filename(no_st3_path)
                                JobManager.update_status(no_st3_root_name, text_msg="T3(ì‹¤íŒ¨)")
                                self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸: {no_st3_root_name} -> T3(ì‹¤íŒ¨)")
                            except Exception as e:
                                self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                        
                        # ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ë“¤ë§Œ ì €ì¥
                        if len(df_with_st3) > 0:
                            df_done = df_with_st3
                        else:
                            self.append_log(f"âš ï¸ {bid}: ST3_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        if safe_save_excel(df_done, final_out_path):
                            out_path_for_history = final_out_path
                            # T3 ë²„ì „ íŒŒì¼ ì €ì¥ ì„±ê³µ ì‹œ, ì½”ì–´ê°€ ìƒì„±í•œ ì¤‘ê°„ íŒŒì¼(_stage3_batch_done) ì‚­ì œ
                            if out_excel != final_out_path and os.path.exists(out_excel):
                                try:
                                    os.remove(out_excel)
                                    self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                                except Exception as e:
                                    self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                        else:
                            out_path_for_history = out_excel
                    except Exception as e:
                        self.append_log(f"[WARN] T3 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                        out_path_for_history = out_excel

                    upsert_batch_job(bid, out_excel=out_path_for_history, status="merged")

                    # History ê¸°ë¡ (naive datetime ê¸°ì¤€)
                    try:
                        c_at_str = local_job.get("created_at", "")
                        if c_at_str:
                            try:
                                # ISO í˜•ì‹ íŒŒì‹± (Zë¥¼ +00:00ìœ¼ë¡œ ë³€í™˜)
                                c_at = datetime.fromisoformat(c_at_str.replace('Z', '+00:00'))
                                # ì‹œê°„ëŒ€ ì •ë³´ê°€ ìˆìœ¼ë©´ naiveë¡œ ë³€í™˜ (datetime.now()ì™€ ì¼ì¹˜)
                                if c_at.tzinfo is not None:
                                    c_at = c_at.replace(tzinfo=None)
                            except:
                                c_at = datetime.now()
                        else:
                            c_at = datetime.now()
                        finish_dt = datetime.now()
                        elapsed = (finish_dt - c_at).total_seconds()

                        # íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì „ì— íŒŒì¼ ê²½ë¡œ í™•ì¸
                        from stage3_run_history import RUN_LOG_PATH
                        self.append_log(f"[DEBUG] íˆìŠ¤í† ë¦¬ íŒŒì¼ ê²½ë¡œ: {RUN_LOG_PATH}")
                        
                        result = append_run_history(
                            stage="Stage 3 Batch",
                            model_name=model_name,
                            reasoning_effort=local_job.get("effort", "medium"),
                            src_file=src_path,
                            out_file=out_path_for_history,
                            total_rows=len(df),
                            api_rows=len(results_map),
                            elapsed_seconds=elapsed,
                            total_in_tok=batch_in_tok,
                            total_out_tok=batch_out_tok,
                            total_reasoning_tok=0,
                            input_cost_usd=cost_in,
                            output_cost_usd=cost_out,
                            total_cost_usd=cost_total,
                            start_dt=c_at,
                            finish_dt=finish_dt,
                            api_type="batch",
                            batch_id=bid,
                            success_rows=cnt,
                            fail_rows=len(results_map)-cnt,
                        )
                        if result:
                            # íŒŒì¼ì´ ì‹¤ì œë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            if os.path.exists(RUN_LOG_PATH):
                                self.append_log(f"[INFO] âœ… ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì™„ë£Œ: {RUN_LOG_PATH} (ë°°ì¹˜ ID: {bid})")
                            else:
                                self.append_log(f"[ERROR] âŒ ì‹¤í–‰ ì´ë ¥ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {RUN_LOG_PATH}")
                        else:
                            self.append_log(f"[INFO] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ê±´ë„ˆëœ€: ë°°ì¹˜ {bid}ëŠ” ì´ë¯¸ ê¸°ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    except Exception as hist_e:
                        # íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì‹¤íŒ¨í•´ë„ ë³‘í•©ì€ ì„±ê³µí•œ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
                        import traceback
                        error_detail = traceback.format_exc()
                        self.append_log(f"[WARN] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨ (ë³‘í•©ì€ ì •ìƒ ì™„ë£Œ)")
                        self.append_log(f"[WARN] ì˜¤ë¥˜ ìƒì„¸: {str(hist_e)}")
                        self.append_log(f"[WARN] {error_detail}")

                    # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage3(Text) ì™„ë£Œ ìƒíƒœ ê¸°ë¡: T3(ìƒì„±ì™„ë£Œ) (img ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                    try:
                        root_name = get_root_filename(src_path)
                        JobManager.update_status(root_name, text_msg="T3(ìƒì„±ì™„ë£Œ)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T3(ìƒì„±ì™„ë£Œ)")
                    except Exception as e:
                        self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")

                    self.append_log(f"âœ… ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(out_path_for_history)}")
                    success_cnt += 1
                else:
                    self.append_log(f"âš ï¸ ì›ë³¸ ì—†ìŒ. JSONLë§Œ ì €ì¥.")
            except Exception as e:
                self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: {e}")
        
        self.append_log(f"=== ì¼ê´„ ë³‘í•© ë (ì„±ê³µ: {success_cnt}, ë¹„ìš©: ${total_cost:.4f}) ===")
        self._load_jobs_all()
        messagebox.showinfo("ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©: ${total_cost:.4f}")

    def _report_selected_unified(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        targets = [bid for bid in ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "merged"]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ìƒíƒœê°€ 'merged'ì¸ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if messagebox.askyesno("ë¦¬í¬íŠ¸", f"ì„ íƒí•œ {len(targets)}ê±´ì˜ í†µí•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_report_unified, args=(targets,))
            t.daemon = True
            t.start()

    def _run_report_unified(self, ids):
        self.append_log(f"--- í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ({len(ids)}ê±´) ---")
        jobs = load_batch_jobs()
        all_reps = []
        for bid in ids:
            local_job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not local_job: continue
            src = local_job.get("src_excel")
            out = local_job.get("out_excel")
            if not src or not out or not os.path.exists(src) or not os.path.exists(out): 
                self.append_log(f"âš ï¸ íŒŒì¼ ëˆ„ë½: {bid}")
                continue
            try:
                df_in = pd.read_excel(src)
                df_out = pd.read_excel(out)
                
                # ì›ë³¸ ìƒí’ˆëª… ì»¬ëŸ¼ ì°¾ê¸° (ìš°ì„ ìˆœìœ„: ST1_ê²°ê³¼ìƒí’ˆëª… > ì›ë³¸ìƒí’ˆëª…)
                orig_col = None
                for col in ["ST1_ê²°ê³¼ìƒí’ˆëª…", "ì›ë³¸ìƒí’ˆëª…", "ê³µê¸‰ì‚¬ìƒí’ˆëª…"]:
                    if col in df_in.columns:
                        orig_col = col
                        break
                
                # Stage 3 ë¦¬í¬íŠ¸ ë¡œì§ (ì›ë³¸ê³¼ ë¹„êµ)
                for idx, row in df_in.iterrows():
                    # ì›ë³¸ ìƒí’ˆëª…
                    orig_name = safe_str(row.get(orig_col, "")) if orig_col else ""
                    
                    # ST3 ê²°ê³¼
                    st3 = ""
                    if idx < len(df_out):
                        st3 = safe_str(df_out.iloc[idx].get("ST3_ê²°ê³¼ìƒí’ˆëª…", ""))
                    
                    cands = [x for x in st3.split('\n') if x.strip()]
                    first_line = cands[0] if cands else "(ìƒì„±ì‹¤íŒ¨)"
                    
                    # ë¹„êµ ì •ë³´
                    is_same = (orig_name.strip() == first_line.strip()) if orig_name and first_line != "(ìƒì„±ì‹¤íŒ¨)" else False
                    length_diff = len(first_line) - len(orig_name) if orig_name else None
                    
                    all_reps.append({
                        "Batch_ID": bid,
                        "í–‰ë²ˆí˜¸": idx + 2,
                        "ìƒí’ˆì½”ë“œ": safe_str(row.get("ìƒí’ˆì½”ë“œ", "")),
                        "ì›ë³¸ìƒí’ˆëª…": orig_name,
                        "ST3_ì²«ì¤„": first_line,
                        "ë™ì¼ì—¬ë¶€": "âœ… ë™ì¼" if is_same else "âŒ ë‹¤ë¦„",
                        "ê¸¸ì´ì°¨ì´": length_diff if length_diff is not None else "-",
                        "ìƒì„±í›„ë³´ìˆ˜": len(cands),
                        "ST2_ê¸¸ì´": len(safe_str(row.get("ST2_JSON", "")))
                    })
            except Exception as e:
                self.append_log(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜ ({bid}): {e}")
                import traceback
                self.append_log(traceback.format_exc())

        if not all_reps:
            messagebox.showinfo("ì•Œë¦¼", "ë°ì´í„° ì—†ìŒ")
            return

        try:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_dir = os.path.dirname(os.path.abspath(__file__))
            report_path = os.path.join(save_dir, f"Stage3_Unified_Report_{ts}.xlsx")
            pd.DataFrame(all_reps).to_excel(report_path, index=False)
            self.append_log(f"ğŸ“Š ë¦¬í¬íŠ¸ ì™„ë£Œ: {os.path.basename(report_path)}")
            if messagebox.askyesno("ì™„ë£Œ", "íŒŒì¼ì„ ì—¬ì‹œê² ìŠµë‹ˆê¹Œ?"):
                os.startfile(report_path)
        except Exception as e:
            self.append_log(f"ì‹¤íŒ¨: {e}")
            messagebox.showerror("ì˜¤ë¥˜", str(e))

    def _archive_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if ids and messagebox.askyesno("ë³´ê´€", f"{len(ids)}ê±´ íœ´ì§€í†µ ì´ë™?"):
            archive_batch_job(ids, True)
            self._load_jobs_all()
            self._load_archive_list()

    def _restore_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ë³µêµ¬", f"{len(ids)}ê±´ ë³µêµ¬?"):
            archive_batch_job(ids, False)
            self._load_jobs_all()
            self._load_archive_list()

    def _hard_delete_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ì‚­ì œ", "ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            hard_delete_batch_job(ids)
            self._load_archive_list()

    def _on_tree_double_click(self, event):
        """ë”ë¸”í´ë¦­ ì‹œ: ê·¸ë£¹ í—¤ë”ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°, ë°°ì¹˜ë©´ ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™"""
        sel = self.tree_active.selection()
        if not sel: return
        
        item = sel[0]
        vals = self.tree_active.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš°: ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
        if not batch_id:
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            children = self.tree_active.get_children(item)
            if children:
                # ìì‹ì´ ìˆìœ¼ë©´ ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
                if self.tree_active.item(item, 'open'):
                    self.tree_active.item(item, open=False)
                else:
                    self.tree_active.item(item, open=True)
        else:
            # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°: ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™
            self.batch_id_var.set(batch_id)
            self.main_tabs.select(self.tab_merge)

    # ----------------------------------------------------
    # Tab 3: Merge (Manual)
    # ----------------------------------------------------
    def _init_tab_merge(self):
        container = ttk.Frame(self.tab_merge, padding=20)
        container.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì„¹ì…˜
        f_retry = ttk.LabelFrame(container, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", padding=15)
        f_retry.pack(fill='x', pady=(0, 15))
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        f_list = ttk.Frame(f_retry)
        f_list.pack(fill='both', expand=True, pady=(0, 10))
        ttk.Label(f_list, text="ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(anchor='w')
        
        # Treeviewë¡œ ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        list_frame = ttk.Frame(f_list)
        list_frame.pack(fill='both', expand=True, pady=5)
        
        scrollbar = ttk.Scrollbar(list_frame)
        scrollbar.pack(side='right', fill='y')
        
        self.failed_chunks_tree = ttk.Treeview(list_frame, columns=("chunk_num", "file_name", "error_type"), 
                                                show='headings', height=4, yscrollcommand=scrollbar.set)
        scrollbar.config(command=self.failed_chunks_tree.yview)
        
        self.failed_chunks_tree.heading("chunk_num", text="ì²­í¬ ë²ˆí˜¸")
        self.failed_chunks_tree.heading("file_name", text="íŒŒì¼ëª…")
        self.failed_chunks_tree.heading("error_type", text="ì˜¤ë¥˜ ìœ í˜•")
        
        self.failed_chunks_tree.column("chunk_num", width=80, anchor="center")
        self.failed_chunks_tree.column("file_name", width=300, anchor="w")
        self.failed_chunks_tree.column("error_type", width=150, anchor="center")
        
        self.failed_chunks_tree.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ ì…ë ¥
        f_file = ttk.Frame(f_retry)
        f_file.pack(fill='x', pady=(10, 0))
        ttk.Label(f_file, text="ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼:", font=("ë§‘ì€ ê³ ë”•", 9)).pack(side='left')
        self.failed_chunks_file_var = tk.StringVar()
        ttk.Entry(f_file, textvariable=self.failed_chunks_file_var, width=50, font=("Consolas", 9)).pack(side='left', padx=5, fill='x', expand=True)
        btn_select = ttk.Button(f_file, text="ğŸ“‚ ì°¾ê¸°", command=self._select_failed_chunks_file)
        btn_select.pack(side='left', padx=5)
        btn_retry = ttk.Button(f_file, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", command=self._retry_failed_chunks, style="Success.TButton")
        btn_retry.pack(side='left', padx=5)
        
        f_in = ttk.LabelFrame(container, text="ê°œë³„ ì‘ì—…", padding=15)
        f_in.pack(fill='x', pady=(0, 15))
        ttk.Label(f_in, text="Batch ID:").pack(side='left')
        ttk.Entry(f_in, textvariable=self.batch_id_var, width=45, font=("Consolas", 10)).pack(side='left', padx=10)
        
        f_btn = ttk.Frame(container)
        f_btn.pack(fill='x', pady=20)
        ttk.Button(f_btn, text="1. ê²°ê³¼ ë³‘í•©", command=self._start_merge, style="Primary.TButton").pack(fill='x', pady=5)
        ttk.Button(f_btn, text="2. ë‹¨ì¼ ë¦¬í¬íŠ¸", command=self._start_diff_report).pack(fill='x', pady=5)

    def _start_merge(self):
        # API í‚¤ í™•ì¸
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.\nìƒë‹¨ì˜ API ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        bid = self.batch_id_var.get().strip()
        if not bid:
            messagebox.showwarning("ì˜¤ë¥˜", "Batch IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        t = threading.Thread(target=self._run_merge)
        t.daemon = True
        t.start()

    def _run_merge(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_merge_multi([bid])

    def _start_diff_report(self):
        t = threading.Thread(target=self._run_diff_report)
        t.daemon = True
        t.start()

    def _run_diff_report(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_report_unified([bid])
    
    def _handle_failed_chunks(self, failed_info_path, failed_chunk_files):
        """ì‹¤íŒ¨í•œ ì²­í¬ê°€ ìˆì„ ë•Œ GUIì— ìë™ìœ¼ë¡œ í‘œì‹œí•˜ê³  ì•Œë¦¼"""
        # ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ìë™ ì„¤ì •
        self.failed_chunks_file_var.set(failed_info_path)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ì„ Treeviewì— í‘œì‹œ
        for item in self.failed_chunks_tree.get_children():
            self.failed_chunks_tree.delete(item)
        
        for failed_info in failed_chunk_files:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
            file_name = os.path.basename(chunk_file)
            
            self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        
        # ì•Œë¦¼ ë©”ì‹œì§€ ë° íƒ­ ì „í™˜
        failed_count = len(failed_chunk_files)
        token_limit_count = sum(1 for f in failed_chunk_files if f.get("is_token_limit", False))
        
        msg = f"âš ï¸ {failed_count}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\n"
        if token_limit_count > 0:
            msg += f"â€¢ í† í° ì œí•œ ì˜¤ë¥˜: {token_limit_count}ê°œ\n"
        msg += f"â€¢ ì‹¤íŒ¨ ì •ë³´ê°€ ìë™ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        msg += f"â€¢ '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        messagebox.showwarning("ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨", msg)
        
        # ì¬ì‹œë„ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
        self.main_tabs.select(self.tab_merge)
    
    def _select_failed_chunks_file(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ"""
        path = filedialog.askopenfilename(
            title="ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ",
            filetypes=[("JSON íŒŒì¼", "*.json"), ("ëª¨ë“  íŒŒì¼", "*.*")]
        )
        if path:
            self.failed_chunks_file_var.set(path)
            # íŒŒì¼ì„ ì„ íƒí•˜ë©´ ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(path)
    
    def _load_failed_chunks_from_file(self, file_path):
        """ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ì„ ì½ì–´ì„œ ëª©ë¡ì— í‘œì‹œ"""
        if not os.path.exists(file_path):
            return
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ê¸°ì¡´ ëª©ë¡ ì‚­ì œ
            for item in self.failed_chunks_tree.get_children():
                self.failed_chunks_tree.delete(item)
            
            # ëª©ë¡ì— ì¶”ê°€
            for failed_info in failed_chunks:
                chunk_num = failed_info.get("chunk_num", 0)
                chunk_file = failed_info.get("chunk_file", "")
                error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
                file_name = os.path.basename(chunk_file)
                
                self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        except Exception as e:
            self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    def _retry_failed_chunks(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„"""
        failed_file = self.failed_chunks_file_var.get().strip()
        if not failed_file:
            messagebox.showwarning("ê²½ê³ ", "ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        if not os.path.exists(failed_file):
            messagebox.showerror("ì˜¤ë¥˜", f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{failed_file}")
            return
        
        if not self.api_key_var.get():
            messagebox.showwarning("ê²½ê³ ", "API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        try:
            with open(failed_file, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(failed_file)
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨:\n{e}")
            return
        
        if not failed_chunks:
            messagebox.showinfo("ì•Œë¦¼", "ì¬ì‹œë„í•  ì‹¤íŒ¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if messagebox.askyesno("í™•ì¸", f"{len(failed_chunks)}ê°œ ì‹¤íŒ¨í•œ ì²­í¬ë¥¼ ì¬ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_retry_failed_chunks, args=(failed_chunks,))
            t.daemon = True
            t.start()
    
    def _run_retry_failed_chunks(self, failed_chunks):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì‹¤í–‰"""
        key = self.api_key_var.get().strip()
        import httpx
        timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
        
        self.append_log(f"[RETRY] ì‹¤íŒ¨í•œ ì²­í¬ {len(failed_chunks)}ê°œ ì¬ì‹œë„ ì‹œì‘...")
        
        retry_batch_ids = []
        for failed_info in failed_chunks:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            excel_path = failed_info.get("excel_path", "")
            model_name = failed_info.get("model_name", "gpt-5-mini")
            effort = failed_info.get("effort", "medium")
            batch_group_id = failed_info.get("batch_group_id", "")
            settings_dict = failed_info.get("settings", {})
            
            # settingsë¥¼ dataclassë¡œ ë³µì› (í•„ìš”í•œ ê²½ìš°)
            from stage3_core import Stage3Settings
            if settings_dict:
                try:
                    settings = Stage3Settings(**settings_dict)
                except Exception:
                    # ê¸°ë³¸ê°’ ì‚¬ìš©
                    settings = Stage3Settings()
            else:
                settings = Stage3Settings()
            
            if not os.path.exists(chunk_file):
                self.append_log(f"âš ï¸ ì²­í¬ {chunk_num}: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_file}")
                continue
            
            self.append_log(f"[RETRY] ì²­í¬ {chunk_num} ì¬ì‹œë„ ì¤‘... ({os.path.basename(chunk_file)})")
            
            try:
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=chunk_file,
                    excel_path=excel_path,
                    model_name=model_name,
                    reasoning_effort=effort,
                    settings=settings,
                )
                
                batch_id = batch.id
                retry_batch_ids.append(batch_id)
                self.append_log(f"âœ… ì²­í¬ {chunk_num} ì¬ì‹œë„ ì„±ê³µ: {batch_id}")
                
                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=excel_path,
                    jsonl_path=chunk_file,
                    model=model_name,
                    effort=effort,
                    status=batch.status,
                    output_file_id=None,
                    batch_group_id=batch_group_id,
                    chunk_index=chunk_num,
                    market=settings.market,
                    strategy=settings.naming_strategy
                )
                
            except Exception as e:
                self.append_log(f"âŒ ì²­í¬ {chunk_num} ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        if retry_batch_ids:
            self.append_log(f"âœ… ì¬ì‹œë„ ì™„ë£Œ: {len(retry_batch_ids)}ê°œ ë°°ì¹˜ ìƒì„±ë¨")
            # ë°°ì¹˜ ëª©ë¡ ê°±ì‹  ë° ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
            self.after(0, lambda: [
                self._load_jobs_all(),
                self._load_archive_list(),
                self.main_tabs.select(self.tab_manage),  # ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
                messagebox.showinfo("ì™„ë£Œ", f"{len(retry_batch_ids)}ê°œ ì²­í¬ ì¬ì‹œë„ ì„±ê³µ:\n{', '.join(retry_batch_ids[:5])}{'...' if len(retry_batch_ids) > 5 else ''}\n\në°°ì¹˜ ê´€ë¦¬ íƒ­ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.")
            ])
        else:
            self.append_log(f"âš ï¸ ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.after(0, lambda: messagebox.showwarning("ê²½ê³ ", "ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤."))

if __name__ == "__main__":
    app = Stage3BatchGUI()
    app.mainloop()