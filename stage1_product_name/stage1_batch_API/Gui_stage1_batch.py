"""
Gui_stage1_batch.py

Stage 1 Batch API ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (GUI) - Path Fixed
- ê¸°ëŠ¥: ì—‘ì…€ ì›ë³¸ -> Batch JSONL ìƒì„±(ìƒí’ˆëª… ì •ì œ) -> ì—…ë¡œë“œ -> ì‹¤í–‰ -> ë³‘í•© -> ì •ì œ ë¦¬í¬íŠ¸
- [Fix] ëŸ°ì²˜ ì‹¤í–‰ ì‹œ ëª¨ë“ˆ ê²½ë¡œ(ModuleNotFoundError) ë¬¸ì œ ì™„ë²½ í•´ê²°
"""

import os
import sys
import json
import math
import threading
import subprocess
import re
from datetime import datetime

# ========================================================
# [CRITICAL] ê²½ë¡œ ê°•ì œ ì„¤ì • (Import ì—ëŸ¬ ë°©ì§€)
# ========================================================
# í˜„ì¬ íŒŒì¼(Gui_stage1_batch.py)ì´ ìˆëŠ” í´ë”ë¥¼ êµ¬í•©ë‹ˆë‹¤.
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

# í•´ë‹¹ í´ë”ë¥¼ íŒŒì´ì¬ ê²€ìƒ‰ ê²½ë¡œ(sys.path)ì˜ ë§¨ ì•ì— ì¶”ê°€í•©ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ ê°™ì€ í´ë”ì— ìˆëŠ” 'batch_stage1_core.py'ë¥¼ ë¬´ì¡°ê±´ ì°¾ìŠµë‹ˆë‹¤.
if CURRENT_DIR not in sys.path:
    sys.path.insert(0, CURRENT_DIR)

# í˜¹ì‹œ core íŒŒì¼ì´ ìƒìœ„ í´ë”ì— ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìƒìœ„ í´ë”ë„ ì¶”ê°€í•©ë‹ˆë‹¤.
PARENT_DIR = os.path.dirname(CURRENT_DIR)
if PARENT_DIR not in sys.path:
    sys.path.append(PARENT_DIR)
# ========================================================

import pandas as pd
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, Menu
from tkinter.scrolledtext import ScrolledText

from openai import OpenAI

# ========================================================
# [NEW] ë©”ì¸ ëŸ°ì²˜ ì—°ë™ìš© JobManager & íŒŒì¼ëª… ìœ í‹¸
# ========================================================
def get_root_filename(filename):
    """
    íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´(_T*_I*) ë° ê¸°íƒ€ ê¼¬ë¦¬í‘œë¥¼ ë–¼ê³  ì›ë³¸ëª…(Key)ë§Œ ì¶”ì¶œ
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0.xlsx -> ë‚˜ì´í‚¤.xlsx
    ì˜ˆ: ì•„ë””ë‹¤ìŠ¤_T2_I1.xlsx -> ì•„ë””ë‹¤ìŠ¤.xlsx
    """
    name = os.path.basename(filename)
    base, ext = os.path.splitext(name)

    # 1. ë²„ì „ íŒ¨í„´ (_Tìˆ«ì_Iìˆ«ì) ì œê±°
    base = re.sub(r"_T\d+_I\d+$", "", base)

    # 2. ê¸°íƒ€ êµ¬í˜• ê¼¬ë¦¬í‘œ ì œê±° (í˜¸í™˜ì„± ìœ ì§€)
    suffixes = ["_stage1_mapping", "_stage1_img_mapping", "_with_images"]
    for s in suffixes:
        base = base.replace(s, "")

    return base + ext


def get_next_version_path(current_path, task_type: str = "text"):
    """
    í˜„ì¬ íŒŒì¼ëª…ì„ ë¶„ì„í•´ì„œ ë‹¤ìŒ ë‹¨ê³„ì˜ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    íŒŒì¼ëª… í˜•ì‹: ì›ë³¸ëª…_T{ìˆ«ì}_I{ìˆ«ì}.xlsx
    - task_type='text'  â†’ T ë²„ì „ +1 (T1 ì˜ë¯¸: í…ìŠ¤íŠ¸ 1ë‹¨ê³„ ì™„ë£Œ)
    - task_type='image' â†’ I ë²„ì „ +1
    """
    dir_name = os.path.dirname(current_path)
    base_name = os.path.basename(current_path)
    name_only, ext = os.path.splitext(base_name)

    pattern = r"_T(\d+)_I(\d+)$"
    match = re.search(pattern, name_only)

    if match:
        current_t = int(match.group(1))
        current_i = int(match.group(2))
        original_name = name_only[: match.start()]
    else:
        current_t = 0
        current_i = 0
        original_name = name_only

    if task_type == "text":
        new_t = current_t + 1
        new_i = current_i
    elif task_type == "image":
        new_t = current_t
        new_i = current_i + 1
    else:
        return current_path

    new_filename = f"{original_name}_T{new_t}_I{new_i}{ext}"
    return os.path.join(dir_name, new_filename)


class JobManager:
    DB_FILE = None

    @classmethod
    def find_db_path(cls):
        if cls.DB_FILE and os.path.exists(cls.DB_FILE):
            return cls.DB_FILE

        current_dir = os.path.dirname(os.path.abspath(__file__))
        search_dirs = [
            current_dir,
            os.path.abspath(os.path.join(current_dir, "..")),
            os.path.abspath(os.path.join(current_dir, "..", "..")),
        ]

        for d in search_dirs:
            target = os.path.join(d, "job_history.json")
            if os.path.exists(target):
                cls.DB_FILE = target
                print(f"[JobManager] DB Found: {target}")
                return target

        default_path = os.path.abspath(os.path.join(current_dir, "..", "job_history.json"))
        cls.DB_FILE = default_path
        return default_path

    @classmethod
    def load_jobs(cls):
        db_path = cls.find_db_path()
        if not os.path.exists(db_path):
            return {}
        try:
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    @classmethod
    def update_status(cls, filename, text_msg=None, img_msg=None):
        """ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        db_path = cls.find_db_path()
        data = cls.load_jobs()
        now = datetime.now().strftime("%m-%d %H:%M")

        if filename not in data:
            data[filename] = {
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "text_status": "ëŒ€ê¸°",
                "text_time": "-",
                "image_status": "ëŒ€ê¸°",
                "image_time": "-",
                "memo": "",
            }

        if text_msg:
            data[filename]["text_status"] = text_msg
            data[filename]["text_time"] = now
        if img_msg:
            data[filename]["image_status"] = img_msg
            data[filename]["image_time"] = now

        data[filename]["last_update"] = now

        try:
            with open(db_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"[JobManager Error] {e}")


def safe_save_excel(df, path: str) -> bool:
    """ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆì–´ ì €ì¥ì´ ì•ˆ ë  ë•Œ ì¬ì‹œë„ë¥¼ ìœ ë„í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            df.to_excel(path, index=False)
            return True
        except PermissionError:
            if not messagebox.askretrycancel(
                "ì €ì¥ ì‹¤íŒ¨",
                f"ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤!\n[{os.path.basename(path)}]\n\n"
                "íŒŒì¼ì„ ë‹«ê³  'ë‹¤ì‹œ ì‹œë„'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
            ):
                return False
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False


# core ëª¨ë“ˆì˜ ì‘ë‹µ íŒŒì„œ ì¬ì‚¬ìš© (Batch JSONL í˜•ì‹ í†µì¼)
try:
    from batch_stage1_core import (
        extract_text_from_response_dict,
        extract_usage_from_response_dict,
    )
except ImportError:
    # êµ¬ë²„ì „/ëˆ„ë½ í™˜ê²½ì—ì„œëŠ” ì¡°ìš©íˆ íŒ¨ìŠ¤í•˜ì§€ë§Œ,
    # ì´ ê²½ìš° Batch ë³‘í•© ì‹œ í† í°/í…ìŠ¤íŠ¸ íŒŒì‹±ì´ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ.
    def extract_text_from_response_dict(resp):  # type: ignore[override]
        body = resp.get("body") if isinstance(resp, dict) and "body" in resp else resp
        # ê°€ì¥ ë‹¨ìˆœí•œ fallback: output_text ë˜ëŠ” ë¬¸ìì—´ ë³€í™˜
        if isinstance(body, dict) and isinstance(body.get("output_text"), str):
            return body["output_text"].strip()
        return ""

    def extract_usage_from_response_dict(resp):  # type: ignore[override]
        return 0, 0, 0

# [í•µì‹¬ ì˜ì¡´ì„±] Stage1 í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ (coreì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
try:
    from prompts_stage1 import build_stage1_prompt, safe_str
except ImportError:
    # prompts_stage1 ì´ ì •ë§ ì—†ëŠ” ê²½ìš° ì•ˆì „í•œ fallback
    def safe_str(x):
        if x is None:
            return ""
        try:
            if isinstance(x, float) and math.isnan(x):
                return ""
        except Exception:
            pass
        return str(x).strip()

    def build_stage1_prompt(category, sale_type, raw_name):
        raise RuntimeError(
            "í•„ìˆ˜ ëª¨ë“ˆ 'prompts_stage1.py'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            "Stage1ì—ì„œ ì‚¬ìš©í•˜ë˜ í”„ë¡¬í”„íŠ¸ ì •ì˜ íŒŒì¼ì´ ê°™ì€ í´ë”ë‚˜ íŒŒì´ì¬ ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        )

# ì—¬ê¸°ì„œ GUI ì „ìš© batch payload ë¹Œë”ë¥¼ êµ¬í˜„ (coreëŠ” ì†ëŒ€ì§€ ì•ŠìŒ)
def build_stage1_batch_payload(idx, row, model, effort):
    """
    í•œ í–‰(row)ì„ Batch APIìš© ìš”ì²­ í•œ ì¤„(JSONL)ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜.
    - coreì˜ create_batch_input_jsonl ê³¼ ë™ì¼í•œ í•„ë“œ(ì¹´í…Œê³ ë¦¬ëª…, íŒë§¤í˜•íƒœ, ì›ë³¸ìƒí’ˆëª…)ë¥¼ ì‚¬ìš©
    - í”„ë¡¬í”„íŠ¸ëŠ” prompts_stage1.build_stage1_prompt ì¬ì‚¬ìš©
    - Batch endpoint: /v1/chat/completions (Gui_stage1_batch.py í˜„ì¬ êµ¬ì¡° ìœ ì§€)
    """
    # í•„ìˆ˜ í•„ë“œ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
    raw_name = safe_str(row.get("ì›ë³¸ìƒí’ˆëª…", ""))
    category = safe_str(row.get("ì¹´í…Œê³ ë¦¬ëª…", ""))
    sale_type = safe_str(row.get("íŒë§¤í˜•íƒœ", ""))

    # í•„ìˆ˜ê°’ì´ í•˜ë‚˜ë¼ë„ ë¹„ì–´ ìˆìœ¼ë©´ ì´ í–‰ì€ ìŠ¤í‚µ
    if not raw_name or not category or not sale_type:
        return None

    # coreì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt_text = build_stage1_prompt(category, sale_type, raw_name)

    # Responses APIìš© body (batch_stage1_core.create_batch_input_jsonl ê³¼ ë™ì¼í•œ í˜•íƒœ)
    body = {
        "model": model,
        "input": [
            {"role": "user", "content": prompt_text}
        ],
        "reasoning": {"effort": effort or "low"},
    }

    # Batch JSONL í•œ ì¤„ êµ¬ì¡° (Responses ì—”ë“œí¬ì¸íŠ¸)
    return {
        "custom_id": f"row-{idx}",
        "method": "POST",
        "url": "/v1/responses",
        "body": body,
    }


# History ëª¨ë“ˆ (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì¡°ìš©íˆ íŒ¨ìŠ¤)
try:
    from stage1_run_history import append_run_history
except ImportError:
    def append_run_history(*args, **kwargs):
        pass


# ========================================================
# [CORE] ê²½ë¡œ ë° ì„¤ì • ê´€ë¦¬
# ========================================================
def get_base_dir():
    """PyInstaller ë“±ìœ¼ë¡œ íŒ¨í‚¤ì§•ëœ ê²½ìš°ì™€ ì¼ë°˜ ì‹¤í–‰ì„ êµ¬ë¶„í•˜ì—¬ ê¸°ë³¸ ê²½ë¡œ ë°˜í™˜"""
    if getattr(sys, "frozen", False):
        return os.path.dirname(sys.executable)
    return os.path.dirname(os.path.abspath(__file__))

BASE_DIR = get_base_dir()
API_KEY_FILE = os.path.join(BASE_DIR, ".openai_api_key_stage1_batch")
BATCH_JOBS_FILE = os.path.join(BASE_DIR, "stage1_batch_jobs.json")

# [ìˆ˜ì •] GPT-5 ê³„ì—´ ëª¨ë¸ë§Œ ìœ ì§€
MODEL_PRICING_USD_PER_MTOK = {
    "gpt-5":       {"input": 1.25, "output": 10.00},
    "gpt-5-mini":  {"input": 0.25, "output": 2.00},
    "gpt-5-nano":  {"input": 0.05, "output": 0.40},
}

# UI Colors
COLOR_BG = "#F8F9FA"
COLOR_WHITE = "#FFFFFF"
COLOR_PRIMARY = "#4A90E2"
COLOR_PRIMARY_HOVER = "#357ABD"
COLOR_SUCCESS = "#28A745"
COLOR_SUCCESS_HOVER = "#218838"
COLOR_DANGER = "#DC3545"
COLOR_DANGER_HOVER = "#C82333"
COLOR_TEXT = "#333333"
COLOR_HEADER = "#E9ECEF"

def get_seoul_now():
    try:
        from pytz import timezone
        return datetime.now(timezone("Asia/Seoul"))
    except:
        return datetime.now()

def load_api_key_from_file(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f: return f.read().strip()
    return ""

def save_api_key_to_file(key, path):
    with open(path, "w", encoding="utf-8") as f: f.write(key)

# ========================================================
# íˆ´íŒ í´ë˜ìŠ¤
# ========================================================
class ToolTip:
    def __init__(self, widget, text, wraplength=400):
        self.widget = widget
        self.text = text
        self.wraplength = wraplength
        self.tipwindow = None
        self.widget.bind("<Enter>", self.show_tip)
        self.widget.bind("<Leave>", self.hide_tip)

    def show_tip(self, event=None):
        if self.tipwindow or not self.text: return
        x = self.widget.winfo_rootx() + 20
        y = self.widget.winfo_rooty() + 20
        self.tipwindow = tw = tk.Toplevel(self.widget)
        tw.wm_overrideredirect(True)
        tw.wm_geometry(f"+{x}+{y}")
        label = tk.Label(tw, text=self.text, justify='left', background="#ffffe0", relief='solid', borderwidth=1, font=("ë§‘ì€ ê³ ë”•", 9), wraplength=self.wraplength)
        label.pack(ipadx=4, ipady=2)

    def hide_tip(self, event=None):
        if self.tipwindow:
            self.tipwindow.destroy()
            self.tipwindow = None

# ========================================================
# ë°°ì¹˜ ì¡ ê´€ë¦¬ (JSON DB)
# ========================================================
def load_batch_jobs():
    if not os.path.exists(BATCH_JOBS_FILE): return []
    try:
        with open(BATCH_JOBS_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return []

def save_batch_jobs(jobs):
    try:
        with open(BATCH_JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
    except Exception as e: print(f"[Error] ì¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def upsert_batch_job(batch_id, **kwargs):
    jobs = load_batch_jobs()
    found = False
    now_str = datetime.now().isoformat()
    for j in jobs:
        if j["batch_id"] == batch_id:
            if kwargs.get("status") == "completed" and j.get("status") != "completed":
                if "completed_at" not in kwargs: j["completed_at"] = now_str
            j.update(kwargs)
            j["updated_at"] = now_str
            found = True
            break
    if not found:
        new_job = {
            "batch_id": batch_id, "created_at": now_str, "updated_at": now_str,
            "completed_at": "", "archived": False, **kwargs
        }
        jobs.insert(0, new_job)
    save_batch_jobs(jobs)

def archive_batch_job(batch_ids, archive=True):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    for j in jobs:
        if j["batch_id"] in batch_ids: j["archived"] = archive
    save_batch_jobs(jobs)

def hard_delete_batch_job(batch_ids):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    jobs = [j for j in jobs if j["batch_id"] not in batch_ids]
    save_batch_jobs(jobs)

# ========================================================
# GUI Class
# ========================================================
class Stage1BatchGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Stage 1: Batch API Manager (Path Fixed)")
        self.geometry("1250x950")
        
        self.api_key_var = tk.StringVar()
        self.src_file_var = tk.StringVar()
        
        # [ì„¤ì •] ê¸°ë³¸ê°’: gpt-5-mini / low
        self.model_var = tk.StringVar(value="gpt-5-mini") 
        self.effort_var = tk.StringVar(value="low") 
        self.skip_exist_var = tk.BooleanVar(value=True)
        
        # ìë™ ê°±ì‹  ê´€ë ¨
        self.auto_refresh_var = tk.BooleanVar(value=False)
        self.refresh_interval_var = tk.IntVar(value=30)
        self.is_refreshing = False

        self.batch_id_var = tk.StringVar()
        
        self._configure_styles()
        self._init_ui()
        self._load_key()
        
        # ìë™ ê°±ì‹  ë£¨í”„ ì‹œì‘
        self._auto_refresh_loop()

    def _configure_styles(self):
        style = ttk.Style()
        try: style.theme_use('clam')
        except: pass
        
        self.configure(background=COLOR_BG)
        style.configure("TFrame", background=COLOR_BG)
        style.configure("TLabel", background=COLOR_BG, foreground=COLOR_TEXT, font=("ë§‘ì€ ê³ ë”•", 10))
        style.configure("Header.TLabel", font=("ë§‘ì€ ê³ ë”•", 11, "bold"), foreground="#444")
        style.configure("TLabelframe", background=COLOR_BG, bordercolor="#D0D7DE")
        style.configure("TLabelframe.Label", background=COLOR_BG, foreground="#0056b3", font=("ë§‘ì€ ê³ ë”•", 10, "bold"))
        style.configure("TNotebook", background=COLOR_BG, borderwidth=0)
        style.configure("TNotebook.Tab", background="#E1E4E8", padding=[12, 5], font=("ë§‘ì€ ê³ ë”•", 10))
        style.map("TNotebook.Tab", background=[("selected", COLOR_WHITE)], foreground=[("selected", COLOR_PRIMARY)])
        style.configure("Treeview", background=COLOR_WHITE, fieldbackground=COLOR_WHITE, font=("ë§‘ì€ ê³ ë”•", 9), rowheight=28)
        style.configure("Treeview.Heading", background=COLOR_HEADER, foreground="#333", font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        style.map("Treeview", background=[('selected', '#CCE5FF')], foreground=[('selected', 'black')])
        style.configure("TButton", font=("ë§‘ì€ ê³ ë”•", 9), padding=5, borderwidth=1)
        style.configure("Primary.TButton", background=COLOR_PRIMARY, foreground="white", bordercolor=COLOR_PRIMARY)
        style.map("Primary.TButton", background=[("active", COLOR_PRIMARY_HOVER)])
        style.configure("Success.TButton", background=COLOR_SUCCESS, foreground="white", bordercolor=COLOR_SUCCESS)
        style.map("Success.TButton", background=[("active", COLOR_SUCCESS_HOVER)])
        style.configure("Danger.TButton", background=COLOR_DANGER, foreground="white", bordercolor=COLOR_DANGER)
        style.map("Danger.TButton", background=[("active", COLOR_DANGER_HOVER)])

    def _init_ui(self):
        main_container = ttk.Frame(self, padding=15)
        main_container.pack(fill='both', expand=True)

        # 1. ìƒë‹¨ API Key
        f_top = ttk.LabelFrame(main_container, text="ğŸ”‘ API ì„¤ì •", padding=10)
        f_top.pack(fill='x', pady=(0, 10))
        ttk.Label(f_top, text="Batch API Key:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(side='left')
        entry_key = ttk.Entry(f_top, textvariable=self.api_key_var, show="*", width=50, font=("Consolas", 10))
        entry_key.pack(side='left', padx=10)
        btn_save = ttk.Button(f_top, text="ì €ì¥", command=self._save_key, style="Primary.TButton")
        btn_save.pack(side='left')
        ToolTip(btn_save, "ì…ë ¥í•œ API Keyë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.")

        btn_help = ttk.Button(f_top, text="â“ ì‚¬ìš© ê°€ì´ë“œ", command=self._show_help_dialog)
        btn_help.pack(side='right')

        # 2. ë©”ì¸ íƒ­
        self.main_tabs = ttk.Notebook(main_container)
        self.main_tabs.pack(fill='both', expand=True, pady=5)
        
        self.tab_create = ttk.Frame(self.main_tabs)
        self.tab_manage = ttk.Frame(self.main_tabs) 
        self.tab_merge = ttk.Frame(self.main_tabs)
        
        self.main_tabs.add(self.tab_create, text=" 1. ë°°ì¹˜ ìƒì„± & ì—…ë¡œë“œ ")
        self.main_tabs.add(self.tab_manage, text=" 2. ë°°ì¹˜ ê´€ë¦¬ (ëª©ë¡/ë³‘í•©/ë¦¬í¬íŠ¸) ")
        self.main_tabs.add(self.tab_merge, text=" 3. ê°œë³„ ë³‘í•© (ìˆ˜ë™) ")
        
        self._init_tab_create()
        self._init_tab_manage()
        self._init_tab_merge()
        
        # 3. ë¡œê·¸
        f_log = ttk.LabelFrame(main_container, text="ğŸ“‹ ì‹œìŠ¤í…œ ë¡œê·¸", padding=10)
        f_log.pack(fill='both', expand=True, pady=(10, 0))
        self.log_widget = ScrolledText(f_log, height=12, state='disabled', font=("Consolas", 9), bg="#F1F3F5")
        self.log_widget.pack(fill='both', expand=True)

    def _load_key(self):
        loaded = load_api_key_from_file(API_KEY_FILE)
        if loaded: self.api_key_var.set(loaded)

    def _save_key(self):
        k = self.api_key_var.get().strip()
        if k:
            save_api_key_to_file(k, API_KEY_FILE)
            messagebox.showinfo("ì €ì¥", "API Key ì €ì¥ ì™„ë£Œ")

    # [Thread-Safe Log]
    def append_log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        full_msg = f"[{ts}] {msg}"
        def _update():
            if not hasattr(self, 'log_widget'): return
            try:
                self.log_widget.config(state='normal')
                self.log_widget.insert('end', full_msg + "\n")
                self.log_widget.see('end')
                self.log_widget.config(state='disabled')
            except: pass
        self.after(0, _update)

    # [Thread-Safe Messagebox]
    def _safe_msgbox(self, type_, title, msg):
        self.after(0, lambda: getattr(messagebox, type_)(title, msg))

    def _show_help_dialog(self):
        msg = (
            "[Stage 1 Batch API ì‚¬ìš© ê°€ì´ë“œ]\n\n"
            "1. [ë°°ì¹˜ ìƒì„± íƒ­]:\n"
            "   - ì›ë³¸ ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ê³  'Start Batch'ë¥¼ í´ë¦­í•˜ì„¸ìš”.\n"
            "   - 'gpt-5-mini' ëª¨ë¸ ì‚¬ìš© ì‹œ ë¹„ìš© íš¨ìœ¨ê³¼ ì†ë„ê°€ ì¢‹ìŠµë‹ˆë‹¤.\n\n"
            "2. [ë°°ì¹˜ ê´€ë¦¬ íƒ­]:\n"
            "   - [ìë™ ê°±ì‹ ]ì„ ì¼œë‘ë©´ ì£¼ê¸°ì ìœ¼ë¡œ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n"
            "   - 'completed' ìƒíƒœê°€ ë˜ë©´ [ì„ íƒ ì¼ê´„ ë³‘í•©] -> [ì •ì œ ë¦¬í¬íŠ¸] ìˆœìœ¼ë¡œ ì§„í–‰í•˜ì„¸ìš”.\n"
            "   - ë¦¬í¬íŠ¸ì—ì„œ ì›ë³¸ vs ì •ì œê²°ê³¼ì˜ ê¸€ì ìˆ˜ ë³€í™”ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
            "â€» ê²°ê³¼ëŠ” ì›ë³¸ ì—‘ì…€ì˜ 'ST1_ê²°ê³¼ìƒí’ˆëª…' ì»¬ëŸ¼ì— ë³‘í•©ë©ë‹ˆë‹¤."
        )
        messagebox.showinfo("ì‚¬ìš©ë²•", msg)

    # ----------------------------------------------------
    # Tab 1: Create
    # ----------------------------------------------------
    def _init_tab_create(self):
        container = ttk.Frame(self.tab_create, padding=20)
        container.pack(fill='both', expand=True)
        
        # Step 1: íŒŒì¼
        f_file = ttk.LabelFrame(container, text="1. ì‘ì—… ëŒ€ìƒ íŒŒì¼ ì„ íƒ", padding=15)
        f_file.pack(fill='x', pady=(0, 15))
        ttk.Entry(f_file, textvariable=self.src_file_var, font=("ë§‘ì€ ê³ ë”•", 10)).pack(side='left', fill='x', expand=True)
        btn_file = ttk.Button(f_file, text="ğŸ“‚ íŒŒì¼ ì°¾ê¸°", command=self._select_src_file)
        btn_file.pack(side='right', padx=5)
        ToolTip(btn_file, "Stage 1ì„ ìˆ˜í–‰í•  ì›ë³¸ ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.\n(ì¹´í…Œê³ ë¦¬ëª…, ì›ë³¸ìƒí’ˆëª… ì»¬ëŸ¼ í•„ìˆ˜)")
        
        # Step 2: ì˜µì…˜
        f_opt = ttk.LabelFrame(container, text="2. ë°°ì¹˜ ì˜µì…˜ ì„¤ì •", padding=15)
        f_opt.pack(fill='x', pady=5)
        
        # ëª¨ë¸
        fr1 = ttk.Frame(f_opt)
        fr1.pack(fill='x', pady=5)
        ttk.Label(fr1, text="ëª¨ë¸ (Model):", width=12).pack(side='left')
        models = list(MODEL_PRICING_USD_PER_MTOK.keys())
        cb_model = ttk.Combobox(fr1, textvariable=self.model_var, values=models, state="readonly", width=20)
        cb_model.pack(side='left', padx=5)
        ToolTip(cb_model, "Stage 1ì€ gpt-5-miniê°€ ê°€ì¥ íš¨ìœ¨ì ì…ë‹ˆë‹¤.")
        
        # Effort
        ttk.Label(fr1, text="Reasoning Effort:", width=15).pack(side='left', padx=(20, 5))
        cb_effort = ttk.Combobox(fr1, textvariable=self.effort_var, values=["low", "medium", "high"], state="readonly", width=12)
        cb_effort.pack(side='left', padx=5)
        ToolTip(cb_effort, "í…ìŠ¤íŠ¸ ì •ì œëŠ” 'low'ë§Œìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.")
        
        # ì²´í¬ë°•ìŠ¤
        fr2 = ttk.Frame(f_opt)
        fr2.pack(fill='x', pady=5)
        chk_skip = ttk.Checkbutton(fr2, text=" ì´ë¯¸ ST1_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ì€ ê±´ë„ˆë›°ê¸° (Skip)", variable=self.skip_exist_var)
        chk_skip.pack(side='left', padx=5)
        ToolTip(chk_skip, "ì¤‘ë³µ ê³¼ê¸ˆ ë°©ì§€ë¥¼ ìœ„í•´ ì´ë¯¸ ê²°ê³¼ê°€ ìˆëŠ” í–‰ì€ ì œì™¸í•©ë‹ˆë‹¤.")

        # Step 3: ì‹¤í–‰
        f_step3 = ttk.LabelFrame(container, text="3. ì‹¤í–‰", padding=15)
        f_step3.pack(fill='x', pady=15)
        btn_run = ttk.Button(f_step3, text="ğŸš€ JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (Start Batch)", command=self._start_create_batch, style="Success.TButton")
        btn_run.pack(fill='x', ipady=8)
        ttk.Label(container, text="â€» ë°°ì¹˜ APIëŠ” ê²°ê³¼ ìˆ˜ì‹ ê¹Œì§€ ìµœëŒ€ 24ì‹œê°„ ì†Œìš” (ë¹„ìš© 50% ì ˆê°)", foreground="#666").pack()

    def _select_src_file(self):
        p = filedialog.askopenfilename(filetypes=[("Excel", "*.xlsx;*.xls")])
        if p: self.src_file_var.set(p)

    def _start_create_batch(self):
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Key í•„ìš”")
            return
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ ì„ íƒ í•„ìš”")
            return
        t = threading.Thread(target=self._run_create_batch)
        t.daemon = True
        t.start()

    def _run_create_batch(self):
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        model = self.model_var.get()
        effort = self.effort_var.get()
        
        try:
            client = OpenAI(api_key=key)
            self.append_log(f"ì—‘ì…€ ë¡œë“œ ì¤‘... {os.path.basename(src)}")
            df = pd.read_excel(src)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            req_cols = ['ì¹´í…Œê³ ë¦¬ëª…', 'ì›ë³¸ìƒí’ˆëª…']
            for c in req_cols:
                if c not in df.columns:
                    raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼({c})ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            jsonl_lines = []
            skipped_cnt = 0
            
            for idx, row in df.iterrows():
                if self.skip_exist_var.get() and "ST1_ê²°ê³¼ìƒí’ˆëª…" in df.columns:
                    val = safe_str(row.get("ST1_ê²°ê³¼ìƒí’ˆëª…", ""))
                    if val and val != "nan":
                        continue
                
                # Core í•¨ìˆ˜ í˜¸ì¶œ
                payload = build_stage1_batch_payload(idx, row, model, effort)
                if payload:
                    jsonl_lines.append(json.dumps(payload, ensure_ascii=False))
                else:
                    skipped_cnt += 1
            
            if not jsonl_lines:
                self.append_log("ìƒì„±í•  ìš”ì²­ ì—†ìŒ.")
                return

            # [Fix] BASE_DIR ì‚¬ìš© (Exe í™˜ê²½ ëŒ€ì‘)
            base_name, _ = os.path.splitext(os.path.basename(src))
            # ê°™ì€ í´ë”ì— JSONL ìƒì„±
            jsonl_path = os.path.join(os.path.dirname(src), f"{base_name}_stage1_batch_input.jsonl")
            
            with open(jsonl_path, "w", encoding="utf-8") as f:
                f.write("\n".join(jsonl_lines))
            
            self.append_log(f"JSONL ìƒì„± ì™„ë£Œ: {len(jsonl_lines)}ê±´ (ìŠ¤í‚µ {skipped_cnt}ê±´)")
            
            # JSONL íŒŒì¼ í¬ê¸° í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            num_requests = len(jsonl_lines)
            
            # ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ : 180MB ì´ìƒì´ë©´ ë¶„í•  ì²˜ë¦¬ (OpenAI Batch API ì œí•œ: 200MB)
            # ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨ (500ê°œ ì œí•œ ì œê±°)
            MAX_FILE_SIZE_MB = 180
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸°ê°€ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤: {jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB")
                
                # ì²­í¬ ë¶„í•  ì²˜ë¦¬ (ìš©ëŸ‰ ê¸°ì¤€ë§Œ ì‚¬ìš©, ìš”ì²­ ìˆ˜ ì œí•œì€ ë§¤ìš° í¬ê²Œ ì„¤ì •)
                batch_ids = self._create_batch_chunks(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model,
                    effort=effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=999999,  # ìš”ì²­ ìˆ˜ ì œí•œ ê±°ì˜ ì œê±° (ìš©ëŸ‰ì´ ìš°ì„ )
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}")
            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                # ë™ì¼í•œ ì—‘ì…€ íŒŒì¼ì—ì„œ ìƒì„±ëœ ê¸°ì¡´ ë°°ì¹˜ ê·¸ë£¹ì´ ìˆëŠ”ì§€ í™•ì¸
                import uuid
                jobs = load_batch_jobs()
                existing_group_id = None
                existing_chunks = []
                
                # ê°™ì€ ì—‘ì…€ íŒŒì¼ì—ì„œ ìƒì„±ëœ ë°°ì¹˜ ì¤‘ ê·¸ë£¹ì´ ìˆëŠ”ì§€ ì°¾ê¸°
                for j in jobs:
                    if j.get("src_excel") == src and j.get("batch_group_id"):
                        existing_group_id = j.get("batch_group_id")
                        existing_chunks.append(j.get("chunk_index", 0))
                        break
                
                if existing_group_id:
                    # ê¸°ì¡´ ê·¸ë£¹ì´ ìˆìœ¼ë©´ ê°™ì€ ê·¸ë£¹ì— ì¶”ê°€
                    batch_group_id = existing_group_id
                    max_chunk_index = max(existing_chunks) if existing_chunks else 0
                    chunk_index = max_chunk_index + 1
                    # ê¸°ì¡´ ë°°ì¹˜ë“¤ì˜ total_chunks ì—…ë°ì´íŠ¸ í•„ìš” (ë‚˜ì¤‘ì— ì²˜ë¦¬)
                    total_chunks = max_chunk_index + 1  # ì¼ë‹¨ í˜„ì¬ê¹Œì§€ì˜ ì²­í¬ ìˆ˜
                    self.append_log(f"[INFO] ê¸°ì¡´ ê·¸ë£¹ì— ì¶”ê°€: {batch_group_id} (ì²­í¬ {chunk_index})")
                else:
                    # ìƒˆ ê·¸ë£¹ ìƒì„±
                    batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
                    chunk_index = 1
                    total_chunks = 1
                    self.append_log(f"[INFO] ìƒˆ ê·¸ë£¹ ìƒì„±: {batch_group_id}")
                
                self.append_log("OpenAI ì—…ë¡œë“œ ì¤‘...")
                with open(jsonl_path, "rb") as f:
                    batch_input_file = client.files.create(file=f, purpose="batch")
                
                file_id = batch_input_file.id
                self.append_log(f"ì—…ë¡œë“œ ì™„ë£Œ ID: {file_id}")
                
                # Responses ì—”ë“œí¬ì¸íŠ¸ë¡œ Batch ìƒì„± (coreì™€ ë™ì¼)
                batch_job = client.batches.create(
                    input_file_id=file_id,
                    endpoint="/v1/responses",
                    completion_window="24h",
                )
                
                batch_id = batch_job.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id} (ê·¸ë£¹ ID: {batch_group_id})")
                
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src,
                    jsonl_path=jsonl_path,
                    model=model,
                    effort=effort,
                    status=batch_job.status,
                    output_file_id=None,
                    batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                    chunk_index=chunk_index,
                    total_chunks=total_chunks,
                )
                
                # ê¸°ì¡´ ê·¸ë£¹ì— ì¶”ê°€í•œ ê²½ìš°, ê¸°ì¡´ ë°°ì¹˜ë“¤ì˜ total_chunks ì—…ë°ì´íŠ¸
                if existing_group_id:
                    updated_count = 0
                    for j in jobs:
                        if j.get("batch_group_id") == batch_group_id:
                            j["total_chunks"] = total_chunks
                            updated_count += 1
                    if updated_count > 0:
                        save_batch_jobs(jobs)
                        self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— T1 ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡
                try:
                    root_name = get_root_filename(src)
                    JobManager.update_status(root_name, text_msg="T1 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T1 (ì§„í–‰ì¤‘)")
                except Exception:
                    # ëŸ°ì²˜ê°€ ì—†ê±°ë‚˜ job_history.json ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ìš©íˆ ë¬´ì‹œ
                    pass
                self._safe_msgbox("showinfo", "ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {batch_id}")
            
            self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            self._safe_msgbox("showerror", "ì—ëŸ¬", str(e))
    
    def _create_batch_chunks(self, client, jsonl_path, excel_path, model_name, effort, max_size_mb=180, max_requests=999999):
        """
        í° JSONL íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ batch_group_idë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        """
        import uuid
        
        # ë°°ì¹˜ ê·¸ë£¹ ID ìƒì„± (ê°™ì€ ì—‘ì…€ì—ì„œ ë¶„í• ëœ ë°°ì¹˜ë“¤ì„ ë¬¶ìŒ)
        batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
        
        # JSONL íŒŒì¼ ì½ê¸°
        requests = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    requests.append(json.loads(line))
        
        total_requests = len(requests)
        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (ìš©ëŸ‰ ê¸°ì¤€ë§Œ ì‚¬ìš©, ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
        original_file_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
        estimated_total_chunks = max(1, int(original_file_size_mb / max_size_mb) + 1)
        
        self.append_log(f"[INFO] ì´ {total_requests}ê°œ ìš”ì²­ì„ ë¶„í• í•©ë‹ˆë‹¤... (ì˜ˆìƒ: ì•½ {estimated_total_chunks}ê°œ ì²­í¬, ê·¸ë£¹ ID: {batch_group_id})")
        
        # base ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜ (ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹œ ì‚¬ìš©)
        base, ext = os.path.splitext(jsonl_path)
        
        batch_ids = []
        chunk_num = 0
        chunk_files_created = []  # ìƒì„±ëœ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì •ë¦¬ìš©)
        failed_chunk_files = []  # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì¬ì‹œë„ìš©)
        
        i = 0
        while i < total_requests:
            chunk_num += 1
            chunk_requests = []
            chunk_size_bytes = 0  # ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì •í™•íˆ ê³„ì‚°
            
            # ì²­í¬ ìƒì„± (ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ , ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
            # ì‹¤ì œ íŒŒì¼ í¬ê¸°ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ JSON ì§ë ¬í™” + ì¤„ë°”ê¿ˆ ë¬¸ì ê³ ë ¤
            while i < total_requests:
                req_json = json.dumps(requests[i], ensure_ascii=False)
                req_size_bytes = len(req_json.encode('utf-8')) + 1  # +1 for newline
                
                # ìš©ëŸ‰ì´ ìš°ì„ : ë‹¤ìŒ ìš”ì²­ì„ ì¶”ê°€í•˜ë©´ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ ë§ˆì§„ 5% í¬í•¨)
                if chunk_size_bytes + req_size_bytes > max_size_mb * 1024 * 1024 * 0.95:
                    break
                
                chunk_requests.append(requests[i])
                chunk_size_bytes += req_size_bytes
                i += 1
            
            if not chunk_requests:
                break
            
            # ì²­í¬ JSONL íŒŒì¼ ìƒì„±
            chunk_jsonl_path = f"{base}_chunk{chunk_num:03d}{ext}"
            chunk_files_created.append(chunk_jsonl_path)
            
            with open(chunk_jsonl_path, "w", encoding="utf-8") as f:
                for req in chunk_requests:
                    f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            chunk_size_mb = os.path.getsize(chunk_jsonl_path) / (1024 * 1024)
            # ì‹¤ì œ ìƒì„±ëœ ì²­í¬ ìˆ˜ë¡œ í‘œì‹œ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë  ìˆ˜ ìˆìŒ)
            self.append_log(f"[INFO] ì²­í¬ {chunk_num}: {len(chunk_requests)}ê°œ ìš”ì²­, {chunk_size_mb:.2f} MB")
            
            # ë°°ì¹˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 3
            retry_count = 0
            batch_created = False
            
            while retry_count < max_retries and not batch_created:
                try:
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹œë„ ì¤‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                    
                    # íŒŒì¼ ì—…ë¡œë“œ
                    with open(chunk_jsonl_path, "rb") as f:
                        batch_input_file = client.files.create(file=f, purpose="batch")
                    
                    file_id = batch_input_file.id
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ì—…ë¡œë“œ ì™„ë£Œ ID: {file_id}")
                    
                    # Responses ì—”ë“œí¬ì¸íŠ¸ë¡œ Batch ìƒì„±
                    batch_job = client.batches.create(
                        input_file_id=file_id,
                        endpoint="/v1/responses",
                        completion_window="24h",
                    )
                    
                    batch_id = batch_job.id
                    batch_ids.append(batch_id)
                    self.append_log(f"âœ… ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_id}")
                    batch_created = True
                    
                    # ì‘ì—… ì´ë ¥ ê¸°ë¡ (ê·¸ë£¹ ì •ë³´ í¬í•¨)
                    # total_chunksëŠ” ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë˜ë¯€ë¡œ ì¼ë‹¨ chunk_numìœ¼ë¡œ ì„¤ì •
                    upsert_batch_job(
                        batch_id=batch_id,
                        src_excel=excel_path,
                        jsonl_path=chunk_jsonl_path,
                        model=model_name,
                        effort=effort,
                        status=batch_job.status,
                        output_file_id=None,
                        batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                        chunk_index=chunk_num,  # ì²­í¬ ë²ˆí˜¸
                        total_chunks=chunk_num,  # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì²­í¬ ìˆ˜ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨)
                    )
                    
                    # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— T1 ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡ (ì²« ë²ˆì§¸ ì²­í¬ë§Œ)
                    if chunk_num == 1:
                        try:
                            root_name = get_root_filename(excel_path)
                            JobManager.update_status(root_name, text_msg="T1 (ì§„í–‰ì¤‘)")
                            self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T1 (ì§„í–‰ì¤‘)")
                        except Exception:
                            pass
                            
                except Exception as e:
                    error_str = str(e).lower()
                    is_token_limit_error = "enqueued token limit" in error_str or "token limit reached" in error_str
                    
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                        self.append_log(f"âš ï¸ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                        if is_token_limit_error:
                            self.append_log(f"[WARN] í† í° ì œí•œ ì˜¤ë¥˜ ê°ì§€. ë” ê¸´ ëŒ€ê¸° ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            wait_time = max(wait_time, 30)  # í† í° ì œí•œ ì˜¤ë¥˜ëŠ” ìµœì†Œ 30ì´ˆ ëŒ€ê¸°
                        self.append_log(f"[INFO] {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        import time
                        time.sleep(wait_time)
                    else:
                        self.append_log(f"âŒ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}")
                        if is_token_limit_error:
                            self.append_log(f"[INFO] í† í° ì œí•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì¼ë¶€ ë°°ì¹˜ê°€ ì™„ë£Œëœ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                            self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼: {chunk_jsonl_path}")
                        import traceback
                        self.append_log(traceback.format_exc())
                        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìœ ì§€ (ìˆ˜ë™ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                        failed_chunk_files.append({
                            "chunk_num": chunk_num,
                            "chunk_file": chunk_jsonl_path,
                            "error": str(e),
                            "is_token_limit": is_token_limit_error,
                            "excel_path": excel_path,
                            "model_name": model_name,
                            "effort": effort,
                            "batch_group_id": batch_group_id,
                        })
        
        # ëª¨ë“  ì²­í¬ ìƒì„± ì™„ë£Œ í›„, total_chunksë¥¼ ì‹¤ì œ ìƒì„±ëœ ë°°ì¹˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        actual_total_chunks = len(batch_ids)
        if actual_total_chunks > 0:
            self.append_log(f"[INFO] ì´ {actual_total_chunks}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ. ì‘ì—… ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
            # ê²½ìŸ ì¡°ê±´ ë°©ì§€ë¥¼ ìœ„í•´ ì›ìì  ì—…ë°ì´íŠ¸
            jobs = load_batch_jobs()
            updated_count = 0
            for j in jobs:
                if j.get("batch_group_id") == batch_group_id:
                    j["total_chunks"] = actual_total_chunks
                    updated_count += 1
            if updated_count > 0:
                save_batch_jobs(jobs)
                self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {actual_total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.append_log(f"âš ï¸ ìƒì„±ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì„±ê³µí•œ ë°°ì¹˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²­í¬ íŒŒì¼ ì •ë¦¬ ì˜µì…˜ ì œê³µ (í˜„ì¬ëŠ” ìœ ì§€)
        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìˆ˜ë™ ì¬ì‹œë„ë¥¼ ìœ„í•´ ìœ ì§€
        if actual_total_chunks < chunk_num:
            failed_chunks = chunk_num - actual_total_chunks
            self.append_log(f"âš ï¸ {failed_chunks}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²­í¬ íŒŒì¼ì€ ìœ ì§€ë©ë‹ˆë‹¤.")
            
            # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ì„ ëª…í™•íˆ í‘œì‹œ
            if failed_chunk_files:
                self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡:")
                for failed_info in failed_chunk_files:
                    self.append_log(f"  - ì²­í¬ {failed_info['chunk_num']}: {os.path.basename(failed_info['chunk_file'])}")
                    if failed_info['is_token_limit']:
                        self.append_log(f"    â†’ í† í° ì œí•œ ì˜¤ë¥˜. ì¼ë¶€ ë°°ì¹˜ ì™„ë£Œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                
                # ì‹¤íŒ¨ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                failed_info_path = f"{base}_failed_chunks.json"
                try:
                    with open(failed_info_path, "w", encoding="utf-8") as f:
                        json.dump(failed_chunk_files, f, indent=2, ensure_ascii=False)
                    self.append_log(f"[INFO] ì‹¤íŒ¨ ì •ë³´ ì €ì¥: {os.path.basename(failed_info_path)}")
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return batch_ids

    # ----------------------------------------------------
    # Tab 2: Manage
    # ----------------------------------------------------
    def _init_tab_manage(self):
        container = ttk.Frame(self.tab_manage, padding=10)
        container.pack(fill='both', expand=True)
        sub_tabs = ttk.Notebook(container)
        sub_tabs.pack(fill='both', expand=True)
        
        self.sub_active = ttk.Frame(sub_tabs, padding=10)
        self.sub_archive = ttk.Frame(sub_tabs, padding=10)
        sub_tabs.add(self.sub_active, text=" â–¶ ì§„í–‰ì¤‘ / ì™„ë£Œ (Active) ")
        sub_tabs.add(self.sub_archive, text=" ğŸ—‘ íœ´ì§€í†µ (Archive) ")
        
        # Active UI
        f_ctrl = ttk.Frame(self.sub_active)
        f_ctrl.pack(fill='x', pady=(0, 10))
        
        # ìë™ ê°±ì‹  ì˜µì…˜
        f_auto = ttk.Frame(f_ctrl)
        f_auto.pack(side='left', padx=5)
        ttk.Checkbutton(f_auto, text="ìë™ ìƒíƒœ ê°±ì‹ ", variable=self.auto_refresh_var).pack(side='left')
        ttk.Spinbox(f_auto, from_=10, to=600, textvariable=self.refresh_interval_var, width=4).pack(side='left', padx=2)
        ttk.Label(f_auto, text="ì´ˆ").pack(side='left')
        
        ttk.Button(f_ctrl, text="ğŸ”„ ìˆ˜ë™ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active)).pack(side='left', padx=10)
        ttk.Button(f_ctrl, text="ğŸ“¥ ì¼ê´„ ë³‘í•©", command=self._merge_selected, style="Primary.TButton").pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ“Š ì •ì œ ë¦¬í¬íŠ¸", command=self._report_selected_unified, style="Success.TButton").pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ—‘ íœ´ì§€í†µ ì´ë™", command=self._archive_selected, style="Danger.TButton").pack(side='right', padx=2)
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ë²„íŠ¼ ì¶”ê°€
        f_group_ctrl = ttk.Frame(self.sub_active)
        f_group_ctrl.pack(fill='x', pady=(0, 5))
        ttk.Label(f_group_ctrl, text="ğŸ’¡ ê·¸ë£¹ í—¤ë”ë¥¼ ë”ë¸”í´ë¦­í•˜ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°ê°€ ë©ë‹ˆë‹¤.", 
                 font=("ë§‘ì€ ê³ ë”•", 8), foreground="#666").pack(side='left', padx=5)
        ttk.Button(f_group_ctrl, text="ğŸ“‚ ëª¨ë“  ê·¸ë£¹ í¼ì¹˜ê¸°", command=lambda: self._expand_all_groups(self.tree_active)).pack(side='right', padx=2)
        ttk.Button(f_group_ctrl, text="ğŸ“ ëª¨ë“  ê·¸ë£¹ ì ‘ê¸°", command=lambda: self._collapse_all_groups(self.tree_active)).pack(side='right', padx=2)
        
        # [Model / Effort ì»¬ëŸ¼ í¬í•¨ + group ì»¬ëŸ¼ ì¶”ê°€]
        cols = ("batch_id", "status", "created", "completed", "model", "effort", "counts", "group")
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš© (íŠ¸ë¦¬ ì•„ì´ì½˜ + ì»¬ëŸ¼ í—¤ë”)
        self.tree_active = ttk.Treeview(self.sub_active, columns=cols, show='tree headings', height=15, selectmode='extended')
        self.tree_active.tag_configure('odd', background=COLOR_WHITE)
        self.tree_active.tag_configure('even', background='#F2F7FF')
        self.tree_active.tag_configure('group', background='#E8F5E9')  # ê·¸ë£¹ ë°°ì¹˜ ê°•ì¡°
        self.tree_active.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))  # ê·¸ë£¹ í—¤ë” ê°•ì¡°
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_active.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_active.heading("status", text="ìƒíƒœ")
        self.tree_active.heading("created", text="ìƒì„±ì¼")
        self.tree_active.heading("completed", text="ì™„ë£Œì¼")
        self.tree_active.heading("model", text="ëª¨ë¸")
        self.tree_active.heading("effort", text="Effort")
        self.tree_active.heading("counts", text="ìš”ì²­ìˆ˜")
        self.tree_active.heading("group", text="ê·¸ë£¹")
        # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •: ê·¸ë£¹ëª…ì´ ê¸¸ì–´ì„œ íŠ¸ë¦¬ ì»¬ëŸ¼ í™•ëŒ€, ì¼ë¶€ ì»¬ëŸ¼ì€ ì¶•ì†Œ
        self.tree_active.column("#0", width=350, anchor="w")  # íŠ¸ë¦¬ ì»¬ëŸ¼ (ê·¸ë£¹ëª… í‘œì‹œ)
        self.tree_active.column("batch_id", width=200, anchor="w")
        self.tree_active.column("status", width=100, anchor="center")
        self.tree_active.column("created", width=120, anchor="center")
        self.tree_active.column("completed", width=120, anchor="center")
        self.tree_active.column("model", width=100, anchor="center")
        self.tree_active.column("effort", width=70, anchor="center")
        self.tree_active.column("counts", width=80, anchor="center")
        self.tree_active.column("group", width=100, anchor="center")
        self.tree_active.pack(fill='both', expand=True, padx=5, pady=5)
        
        self.menu_active = Menu(self, tearoff=0)
        self.menu_active.add_command(label="ìƒíƒœ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active))
        self.menu_active.add_command(label="ê²°ê³¼ ë³‘í•©", command=self._merge_selected)
        self.menu_active.add_command(label="ì •ì œ ë¦¬í¬íŠ¸ ìƒì„±", command=self._report_selected_unified)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="íœ´ì§€í†µìœ¼ë¡œ ì´ë™", command=self._archive_selected)
        self.tree_active.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_active, self.menu_active))
        self.tree_active.bind("<Double-1>", self._on_tree_double_click)
        self.tree_arch.bind("<Double-1>", self._on_tree_double_click)

        # Archive UI
        f_arch_ctrl = ttk.Frame(self.sub_archive)
        f_arch_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_arch_ctrl, text="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected, style="Primary.TButton").pack(side='left')
        ttk.Button(f_arch_ctrl, text="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected, style="Danger.TButton").pack(side='right')
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ë²„íŠ¼ ì¶”ê°€
        f_arch_group_ctrl = ttk.Frame(self.sub_archive)
        f_arch_group_ctrl.pack(fill='x', pady=(0, 5))
        ttk.Label(f_arch_group_ctrl, text="ğŸ’¡ ê·¸ë£¹ í—¤ë”ë¥¼ ë”ë¸”í´ë¦­í•˜ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°ê°€ ë©ë‹ˆë‹¤.", 
                 font=("ë§‘ì€ ê³ ë”•", 8), foreground="#666").pack(side='left', padx=5)
        ttk.Button(f_arch_group_ctrl, text="ğŸ“‚ ëª¨ë“  ê·¸ë£¹ í¼ì¹˜ê¸°", command=lambda: self._expand_all_groups(self.tree_arch)).pack(side='right', padx=2)
        ttk.Button(f_arch_group_ctrl, text="ğŸ“ ëª¨ë“  ê·¸ë£¹ ì ‘ê¸°", command=lambda: self._collapse_all_groups(self.tree_arch)).pack(side='right', padx=2)
        
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš©
        self.tree_arch = ttk.Treeview(self.sub_archive, columns=cols, show='tree headings', height=15, selectmode='extended')
        self.tree_arch.tag_configure('odd', background=COLOR_WHITE)
        self.tree_arch.tag_configure('even', background='#FFF2F2')
        self.tree_arch.tag_configure('group', background='#FFE8E8')  # ê·¸ë£¹ ë°°ì¹˜ ê°•ì¡°
        self.tree_arch.tag_configure('group_header', background='#FFCDD2', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))  # ê·¸ë£¹ í—¤ë” ê°•ì¡°
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_arch.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_arch.heading("status", text="ìƒíƒœ")
        self.tree_arch.heading("created", text="ìƒì„±ì¼")
        self.tree_arch.heading("completed", text="ì™„ë£Œì¼")
        self.tree_arch.heading("model", text="ëª¨ë¸")
        self.tree_arch.heading("effort", text="Effort")
        self.tree_arch.heading("counts", text="ìš”ì²­ìˆ˜")
        self.tree_arch.heading("group", text="ê·¸ë£¹")
        # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •: ê·¸ë£¹ëª…ì´ ê¸¸ì–´ì„œ íŠ¸ë¦¬ ì»¬ëŸ¼ í™•ëŒ€, ì¼ë¶€ ì»¬ëŸ¼ì€ ì¶•ì†Œ
        self.tree_arch.column("#0", width=350, anchor="w")  # íŠ¸ë¦¬ ì»¬ëŸ¼ (ê·¸ë£¹ëª… í‘œì‹œ)
        self.tree_arch.column("batch_id", width=200, anchor="w")
        self.tree_arch.column("status", width=100, anchor="center")
        self.tree_arch.column("created", width=120, anchor="center")
        self.tree_arch.column("completed", width=120, anchor="center")
        self.tree_arch.column("model", width=100, anchor="center")
        self.tree_arch.column("effort", width=70, anchor="center")
        self.tree_arch.column("counts", width=80, anchor="center")
        self.tree_arch.column("group", width=100, anchor="center")
        self.tree_arch.pack(fill='both', expand=True)
        
        self._load_jobs_all()
        self._load_archive_list()

    def _expand_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ í¼ì¹©ë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=True)
    
    def _collapse_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ ì ‘ìŠµë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=False)

    def _auto_refresh_loop(self):
        """ìë™ ìƒíƒœ ê°±ì‹  ë£¨í”„"""
        if self.auto_refresh_var.get() and not self.is_refreshing:
            # merged, failed ë“± ì´ë¯¸ ëë‚œ ìƒíƒœëŠ” ì¡°íšŒ ëŒ€ìƒì—ì„œ ì œì™¸
            jobs = load_batch_jobs()
            active_ids = [
                j['batch_id'] for j in jobs 
                if not j.get('archived') and j.get('status') not in ['completed', 'failed', 'expired', 'cancelled', 'merged']
            ]
            if active_ids:
                t = threading.Thread(target=self._run_refresh_ids, args=(active_ids, True))
                t.daemon = True
                t.start()
        
        interval = max(10, self.refresh_interval_var.get()) * 1000
        self.after(interval, self._auto_refresh_loop)

    def _show_context_menu(self, event, tree, menu):
        item = tree.identify_row(event.y)
        if item:
            if item not in tree.selection(): tree.selection_set(item)
            menu.post(event.x_root, event.y_root)

    def _get_selected_ids(self, tree):
        """
        ì„ íƒëœ í•­ëª©ì—ì„œ ë°°ì¹˜ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê·¸ë£¹ í—¤ë”ë¥¼ ì„ íƒí•˜ë©´ ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        selection = tree.selection()
        ids = []
        
        for item in selection:
            vals = tree.item(item)['values']
            batch_id = vals[0] if vals else ""
            
            # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
            if not batch_id:
                # ê·¸ë£¹ í—¤ë”ì˜ ìì‹ ë…¸ë“œë“¤(ë°°ì¹˜ë“¤)ì„ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
                children = tree.get_children(item)
                for child in children:
                    child_vals = tree.item(child)['values']
                    if child_vals and child_vals[0]:
                        ids.append(child_vals[0])
            else:
                # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°
                ids.append(batch_id)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(ids))

    def _load_jobs_all(self):
        if not hasattr(self, 'tree_active'): return
        for i in self.tree_active.get_children(): self.tree_active.delete(i)
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ì„ ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0]
            created_at = first_job.get("created_at", "")
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = dt.strftime("%m-%d %H:%M")
                except:
                    date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
            else:
                date_str = "-"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
            group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
            group_node = self.tree_active.insert("", "end", 
                text=group_header_text,
                values=("", "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                tag = 'group'
                self.tree_active.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            tag = 'even'
            self.tree_active.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))

    def _load_archive_list(self):
        if not hasattr(self, 'tree_arch'): return
        for i in self.tree_arch.get_children(): self.tree_arch.delete(i)
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if not j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs)) if group_jobs else len(group_jobs)
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0]
            created_at = first_job.get("created_at", "")
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = dt.strftime("%m-%d %H:%M")
                except:
                    date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
            else:
                date_str = "-"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
            group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
            group_node = self.tree_arch.insert("", "end", 
                text=group_header_text,
                values=("", "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                tag = 'group'
                self.tree_arch.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            tag = 'even'
            self.tree_arch.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))

    # --- Actions ---
    def _refresh_selected(self, tree):
        ids = self._get_selected_ids(tree)
        if not ids: return
        jobs = load_batch_jobs()
        completed = [bid for bid in ids if next((x for x in jobs if x["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        if completed:
            if messagebox.askyesno("í™•ì¸", f"{len(completed)}ê±´ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\në¯¸ì™„ë£Œ ê±´ë§Œ ê°±ì‹ í• ê¹Œìš”?"):
                ids = [i for i in ids if i not in completed]
        if not ids: return
        t = threading.Thread(target=self._run_refresh_ids, args=(ids,))
        t.daemon = True
        t.start()

    def _run_refresh_ids(self, ids, silent=False):
        if self.is_refreshing: return
        self.is_refreshing = True
        
        key = self.api_key_var.get().strip()
        if not key:
            self.is_refreshing = False
            return
            
        if not silent: self.append_log(f"ì„ íƒëœ {len(ids)}ê±´ ê°±ì‹  ì¤‘...")
        
        try:
            client = OpenAI(api_key=key)
            for bid in ids:
                try:
                    remote = client.batches.retrieve(bid)
                    rc = None
                    if remote.request_counts:
                        rc = {"total": remote.request_counts.total, "completed": remote.request_counts.completed, "failed": remote.request_counts.failed}
                    upsert_batch_job(bid, status=remote.status, output_file_id=remote.output_file_id, request_counts=rc)
                except Exception as e:
                    if not silent: self.append_log(f"{bid} ê°±ì‹  ì‹¤íŒ¨: {e}")
        finally:
            self.is_refreshing = False
            self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])
            if not silent: self.append_log("ê°±ì‹  ì™„ë£Œ")

    def _merge_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        targets = [bid for bid in ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "completed"]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ë³‘í•©í•  ìˆ˜ ìˆëŠ” 'completed' ìƒíƒœì˜ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if messagebox.askyesno("ë³‘í•©", f"ì„ íƒí•œ {len(targets)}ê±´ì„ ë³‘í•©í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_merge_multi, args=(targets,))
            t.daemon = True
            t.start()

    def _run_merge_multi(self, ids):
        key = self.api_key_var.get().strip()
        client = OpenAI(api_key=key)
        success_cnt = 0
        total_cost = 0.0
        
        for bid in ids:
            self.append_log(f"--- ë³‘í•© ì‹œì‘: {bid} ---")
            try:
                jobs = load_batch_jobs()
                local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                remote = client.batches.retrieve(bid)

                # output_file_id / output_file_ids ì²˜ë¦¬ (ì‹ ë²„ì „ í˜¸í™˜)
                output_file_id = getattr(remote, "output_file_id", None)
                if not output_file_id:
                    output_ids = getattr(remote, "output_file_ids", None)
                    if output_ids and isinstance(output_ids, (list, tuple)) and len(output_ids) > 0:
                        output_file_id = output_ids[0]
                if not output_file_id:
                    self.append_log(f"âŒ output_file_id ì—†ìŒ: {bid}")
                    continue

                file_content = client.files.content(output_file_id)
                if hasattr(file_content, "read"):
                    content_bytes = file_content.read()
                elif hasattr(file_content, "iter_bytes"):
                    chunks = []
                    for ch in file_content.iter_bytes():
                        chunks.append(ch)
                    content_bytes = b"".join(chunks)
                else:
                    content_bytes = file_content  # type: ignore

                if local_job and local_job.get("src_excel"):
                    src_path = local_job["src_excel"]
                    base_name, _ = os.path.splitext(os.path.basename(src_path))
                    base_dir = os.path.dirname(src_path)
                    # JSONLì€ ì›ë³¸ê³¼ ê°™ì€ í´ë”ì— ì €ì¥
                    out_jsonl = os.path.join(base_dir, f"{base_name}_stage1_batch_output.jsonl")
                    # í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ 1ë‹¨ê³„ ì™„ë£Œ íŒŒì¼ëª…: *_T1_I0.xlsx í˜•íƒœë¡œ ë²„ì „ ì—…
                    out_excel = get_next_version_path(src_path, task_type="text")
                else:
                    out_jsonl = os.path.join(BASE_DIR, f"output_{bid}.jsonl")
                    out_excel = os.path.join(BASE_DIR, f"output_{bid}.xlsx")
                    src_path = None

                with open(out_jsonl, "wb") as f:
                    f.write(content_bytes)

                results_map = {}
                batch_in_tok = 0
                batch_out_tok = 0

                with open(out_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip():
                            continue
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        resp = data.get("response")
                        error = data.get("error")
                        if error is not None or not resp or not cid:
                            continue

                        # core ì˜ íŒŒì„œ ì‚¬ìš© (Responses í¬ë§· ê¸°ì¤€)
                        refined = extract_text_from_response_dict(resp)
                        in_tok, out_tok, _ = extract_usage_from_response_dict(resp)
                        batch_in_tok += in_tok
                        batch_out_tok += out_tok
                        results_map[cid] = refined
                
                model_name = local_job.get("model", "gpt-5-mini") if local_job else "gpt-5-mini"
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0.25, "output": 2.0})
                # Batch í• ì¸(50%) ë°˜ì˜
                cost = ((batch_in_tok * pricing["input"] + batch_out_tok * pricing["output"]) / 1_000_000) * 0.5
                total_cost += cost

                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    target_col = "ST1_ê²°ê³¼ìƒí’ˆëª…"
                    if target_col not in df.columns:
                        df[target_col] = ""
                    df[target_col] = df[target_col].astype(str)
                    cnt = 0
                    for cid, val in results_map.items():
                        try:
                            # custom_id í˜•ì‹: row-123  â†’ ì¸ë±ìŠ¤ 123
                            idx = int(str(cid).split("-")[1])
                            if 0 <= idx < len(df):
                                df.at[idx, target_col] = val
                                cnt += 1
                        except Exception:
                            continue
                    # ì—‘ì…€ ì €ì¥ (ì—´ë ¤ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ ì €ì¥ ìœ í‹¸ ì‚¬ìš©)
                    if safe_save_excel(df, out_excel):
                        upsert_batch_job(bid, out_excel=out_excel, status="merged")

                        # History ê¸°ë¡ (Stage 1) - íƒ€ì„ì¡´ ì •ë³´ í˜¼í•© ë°©ì§€ë¥¼ ìœ„í•´ naive datetime ì‚¬ìš©
                        c_at_str = local_job.get("created_at", "")
                        if c_at_str:
                            try:
                                # ISO í˜•ì‹ íŒŒì‹± (Zë¥¼ +00:00ìœ¼ë¡œ ë³€í™˜)
                                c_at = datetime.fromisoformat(c_at_str.replace('Z', '+00:00'))
                                # ì‹œê°„ëŒ€ ì •ë³´ê°€ ìˆìœ¼ë©´ naiveë¡œ ë³€í™˜ (datetime.now()ì™€ ì¼ì¹˜)
                                if c_at.tzinfo is not None:
                                    c_at = c_at.replace(tzinfo=None)
                            except:
                                c_at = datetime.now()
                        else:
                            c_at = datetime.now()
                        finish_dt = datetime.now()
                        append_run_history(
                            stage="Stage 1 Batch",
                            model_name=model_name,
                            reasoning_effort=local_job.get("effort", "low"),
                            src_file=src_path,
                            out_file=out_excel,
                            total_rows=len(df),
                            api_rows=len(results_map),
                            elapsed_seconds=(finish_dt - c_at).total_seconds(),
                            total_in_tok=batch_in_tok,
                            total_out_tok=batch_out_tok,
                            total_reasoning_tok=0,
                            input_cost_usd=0,
                            output_cost_usd=0,
                            total_cost_usd=cost,
                            start_dt=c_at,
                            finish_dt=finish_dt,
                            api_type="batch",
                            batch_id=bid,
                            success_rows=cnt,
                            fail_rows=len(results_map) - cnt,
                        )

                        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage1 í…ìŠ¤íŠ¸(T1) ì™„ë£Œ ìƒíƒœ ê¸°ë¡
                        try:
                            root_name = get_root_filename(src_path)
                            JobManager.update_status(root_name, text_msg="T1 (ì™„ë£Œ)")
                            self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T1 (ì™„ë£Œ)")
                        except Exception as e:
                            self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")

                        self.append_log(f"âœ… ë³‘í•© ì™„ë£Œ: {os.path.basename(out_excel)}")
                        success_cnt += 1
                else:
                    self.append_log(f"âš ï¸ ì›ë³¸ ì—†ìŒ. JSONLë§Œ ì €ì¥.")
            except Exception as e:
                self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: {e}")
        
        self.append_log(f"=== ì¼ê´„ ë³‘í•© ë (ì„±ê³µ: {success_cnt}, ë¹„ìš©: ${total_cost:.4f}) ===")
        self._load_jobs_all()
        self._safe_msgbox("showinfo", "ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©: ${total_cost:.4f}")

    def _report_selected_unified(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        targets = [bid for bid in ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "merged"]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ìƒíƒœê°€ 'merged'ì¸ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if messagebox.askyesno("ë¦¬í¬íŠ¸", f"ì„ íƒí•œ {len(targets)}ê±´ì˜ ì •ì œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_report_unified, args=(targets,))
            t.daemon = True
            t.start()

    def _run_report_unified(self, ids):
        self.append_log(f"--- ì •ì œ ë¦¬í¬íŠ¸ ìƒì„± ({len(ids)}ê±´) ---")
        jobs = load_batch_jobs()
        all_reps = []
        for bid in ids:
            local_job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not local_job: continue
            out_path = local_job.get("out_excel")
            if not out_path or not os.path.exists(out_path):
                self.append_log(f"âŒ íŒŒì¼ ëˆ„ë½: {bid}")
                continue
            
            try:
                df = pd.read_excel(out_path)
                if "ST1_ê²°ê³¼ìƒí’ˆëª…" not in df.columns or "ì›ë³¸ìƒí’ˆëª…" not in df.columns: continue
                for idx, row in df.iterrows():
                    raw = safe_str(row.get("ì›ë³¸ìƒí’ˆëª…", ""))
                    res = safe_str(row.get("ST1_ê²°ê³¼ìƒí’ˆëª…", ""))
                    
                    is_changed = "O" if raw != res else "X"
                    len_diff = len(res) - len(raw)
                    
                    all_reps.append({
                        "Batch_ID": bid,
                        "í–‰ë²ˆí˜¸": idx+2,
                        "ìƒí’ˆì½”ë“œ": safe_str(row.get("ìƒí’ˆì½”ë“œ", "")),
                        "ì›ë³¸ìƒí’ˆëª…": raw,
                        "ì •ì œìƒí’ˆëª…": res,
                        "ë³€ê²½ì—¬ë¶€": is_changed,
                        "ê¸¸ì´ë³€í™”": f"{len(raw)} -> {len(res)} ({len_diff:+d})"
                    })
            except: pass

        if not all_reps:
            self._safe_msgbox("showinfo", "ì•Œë¦¼", "ë°ì´í„° ì—†ìŒ")
            return

        try:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            path = os.path.join(BASE_DIR, f"Stage1_Clean_Report_{ts}.xlsx")
            pd.DataFrame(all_reps).to_excel(path, index=False)
            self.append_log(f"ğŸ“Š ë¦¬í¬íŠ¸ ì™„ë£Œ: {os.path.basename(path)}")
            
            self.after(0, lambda: self._ask_open_file(path))
            
        except Exception as e: self._safe_msgbox("showerror", "ì˜¤ë¥˜", str(e))

    def _ask_open_file(self, path):
        if messagebox.askyesno("ì™„ë£Œ", "ë¦¬í¬íŠ¸ íŒŒì¼ì„ ì—¬ì‹œê² ìŠµë‹ˆê¹Œ?"):
            try: os.startfile(path)
            except: pass

    def _archive_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if ids and messagebox.askyesno("ë³´ê´€", f"{len(ids)}ê±´ íœ´ì§€í†µ ì´ë™?"):
            archive_batch_job(ids, True)
            self._load_jobs_all()
            self._load_archive_list()

    def _restore_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ë³µêµ¬", f"{len(ids)}ê±´ ë³µêµ¬?"):
            archive_batch_job(ids, False)
            self._load_jobs_all()
            self._load_archive_list()

    def _hard_delete_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ì‚­ì œ", "ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            hard_delete_batch_job(ids)
            self._load_archive_list()

    def _on_tree_double_click(self, event):
        """ë”ë¸”í´ë¦­ ì‹œ: ê·¸ë£¹ í—¤ë”ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°, ë°°ì¹˜ë©´ ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™"""
        # ì–´ë–¤ íŠ¸ë¦¬ì¸ì§€ í™•ì¸
        widget = event.widget
        sel = widget.selection()
        if not sel: return
        
        item = sel[0]
        vals = widget.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš°: ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
        if not batch_id:
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            children = widget.get_children(item)
            if children:
                # ìì‹ì´ ìˆìœ¼ë©´ ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
                if widget.item(item, 'open'):
                    widget.item(item, open=False)
                else:
                    widget.item(item, open=True)
        else:
            # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°: ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™
            self.batch_id_var.set(batch_id)
            self.main_tabs.select(self.tab_merge)

    # ----------------------------------------------------
    # Tab 3: Manual
    # ----------------------------------------------------
    def _init_tab_merge(self):
        container = ttk.Frame(self.tab_merge, padding=20)
        container.pack(fill='both', expand=True)
        f_in = ttk.LabelFrame(container, text="ê°œë³„ ì‘ì—…", padding=15)
        f_in.pack(fill='x')
        ttk.Label(f_in, text="Batch ID:").pack(side='left')
        ttk.Entry(f_in, textvariable=self.batch_id_var, width=45, font=("Consolas", 10)).pack(side='left', padx=10)
        
        f_btn = ttk.Frame(container)
        f_btn.pack(fill='x', pady=20)
        ttk.Button(f_btn, text="1. ê²°ê³¼ ë³‘í•©", command=self._start_merge, style="Primary.TButton").pack(fill='x', pady=5)
        ttk.Button(f_btn, text="2. ë‹¨ì¼ ë¦¬í¬íŠ¸", command=self._start_diff_report).pack(fill='x', pady=5)

    def _start_merge(self):
        bid = self.batch_id_var.get().strip()
        if not bid:
            messagebox.showwarning("ê²½ê³ ", "Batch IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            return
        
        # ìƒíƒœ ì²´í¬ (ì•ˆì „ì¥ì¹˜)
        jobs = load_batch_jobs()
        job = next((j for j in jobs if j["batch_id"] == bid), None)
        if job and job.get("status") != "completed":
            if not messagebox.askyesno("ê²½ê³ ", f"í˜„ì¬ ìƒíƒœê°€ '{job.get('status')}'ì…ë‹ˆë‹¤.\nê·¸ë˜ë„ ë³‘í•©ì„ ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
                return

        t = threading.Thread(target=self._run_merge_multi, args=([bid],))
        t.daemon = True
        t.start()

    def _start_diff_report(self):
        t = threading.Thread(target=self._run_diff_report)
        t.daemon = True
        t.start()

    def _run_diff_report(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_report_unified([bid])

if __name__ == "__main__":
    app = Stage1BatchGUI()
    app.mainloop()