"""
Gui_stage1_batch_Casche.py

Stage 1 Batch API ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (GUI) - ìºì‹± ìµœì í™” ë²„ì „
- ê¸°ëŠ¥: ì—‘ì…€ ì›ë³¸ -> Batch JSONL ìƒì„±(ìƒí’ˆëª… ì •ì œ) -> ì—…ë¡œë“œ -> ì‹¤í–‰ -> ë³‘í•© -> ì •ì œ ë¦¬í¬íŠ¸
- [Fix] ëŸ°ì²˜ ì‹¤í–‰ ì‹œ ëª¨ë“ˆ ê²½ë¡œ(ModuleNotFoundError) ë¬¸ì œ ì™„ë²½ í•´ê²°
- ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™”: OpenAI Prompt Caching ê°€ì´ë“œì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì¬êµ¬ì„±
  * ì •ì  ì½˜í…ì¸ (ì—­í• , ì œì•½, ê·œì¹™)ë¥¼ system í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * ë™ì  ì½˜í…ì¸ (ì…ë ¥ ë°ì´í„°)ë¥¼ user í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * prompt_cache_key ì‚¬ìš©ìœ¼ë¡œ ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ (í† í° ë¹„ìš© ìµœëŒ€ 90% ì ˆê° ê°€ëŠ¥)
"""

import os
import sys
import json
import math
import threading
import subprocess
import re
from datetime import datetime

# ========================================================
# [CRITICAL] ê²½ë¡œ ê°•ì œ ì„¤ì • (Import ì—ëŸ¬ ë°©ì§€)
# ========================================================
# í˜„ì¬ íŒŒì¼(Gui_stage1_batch.py)ì´ ìˆëŠ” í´ë”ë¥¼ êµ¬í•©ë‹ˆë‹¤.
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

# í•´ë‹¹ í´ë”ë¥¼ íŒŒì´ì¬ ê²€ìƒ‰ ê²½ë¡œ(sys.path)ì˜ ë§¨ ì•ì— ì¶”ê°€í•©ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ ê°™ì€ í´ë”ì— ìˆëŠ” 'batch_stage1_core.py'ë¥¼ ë¬´ì¡°ê±´ ì°¾ìŠµë‹ˆë‹¤.
if CURRENT_DIR not in sys.path:
    sys.path.insert(0, CURRENT_DIR)

# í˜¹ì‹œ core íŒŒì¼ì´ ìƒìœ„ í´ë”ì— ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìƒìœ„ í´ë”ë„ ì¶”ê°€í•©ë‹ˆë‹¤.
PARENT_DIR = os.path.dirname(CURRENT_DIR)
if PARENT_DIR not in sys.path:
    sys.path.append(PARENT_DIR)
# ========================================================

import pandas as pd
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, Menu
from tkinter.scrolledtext import ScrolledText

from openai import OpenAI

# ========================================================
# [NEW] ë©”ì¸ ëŸ°ì²˜ ì—°ë™ìš© JobManager & íŒŒì¼ëª… ìœ í‹¸
# ========================================================
def get_root_filename(filename):
    """
    íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´(_T*_I* ë˜ëŠ” _t*_i*, _I*(ì—…ì™„) í¬í•¨) ë° ê¸°íƒ€ ê¼¬ë¦¬í‘œë¥¼ ë–¼ê³  ì›ë³¸ëª…(Key)ë§Œ ì¶”ì¶œ
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0.xlsx -> ë‚˜ì´í‚¤.xlsx
    ì˜ˆ: ì•„ë””ë‹¤ìŠ¤_T2_I1.xlsx -> ì•„ë””ë‹¤ìŠ¤.xlsx
    ì˜ˆ: ë‚˜ì´í‚¤_T1_I0(ì—…ì™„).xlsx -> ë‚˜ì´í‚¤.xlsx
    ì˜ˆ: ë‚˜ì´í‚¤_T1_I0_T2_I1.xlsx -> ë‚˜ì´í‚¤.xlsx (ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ ì œê±°)
    ì˜ˆ: ë‚˜ì´í‚¤_T1_I5(ì—…ì™„).xlsx -> ë‚˜ì´í‚¤.xlsx
    """
    name = os.path.basename(filename)
    base, ext = os.path.splitext(name)

    # 1. ë²„ì „ íŒ¨í„´ (_Tìˆ«ì_Iìˆ«ì(ê´„í˜¸)? ë˜ëŠ” _tìˆ«ì_iìˆ«ì(ê´„í˜¸)?) ë°˜ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    # íŒ¨í„´ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì œê±°, ê´„í˜¸ê°€ ë¶™ì€ ê²½ìš°ë„ í¬í•¨
    while True:
        new_base = re.sub(r"_[Tt]\d+_[Ii]\d+(\([^)]+\))?", "", base, flags=re.IGNORECASE)
        if new_base == base:
            break
        base = new_base
    
    # 2. ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„), (ì™„ë£Œ) ë“±) - ë²„ì „ íŒ¨í„´ì˜ ê´„í˜¸ëŠ” ì´ë¯¸ ì œê±°ë¨
    base = re.sub(r"\([^)]*\)", "", base)
    
    # 3. ê¸°íƒ€ êµ¬í˜• ê¼¬ë¦¬í‘œ ì œê±° (í˜¸í™˜ì„± ìœ ì§€)
    suffixes = ["_stage1_mapping", "_stage1_img_mapping", "_with_images"]
    for s in suffixes:
        base = base.replace(s, "")
    
    # 4. ëì— ë‚¨ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    base = base.rstrip("_")

    return base + ext


def get_next_version_path(current_path, task_type: str = "text"):
    """
    í˜„ì¬ íŒŒì¼ëª…ì„ ë¶„ì„í•´ì„œ ë‹¤ìŒ ë‹¨ê³„ì˜ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    íŒŒì¼ëª… í˜•ì‹: ì›ë³¸ëª…_T{ìˆ«ì}_I{ìˆ«ì}.xlsx ë˜ëŠ” ì›ë³¸ëª…_T{ìˆ«ì}_I{ìˆ«ì}(ê´„í˜¸).xlsx
    - task_type='text'  â†’ T ë²„ì „ +1 (T1 ì˜ë¯¸: í…ìŠ¤íŠ¸ 1ë‹¨ê³„ ì™„ë£Œ)
    - task_type='image' â†’ I ë²„ì „ +1
    
    ì£¼ì˜: íŒŒì¼ëª…ì— ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ íŒ¨í„´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì˜ˆ: ë‚˜ì´í‚¤_T1_I0_T2_I1.xlsx -> ë‚˜ì´í‚¤_T2_I2.xlsx (text) ë˜ëŠ” ë‚˜ì´í‚¤_T2_I2.xlsx (image)
    ì˜ˆ: ë‚˜ì´í‚¤_T1_I5(ì—…ì™„).xlsx -> ë‚˜ì´í‚¤_T2_I5(ì—…ì™„).xlsx (text) ë˜ëŠ” ë‚˜ì´í‚¤_T1_I6(ì—…ì™„).xlsx (image)
    """
    dir_name = os.path.dirname(current_path)
    base_name = os.path.basename(current_path)
    name_only, ext = os.path.splitext(base_name)

    # ë§ˆì§€ë§‰ _T*_I*(ê´„í˜¸)? íŒ¨í„´ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ, ì—¬ëŸ¬ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ ê²ƒë§Œ)
    # ê´„í˜¸ê°€ ë¶™ì€ ê²½ìš°ë„ ì¸ì‹ (ì˜ˆ: _I5(ì—…ì™„))
    all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)(\([^)]+\))?", name_only, re.IGNORECASE))
    
    if all_matches:
        # ë§ˆì§€ë§‰ ë§¤ì¹­ ì‚¬ìš©
        match = all_matches[-1]
        current_t = int(match.group(2))
        current_i = int(match.group(4))
        i_suffix = match.group(5) or ""  # ê´„í˜¸ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ìœ ì§€ (ì˜ˆ: (ì—…ì™„))
        # ì›ë³¸ëª…ì€ ë§ˆì§€ë§‰ íŒ¨í„´ ì´ì „ê¹Œì§€
        original_name = name_only[: match.start()].rstrip("_")
    else:
        # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì›ë³¸ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì œê±° í›„ ì‚¬ìš©
        original_name = name_only
        # ê¸°ì¡´ ë²„ì „ íŒ¨í„´ ì œê±° (ê´„í˜¸ í¬í•¨)
        while True:
            new_name = re.sub(r"_[Tt]\d+_[Ii]\d+(\([^)]+\))?", "", original_name, flags=re.IGNORECASE)
            if new_name == original_name:
                break
            original_name = new_name
        original_name = original_name.rstrip("_")
        current_t = 0
        current_i = 0
        i_suffix = ""

    if task_type == "text":
        new_t = current_t + 1
        new_i = current_i
    elif task_type == "image":
        new_t = current_t
        new_i = current_i + 1
    else:
        return current_path

    # ê´„í˜¸ ë¶€ë¶„ ìœ ì§€ (ì˜ˆ: (ì—…ì™„))
    new_filename = f"{original_name}_T{new_t}_I{new_i}{i_suffix}{ext}"
    return os.path.join(dir_name, new_filename)


class JobManager:
    DB_FILE = None

    @classmethod
    def find_db_path(cls):
        if cls.DB_FILE and os.path.exists(cls.DB_FILE):
            return cls.DB_FILE

        current_dir = os.path.dirname(os.path.abspath(__file__))
        search_dirs = [
            current_dir,
            os.path.abspath(os.path.join(current_dir, "..")),
            os.path.abspath(os.path.join(current_dir, "..", "..")),
        ]

        for d in search_dirs:
            target = os.path.join(d, "job_history.json")
            if os.path.exists(target):
                cls.DB_FILE = target
                print(f"[JobManager] DB Found: {target}")
                return target

        default_path = os.path.abspath(os.path.join(current_dir, "..", "job_history.json"))
        cls.DB_FILE = default_path
        return default_path

    @classmethod
    def load_jobs(cls):
        db_path = cls.find_db_path()
        if not os.path.exists(db_path):
            return {}
        try:
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    @classmethod
    def update_status(cls, filename, text_msg=None, img_msg=None):
        """ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        db_path = cls.find_db_path()
        data = cls.load_jobs()
        now = datetime.now().strftime("%m-%d %H:%M")

        if filename not in data:
            data[filename] = {
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "text_status": "ëŒ€ê¸°",
                "text_time": "-",
                "image_status": "ëŒ€ê¸°",
                "image_time": "-",
                "memo": "",
            }

        if text_msg:
            data[filename]["text_status"] = text_msg
            data[filename]["text_time"] = now
        if img_msg:
            data[filename]["image_status"] = img_msg
            data[filename]["image_time"] = now

        data[filename]["last_update"] = now

        try:
            with open(db_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"[JobManager Error] {e}")


def safe_save_excel(df, path: str) -> bool:
    """ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆì–´ ì €ì¥ì´ ì•ˆ ë  ë•Œ ì¬ì‹œë„ë¥¼ ìœ ë„í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            df.to_excel(path, index=False)
            return True
        except PermissionError:
            if not messagebox.askretrycancel(
                "ì €ì¥ ì‹¤íŒ¨",
                f"ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤!\n[{os.path.basename(path)}]\n\n"
                "íŒŒì¼ì„ ë‹«ê³  'ë‹¤ì‹œ ì‹œë„'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
            ):
                return False
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False


# core ëª¨ë“ˆì˜ ì‘ë‹µ íŒŒì„œ ì¬ì‚¬ìš© (Batch JSONL í˜•ì‹ í†µì¼)
# ìºì‹± ìµœì í™” ë²„ì „ ì‚¬ìš© (batch_stage1_core_Casche.py)
try:
    from batch_stage1_core_Casche import (
        extract_text_from_response_dict,
        extract_usage_from_response_dict,
        STAGE1_SYSTEM_PROMPT,
        STAGE1_USER_PROMPT_TEMPLATE,
        fmt_safe,
    )
    CACHE_MODE_CORE = True
except ImportError:
    # ìºì‹± ë²„ì „ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ë²„ì „ ì‚¬ìš©
    try:
        from batch_stage1_core import (
            extract_text_from_response_dict,
            extract_usage_from_response_dict,
        )
        CACHE_MODE_CORE = False
        STAGE1_SYSTEM_PROMPT = ""
        STAGE1_USER_PROMPT_TEMPLATE = ""
        def fmt_safe(x): return str(x) if x is not None else ""
    except ImportError:
        # êµ¬ë²„ì „/ëˆ„ë½ í™˜ê²½ì—ì„œëŠ” ì¡°ìš©íˆ íŒ¨ìŠ¤í•˜ì§€ë§Œ,
        # ì´ ê²½ìš° Batch ë³‘í•© ì‹œ í† í°/í…ìŠ¤íŠ¸ íŒŒì‹±ì´ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ.
        def extract_text_from_response_dict(resp):  # type: ignore[override]
            body = resp.get("body") if isinstance(resp, dict) and "body" in resp else resp
            # ê°€ì¥ ë‹¨ìˆœí•œ fallback: output_text ë˜ëŠ” ë¬¸ìì—´ ë³€í™˜
            if isinstance(body, dict) and isinstance(body.get("output_text"), str):
                return body["output_text"].strip()
            return ""

        def extract_usage_from_response_dict(resp):  # type: ignore[override]
            return 0, 0, 0
        
        CACHE_MODE_CORE = False
        STAGE1_SYSTEM_PROMPT = ""
        STAGE1_USER_PROMPT_TEMPLATE = ""
        def fmt_safe(x): return str(x) if x is not None else ""

# [í•µì‹¬ ì˜ì¡´ì„±] Stage1 í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ (coreì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
try:
    from prompts_stage1 import build_stage1_prompt, safe_str
except ImportError:
    # prompts_stage1 ì´ ì •ë§ ì—†ëŠ” ê²½ìš° ì•ˆì „í•œ fallback
    def safe_str(x):
        if x is None:
            return ""
        try:
            if isinstance(x, float) and math.isnan(x):
                return ""
        except Exception:
            pass
        return str(x).strip()

    def build_stage1_prompt(category, sale_type, raw_name):
        raise RuntimeError(
            "í•„ìˆ˜ ëª¨ë“ˆ 'prompts_stage1.py'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            "Stage1ì—ì„œ ì‚¬ìš©í•˜ë˜ í”„ë¡¬í”„íŠ¸ ì •ì˜ íŒŒì¼ì´ ê°™ì€ í´ë”ë‚˜ íŒŒì´ì¬ ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        )

# ì—¬ê¸°ì„œ GUI ì „ìš© batch payload ë¹Œë”ë¥¼ êµ¬í˜„ (ìºì‹± ìµœì í™” ë²„ì „)
def build_stage1_batch_payload(idx, row, model, effort):
    """
    í•œ í–‰(row)ì„ Batch APIìš© ìš”ì²­ í•œ ì¤„(JSONL)ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜.
    - coreì˜ create_batch_input_jsonl ê³¼ ë™ì¼í•œ í•„ë“œ(ì¹´í…Œê³ ë¦¬ëª…, íŒë§¤í˜•íƒœ, ì›ë³¸ìƒí’ˆëª…)ë¥¼ ì‚¬ìš©
    - í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™”: system/user ë¶„ë¦¬
    - Batch endpoint: /v1/responses
    """
    # í•„ìˆ˜ í•„ë“œ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
    raw_name = safe_str(row.get("ì›ë³¸ìƒí’ˆëª…", ""))
    category = safe_str(row.get("ì¹´í…Œê³ ë¦¬ëª…", ""))
    sale_type = safe_str(row.get("íŒë§¤í˜•íƒœ", ""))

    # í•„ìˆ˜ê°’ì´ í•˜ë‚˜ë¼ë„ ë¹„ì–´ ìˆìœ¼ë©´ ì´ í–‰ì€ ìŠ¤í‚µ
    if not raw_name or not category or not sale_type:
        return None

    if CACHE_MODE_CORE:
        # ìºì‹± ìµœì í™” ëª¨ë“œ: system/user ë¶„ë¦¬
        system_content = [{"type": "input_text", "text": STAGE1_SYSTEM_PROMPT}]
        user_prompt = STAGE1_USER_PROMPT_TEMPLATE.format(
            category=fmt_safe(category),
            sale_type=fmt_safe(sale_type),
            raw_name=fmt_safe(raw_name)
        )
        user_content = [{"type": "input_text", "text": user_prompt}]

        body = {
            "model": model,
            "input": [
                {
                    "role": "system",
                    "content": system_content,
                },
                {
                    "role": "user",
                    "content": user_content,
                }
            ],
            "reasoning": {"effort": effort or "low"},
        }
    else:
        # ì¼ë°˜ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ ìœ ì§€
        prompt_text = build_stage1_prompt(category, sale_type, raw_name)
        body = {
            "model": model,
            "input": [
                {"role": "user", "content": prompt_text}
            ],
            "reasoning": {"effort": effort or "low"},
        }

    # Batch JSONL í•œ ì¤„ êµ¬ì¡° (Responses ì—”ë“œí¬ì¸íŠ¸)
    return {
        "custom_id": f"row-{idx}",
        "method": "POST",
        "url": "/v1/responses",
        "body": body,
    }


# History ëª¨ë“ˆ (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì¡°ìš©íˆ íŒ¨ìŠ¤)
try:
    from stage1_run_history import append_run_history
except ImportError:
    def append_run_history(*args, **kwargs):
        pass


# ========================================================
# [CORE] ê²½ë¡œ ë° ì„¤ì • ê´€ë¦¬
# ========================================================
def get_base_dir():
    """PyInstaller ë“±ìœ¼ë¡œ íŒ¨í‚¤ì§•ëœ ê²½ìš°ì™€ ì¼ë°˜ ì‹¤í–‰ì„ êµ¬ë¶„í•˜ì—¬ ê¸°ë³¸ ê²½ë¡œ ë°˜í™˜"""
    if getattr(sys, "frozen", False):
        return os.path.dirname(sys.executable)
    return os.path.dirname(os.path.abspath(__file__))

BASE_DIR = get_base_dir()
API_KEY_FILE = os.path.join(BASE_DIR, ".openai_api_key_stage1_batch")
BATCH_JOBS_FILE = os.path.join(BASE_DIR, "stage1_batch_jobs.json")

# [ìˆ˜ì •] GPT-5 ê³„ì—´ ëª¨ë¸ë§Œ ìœ ì§€
MODEL_PRICING_USD_PER_MTOK = {
    "gpt-5":       {"input": 1.25, "output": 10.00},
    "gpt-5-mini":  {"input": 0.25, "output": 2.00},
    "gpt-5-nano":  {"input": 0.05, "output": 0.40},
}

# UI Colors
COLOR_BG = "#F8F9FA"
COLOR_WHITE = "#FFFFFF"
COLOR_PRIMARY = "#4A90E2"
COLOR_PRIMARY_HOVER = "#357ABD"
COLOR_SUCCESS = "#28A745"
COLOR_SUCCESS_HOVER = "#218838"
COLOR_DANGER = "#DC3545"
COLOR_DANGER_HOVER = "#C82333"
COLOR_TEXT = "#333333"
COLOR_HEADER = "#E9ECEF"

def get_seoul_now():
    try:
        from pytz import timezone
        return datetime.now(timezone("Asia/Seoul"))
    except:
        return datetime.now()

def load_api_key_from_file(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f: return f.read().strip()
    return ""

def save_api_key_to_file(key, path):
    with open(path, "w", encoding="utf-8") as f: f.write(key)

# ========================================================
# íˆ´íŒ í´ë˜ìŠ¤
# ========================================================
class ToolTip:
    def __init__(self, widget, text, wraplength=400):
        self.widget = widget
        self.text = text
        self.wraplength = wraplength
        self.tipwindow = None
        self.widget.bind("<Enter>", self.show_tip)
        self.widget.bind("<Leave>", self.hide_tip)

    def show_tip(self, event=None):
        if self.tipwindow or not self.text: return
        x = self.widget.winfo_rootx() + 20
        y = self.widget.winfo_rooty() + 20
        self.tipwindow = tw = tk.Toplevel(self.widget)
        tw.wm_overrideredirect(True)
        tw.wm_geometry(f"+{x}+{y}")
        label = tk.Label(tw, text=self.text, justify='left', background="#ffffe0", relief='solid', borderwidth=1, font=("ë§‘ì€ ê³ ë”•", 9), wraplength=self.wraplength)
        label.pack(ipadx=4, ipady=2)

    def hide_tip(self, event=None):
        if self.tipwindow:
            self.tipwindow.destroy()
            self.tipwindow = None

# ========================================================
# ë°°ì¹˜ ì¡ ê´€ë¦¬ (JSON DB)
# ========================================================
def load_batch_jobs():
    if not os.path.exists(BATCH_JOBS_FILE): return []
    try:
        with open(BATCH_JOBS_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return []

def save_batch_jobs(jobs):
    try:
        with open(BATCH_JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
    except Exception as e: print(f"[Error] ì¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_excel_name_from_path(path: str) -> str:
    """ì „ì²´ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ"""
    if not path:
        return "-"
    return os.path.basename(path)

def upsert_batch_job(batch_id, **kwargs):
    jobs = load_batch_jobs()
    found = False
    now_str = datetime.now().isoformat()
    for j in jobs:
        if j["batch_id"] == batch_id:
            if kwargs.get("status") == "completed" and j.get("status") != "completed":
                if "completed_at" not in kwargs: j["completed_at"] = now_str
            j.update(kwargs)
            j["updated_at"] = now_str
            found = True
            break
    if not found:
        new_job = {
            "batch_id": batch_id, "created_at": now_str, "updated_at": now_str,
            "completed_at": "", "archived": False, **kwargs
        }
        jobs.insert(0, new_job)
    save_batch_jobs(jobs)

def archive_batch_job(batch_ids, archive=True):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    for j in jobs:
        if j["batch_id"] in batch_ids: j["archived"] = archive
    save_batch_jobs(jobs)

def hard_delete_batch_job(batch_ids):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    jobs = [j for j in jobs if j["batch_id"] not in batch_ids]
    save_batch_jobs(jobs)

# ========================================================
# GUI Class
# ========================================================
class Stage1BatchGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Stage 1: Batch API Manager (Path Fixed) ğŸš€ ìºì‹± ìµœì í™” ë²„ì „")
        self.geometry("1250x950")
        
        self.api_key_var = tk.StringVar()
        self.src_file_var = tk.StringVar()
        
        # [ì„¤ì •] ê¸°ë³¸ê°’: gpt-5-mini / low
        self.model_var = tk.StringVar(value="gpt-5-mini") 
        self.effort_var = tk.StringVar(value="low") 
        self.skip_exist_var = tk.BooleanVar(value=True)
        
        # ìë™ ê°±ì‹  ê´€ë ¨
        self.auto_refresh_var = tk.BooleanVar(value=False)
        self.refresh_interval_var = tk.IntVar(value=30)
        self.is_refreshing = False

        self.batch_id_var = tk.StringVar()
        self.failed_chunks_file_var = tk.StringVar()
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ìƒíƒœ ì¶”ì 
        self.collapsed_groups = set()  # ì ‘íŒ ê·¸ë£¹ ID ì§‘í•©
        
        self._configure_styles()
        self._init_ui()
        self._load_key()
        
        # ìë™ ê°±ì‹  ë£¨í”„ ì‹œì‘
        self._auto_refresh_loop()

    def _configure_styles(self):
        style = ttk.Style()
        try: style.theme_use('clam')
        except: pass
        
        self.configure(background=COLOR_BG)
        style.configure("TFrame", background=COLOR_BG)
        style.configure("TLabel", background=COLOR_BG, foreground=COLOR_TEXT, font=("ë§‘ì€ ê³ ë”•", 10))
        style.configure("Header.TLabel", font=("ë§‘ì€ ê³ ë”•", 11, "bold"), foreground="#444")
        style.configure("TLabelframe", background=COLOR_BG, bordercolor="#D0D7DE")
        style.configure("TLabelframe.Label", background=COLOR_BG, foreground="#0056b3", font=("ë§‘ì€ ê³ ë”•", 10, "bold"))
        style.configure("TNotebook", background=COLOR_BG, borderwidth=0)
        style.configure("TNotebook.Tab", background="#E1E4E8", padding=[12, 5], font=("ë§‘ì€ ê³ ë”•", 10))
        style.map("TNotebook.Tab", background=[("selected", COLOR_WHITE)], foreground=[("selected", COLOR_PRIMARY)])
        style.configure("Treeview", background=COLOR_WHITE, fieldbackground=COLOR_WHITE, font=("ë§‘ì€ ê³ ë”•", 9), rowheight=28)
        style.configure("Treeview.Heading", background=COLOR_HEADER, foreground="#333", font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        style.map("Treeview", background=[('selected', '#CCE5FF')], foreground=[('selected', 'black')])
        style.configure("TButton", font=("ë§‘ì€ ê³ ë”•", 9), padding=5, borderwidth=1)
        style.configure("Primary.TButton", background=COLOR_PRIMARY, foreground="white", bordercolor=COLOR_PRIMARY)
        style.map("Primary.TButton", background=[("active", COLOR_PRIMARY_HOVER)])
        style.configure("Success.TButton", background=COLOR_SUCCESS, foreground="white", bordercolor=COLOR_SUCCESS)
        style.map("Success.TButton", background=[("active", COLOR_SUCCESS_HOVER)])
        style.configure("Danger.TButton", background=COLOR_DANGER, foreground="white", bordercolor=COLOR_DANGER)
        style.map("Danger.TButton", background=[("active", COLOR_DANGER_HOVER)])

    def _init_ui(self):
        main_container = ttk.Frame(self, padding=15)
        main_container.pack(fill='both', expand=True)

        # 1. ìƒë‹¨ API Key
        f_top = ttk.LabelFrame(main_container, text="ğŸ”‘ API ì„¤ì •", padding=10)
        f_top.pack(fill='x', pady=(0, 10))
        ttk.Label(f_top, text="Batch API Key:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(side='left')
        entry_key = ttk.Entry(f_top, textvariable=self.api_key_var, show="*", width=50, font=("Consolas", 10))
        entry_key.pack(side='left', padx=10)
        btn_save = ttk.Button(f_top, text="ì €ì¥", command=self._save_key, style="Primary.TButton")
        btn_save.pack(side='left')
        ToolTip(btn_save, "ì…ë ¥í•œ API Keyë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.")

        btn_help = ttk.Button(f_top, text="â“ ì‚¬ìš© ê°€ì´ë“œ", command=self._show_help_dialog)
        btn_help.pack(side='right')

        # 2. ë©”ì¸ íƒ­
        self.main_tabs = ttk.Notebook(main_container)
        self.main_tabs.pack(fill='both', expand=True, pady=5)
        
        self.tab_create = ttk.Frame(self.main_tabs)
        self.tab_manage = ttk.Frame(self.main_tabs) 
        self.tab_merge = ttk.Frame(self.main_tabs)
        
        self.main_tabs.add(self.tab_create, text=" 1. ë°°ì¹˜ ìƒì„± & ì—…ë¡œë“œ ")
        self.main_tabs.add(self.tab_manage, text=" 2. ë°°ì¹˜ ê´€ë¦¬ (ëª©ë¡/ë³‘í•©/ë¦¬í¬íŠ¸) ")
        self.main_tabs.add(self.tab_merge, text=" 3. ê°œë³„ ë³‘í•© (ìˆ˜ë™) ")
        
        self._init_tab_create()
        self._init_tab_manage()
        self._init_tab_merge()
        
        # 3. ë¡œê·¸
        f_log = ttk.LabelFrame(main_container, text="ğŸ“‹ ì‹œìŠ¤í…œ ë¡œê·¸", padding=10)
        f_log.pack(fill='both', expand=True, pady=(10, 0))
        self.log_widget = ScrolledText(f_log, height=12, state='disabled', font=("Consolas", 9), bg="#F1F3F5")
        self.log_widget.pack(fill='both', expand=True)

    def _load_key(self):
        loaded = load_api_key_from_file(API_KEY_FILE)
        if loaded: self.api_key_var.set(loaded)

    def _save_key(self):
        k = self.api_key_var.get().strip()
        if k:
            save_api_key_to_file(k, API_KEY_FILE)
            messagebox.showinfo("ì €ì¥", "API Key ì €ì¥ ì™„ë£Œ")

    # [Thread-Safe Log]
    def append_log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        full_msg = f"[{ts}] {msg}"
        def _update():
            if not hasattr(self, 'log_widget'): return
            try:
                self.log_widget.config(state='normal')
                self.log_widget.insert('end', full_msg + "\n")
                self.log_widget.see('end')
                self.log_widget.config(state='disabled')
            except: pass
        self.after(0, _update)

    # [Thread-Safe Messagebox]
    def _safe_msgbox(self, type_, title, msg):
        self.after(0, lambda: getattr(messagebox, type_)(title, msg))

    def _show_help_dialog(self):
        msg = (
            "[Stage 1 Batch API ì‚¬ìš© ê°€ì´ë“œ]\n\n"
            "1. [ë°°ì¹˜ ìƒì„± íƒ­]:\n"
            "   - ì›ë³¸ ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ê³  'Start Batch'ë¥¼ í´ë¦­í•˜ì„¸ìš”.\n"
            "   - 'gpt-5-mini' ëª¨ë¸ ì‚¬ìš© ì‹œ ë¹„ìš© íš¨ìœ¨ê³¼ ì†ë„ê°€ ì¢‹ìŠµë‹ˆë‹¤.\n\n"
            "2. [ë°°ì¹˜ ê´€ë¦¬ íƒ­]:\n"
            "   - [ìë™ ê°±ì‹ ]ì„ ì¼œë‘ë©´ ì£¼ê¸°ì ìœ¼ë¡œ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n"
            "   - 'completed' ìƒíƒœê°€ ë˜ë©´ [ì„ íƒ ì¼ê´„ ë³‘í•©] -> [ì •ì œ ë¦¬í¬íŠ¸] ìˆœìœ¼ë¡œ ì§„í–‰í•˜ì„¸ìš”.\n"
            "   - ë¦¬í¬íŠ¸ì—ì„œ ì›ë³¸ vs ì •ì œê²°ê³¼ì˜ ê¸€ì ìˆ˜ ë³€í™”ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
            "â€» ê²°ê³¼ëŠ” ì›ë³¸ ì—‘ì…€ì˜ 'ST1_ê²°ê³¼ìƒí’ˆëª…' ì»¬ëŸ¼ì— ë³‘í•©ë©ë‹ˆë‹¤."
        )
        messagebox.showinfo("ì‚¬ìš©ë²•", msg)

    # ----------------------------------------------------
    # Tab 1: Create
    # ----------------------------------------------------
    def _init_tab_create(self):
        container = ttk.Frame(self.tab_create, padding=20)
        container.pack(fill='both', expand=True)
        
        # Step 1: íŒŒì¼
        f_file = ttk.LabelFrame(container, text="1. ì‘ì—… ëŒ€ìƒ íŒŒì¼ ì„ íƒ", padding=15)
        f_file.pack(fill='x', pady=(0, 15))
        ttk.Entry(f_file, textvariable=self.src_file_var, font=("ë§‘ì€ ê³ ë”•", 10)).pack(side='left', fill='x', expand=True)
        btn_file = ttk.Button(f_file, text="ğŸ“‚ íŒŒì¼ ì°¾ê¸°", command=self._select_src_file)
        btn_file.pack(side='right', padx=5)
        ToolTip(btn_file, "Stage 1ì„ ìˆ˜í–‰í•  ì›ë³¸ ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.\n(ì¹´í…Œê³ ë¦¬ëª…, ì›ë³¸ìƒí’ˆëª… ì»¬ëŸ¼ í•„ìˆ˜)")
        
        # Step 2: ì˜µì…˜
        f_opt = ttk.LabelFrame(container, text="2. ë°°ì¹˜ ì˜µì…˜ ì„¤ì •", padding=15)
        f_opt.pack(fill='x', pady=5)
        
        # ëª¨ë¸
        fr1 = ttk.Frame(f_opt)
        fr1.pack(fill='x', pady=5)
        ttk.Label(fr1, text="ëª¨ë¸ (Model):", width=12).pack(side='left')
        models = list(MODEL_PRICING_USD_PER_MTOK.keys())
        cb_model = ttk.Combobox(fr1, textvariable=self.model_var, values=models, state="readonly", width=20)
        cb_model.pack(side='left', padx=5)
        ToolTip(cb_model, "Stage 1ì€ gpt-5-miniê°€ ê°€ì¥ íš¨ìœ¨ì ì…ë‹ˆë‹¤.")
        
        # Effort
        ttk.Label(fr1, text="Reasoning Effort:", width=15).pack(side='left', padx=(20, 5))
        cb_effort = ttk.Combobox(fr1, textvariable=self.effort_var, values=["low", "medium", "high"], state="readonly", width=12)
        cb_effort.pack(side='left', padx=5)
        ToolTip(cb_effort, "í…ìŠ¤íŠ¸ ì •ì œëŠ” 'low'ë§Œìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.")
        
        # ì²´í¬ë°•ìŠ¤
        fr2 = ttk.Frame(f_opt)
        fr2.pack(fill='x', pady=5)
        chk_skip = ttk.Checkbutton(fr2, text=" ì´ë¯¸ ST1_ê²°ê³¼ìƒí’ˆëª…ì´ ìˆëŠ” í–‰ì€ ê±´ë„ˆë›°ê¸° (Skip)", variable=self.skip_exist_var)
        chk_skip.pack(side='left', padx=5)
        ToolTip(chk_skip, "ì¤‘ë³µ ê³¼ê¸ˆ ë°©ì§€ë¥¼ ìœ„í•´ ì´ë¯¸ ê²°ê³¼ê°€ ìˆëŠ” í–‰ì€ ì œì™¸í•©ë‹ˆë‹¤.")

        # Step 3: ì‹¤í–‰
        f_step3 = ttk.LabelFrame(container, text="3. ì‹¤í–‰", padding=15)
        f_step3.pack(fill='x', pady=15)
        btn_run = ttk.Button(f_step3, text="ğŸš€ JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (Start Batch)", command=self._start_create_batch, style="Success.TButton")
        btn_run.pack(fill='x', ipady=8)
        ttk.Label(container, text="â€» ë°°ì¹˜ APIëŠ” ê²°ê³¼ ìˆ˜ì‹ ê¹Œì§€ ìµœëŒ€ 24ì‹œê°„ ì†Œìš” (ë¹„ìš© 50% ì ˆê°)", foreground="#666").pack()

    def _select_src_file(self):
        p = filedialog.askopenfilename(filetypes=[("Excel", "*.xlsx;*.xls")])
        if p: self.src_file_var.set(p)

    def _start_create_batch(self):
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Key í•„ìš”")
            return
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ ì„ íƒ í•„ìš”")
            return
        t = threading.Thread(target=self._run_create_batch)
        t.daemon = True
        t.start()

    def _run_create_batch(self):
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        model = self.model_var.get()
        effort = self.effort_var.get()
        
        try:
            client = OpenAI(api_key=key)
            self.append_log(f"ì—‘ì…€ ë¡œë“œ ì¤‘... {os.path.basename(src)}")
            df = pd.read_excel(src)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            req_cols = ['ì¹´í…Œê³ ë¦¬ëª…', 'ì›ë³¸ìƒí’ˆëª…']
            for c in req_cols:
                if c not in df.columns:
                    raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼({c})ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ìºì‹± ëª¨ë“œ í™•ì¸ ë° ë¡œê·¸
            if CACHE_MODE_CORE:
                self.append_log(f"[INFO] ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ëª¨ë“œ í™œì„±í™” (batch_stage1_core_Casche.py)")
            else:
                self.append_log(f"[INFO] âš ï¸ ì¼ë°˜ ëª¨ë“œ (batch_stage1_core.py) - ìºì‹± ìµœì í™” ë¯¸ì ìš©")
            
            # ë¨¼ì € ì „ì²´ ëŒ€ìƒ ìš”ì²­ ìˆ˜ë¥¼ ê³„ì‚° (ë²„í‚· ìˆ˜ ê²°ì •ìš©)
            target_rows = 0
            for idx, row in df.iterrows():
                if self.skip_exist_var.get() and "ST1_ê²°ê³¼ìƒí’ˆëª…" in df.columns:
                    val = safe_str(row.get("ST1_ê²°ê³¼ìƒí’ˆëª…", ""))
                    if val and val != "nan":
                        continue
                
                raw_name = safe_str(row.get("ì›ë³¸ìƒí’ˆëª…", ""))
                category = safe_str(row.get("ì¹´í…Œê³ ë¦¬ëª…", ""))
                sale_type = safe_str(row.get("íŒë§¤í˜•íƒœ", ""))
                
                if not raw_name or not category or not sale_type:
                    continue
                
                target_rows += 1

            # ë²„í‚· ìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚° (ëª¨ë“  ìš”ì²­ì— ë™ì¼í•˜ê²Œ ì ìš©)
            if CACHE_MODE_CORE and target_rows > 0:
                # [í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ì „ëµ - í‚¤ ê³ ì •]
                # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼: ë²„í‚· ë¶„ì‚° ì‹œ ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ì•„ì§ (10% ìˆ˜ì¤€)
                # í•´ê²°ì±…: prompt_cache_keyë¥¼ í•˜ë‚˜ë¡œ ê³ ì •í•˜ì—¬ ëª¨ë“  ìš”ì²­ì´ ê°™ì€ ìºì‹œ í’€ ê³µìœ 
                # Batch APIëŠ” 24ì‹œê°„ì— ê±¸ì³ ì²˜ë¦¬ë˜ë¯€ë¡œ overflow ìš°ë ¤ëŠ” ë‚®ìŒ
                PROMPT_CACHE_BUCKETS = 1
                
                self.append_log(f"[INFO] í”„ë¡¬í”„íŠ¸ ìºì‹±: í‚¤ ê³ ì • ì „ëµ ì‚¬ìš© (ëª¨ë“  ìš”ì²­ì´ 'stage1_v1' í‚¤ ê³µìœ )")
                self.append_log(f"[INFO] ì˜ˆìƒ ìš”ì²­ ìˆ˜: {target_rows}ê°œ, ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ ì˜ˆìƒ")
            else:
                PROMPT_CACHE_BUCKETS = 1
            
            jsonl_lines = []
            skipped_cnt = 0
            seen_custom_ids = set()
            duplicate_count = 0
            
            for idx, row in df.iterrows():
                if self.skip_exist_var.get() and "ST1_ê²°ê³¼ìƒí’ˆëª…" in df.columns:
                    val = safe_str(row.get("ST1_ê²°ê³¼ìƒí’ˆëª…", ""))
                    if val and val != "nan":
                        skipped_cnt += 1
                        continue
                
                # Core í•¨ìˆ˜ í˜¸ì¶œ
                payload = build_stage1_batch_payload(idx, row, model, effort)
                if not payload:
                    skipped_cnt += 1
                    continue
                
                # Prompt Caching ìµœì í™” (ìºì‹± ëª¨ë“œì¼ ë•Œë§Œ)
                if CACHE_MODE_CORE and "body" in payload:
                    custom_id = payload.get("custom_id", f"row-{idx}")
                    
                    # ì¤‘ë³µ custom_id ì²´í¬
                    if custom_id in seen_custom_ids:
                        duplicate_count += 1
                        continue
                    seen_custom_ids.add(custom_id)
                    
                    # prompt_cache_key: í‚¤ ê³ ì • ì „ëµ (ëª¨ë“  ìš”ì²­ì´ ë™ì¼í•œ í‚¤ ì‚¬ìš©)
                    payload["body"]["prompt_cache_key"] = "stage1_v1"
                    
                    # prompt_cache_retentionì€ ëª¨ë¸ì´ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°
                    # (prompt_cache_keyë§Œìœ¼ë¡œë„ í”„ë¡¬í”„íŠ¸ ìºì‹±ì´ ì‘ë™í•  ìˆ˜ ìˆìŒ)
                
                jsonl_lines.append(json.dumps(payload, ensure_ascii=False))
            
            if duplicate_count > 0:
                self.append_log(f"[WARN] âš ï¸ ì¤‘ë³µ ìš”ì²­ {duplicate_count}ê°œê°€ ê°ì§€ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if not jsonl_lines:
                self.append_log("ìƒì„±í•  ìš”ì²­ ì—†ìŒ.")
                return

            # [Fix] BASE_DIR ì‚¬ìš© (Exe í™˜ê²½ ëŒ€ì‘)
            base_name, _ = os.path.splitext(os.path.basename(src))
            # ê°™ì€ í´ë”ì— JSONL ìƒì„±
            jsonl_path = os.path.join(os.path.dirname(src), f"{base_name}_stage1_batch_input.jsonl")
            
            with open(jsonl_path, "w", encoding="utf-8") as f:
                f.write("\n".join(jsonl_lines))
            
            self.append_log(f"JSONL ìƒì„± ì™„ë£Œ: {len(jsonl_lines)}ê±´ (ìŠ¤í‚µ {skipped_cnt}ê±´)")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸ ë° ë¶„í•  ì²˜ë¦¬
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ìš”ì²­ ìˆ˜: {len(jsonl_lines)}ê°œ")
            
            # 190MB ì´ìƒì´ë©´ ë¶„í•  ì²˜ë¦¬ (OpenAI Batch API ì œí•œ: 200MB)
            MAX_FILE_SIZE_MB = 190
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸° ({jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB)ë¡œ ì¸í•´ ë¶„í•  ì²˜ë¦¬í•©ë‹ˆë‹¤... (OpenAI ì œí•œ: 200MB)")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client_with_timeout = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch_ids = self._create_batch_chunks(
                    client=client_with_timeout,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model,
                    effort=effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                self._safe_msgbox("showinfo", "ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}")
            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                self.append_log("OpenAI ì—…ë¡œë“œ ì¤‘...")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                
                with open(jsonl_path, "rb") as f:
                    batch_input_file = client.files.create(file=f, purpose="batch")
                
                file_id = batch_input_file.id
                self.append_log(f"ì—…ë¡œë“œ ì™„ë£Œ ID: {file_id}")
                
                # Responses ì—”ë“œí¬ì¸íŠ¸ë¡œ Batch ìƒì„± (coreì™€ ë™ì¼)
                batch_job = client.batches.create(
                    input_file_id=file_id,
                    endpoint="/v1/responses",
                    completion_window="24h",
                )
                
                batch_id = batch_job.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}")
                
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src,
                    jsonl_path=jsonl_path,
                    model=model,
                    effort=effort,
                    status=batch_job.status,
                    output_file_id=None,
                )
                
                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— T1 ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡
                try:
                    root_name = get_root_filename(src)
                    JobManager.update_status(root_name, text_msg="T1 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T1 (ì§„í–‰ì¤‘)")
                except Exception:
                    # ëŸ°ì²˜ê°€ ì—†ê±°ë‚˜ job_history.json ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ìš©íˆ ë¬´ì‹œ
                    pass
                
                self._safe_msgbox("showinfo", "ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {batch_id}")
            
            self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            self._safe_msgbox("showerror", "ì—ëŸ¬", str(e))
    
    def _create_batch_chunks(self, client, jsonl_path, excel_path, model_name, effort, max_size_mb=190):
        """
        í° JSONL íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ batch_group_idë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        """
        import uuid
        
        # ë°°ì¹˜ ê·¸ë£¹ ID ìƒì„± (ê°™ì€ ì—‘ì…€ì—ì„œ ë¶„í• ëœ ë°°ì¹˜ë“¤ì„ ë¬¶ìŒ)
        batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
        
        # JSONL íŒŒì¼ ì½ê¸°
        requests = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    requests.append(json.loads(line))
        
        total_requests = len(requests)
        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (ìš©ëŸ‰ ê¸°ì¤€ìœ¼ë¡œë§Œ ê³„ì‚°)
        original_file_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
        estimated_total_chunks = max(1, int(original_file_size_mb / max_size_mb) + 1)
        
        self.append_log(f"[INFO] ì´ {total_requests}ê°œ ìš”ì²­ì„ ë¶„í• í•©ë‹ˆë‹¤... (ì˜ˆìƒ: ì•½ {estimated_total_chunks}ê°œ ì²­í¬, ê·¸ë£¹ ID: {batch_group_id})")
        
        # base ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜ (ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹œ ì‚¬ìš©)
        base, ext = os.path.splitext(jsonl_path)
        
        batch_ids = []
        chunk_num = 0
        chunk_files_created = []  # ìƒì„±ëœ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì •ë¦¬ìš©)
        failed_chunk_files = []  # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì¬ì‹œë„ìš©)
        
        i = 0
        while i < total_requests:
            chunk_num += 1
            chunk_requests = []
            chunk_size_bytes = 0
            
            # ì²­í¬ ìƒì„± (ìš©ëŸ‰ ê¸°ì¤€ìœ¼ë¡œë§Œ ë¶„í• , 500ê°œ ì œí•œ ì—†ìŒ)
            while i < total_requests:
                req_json = json.dumps(requests[i], ensure_ascii=False)
                req_size_bytes = len(req_json.encode('utf-8')) + 1  # +1 for newline
                
                # ë‹¤ìŒ ìš”ì²­ì„ ì¶”ê°€í•˜ë©´ í¬ê¸° ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ ë§ˆì§„ 5% í¬í•¨)
                if chunk_size_bytes + req_size_bytes > max_size_mb * 1024 * 1024 * 0.95:
                    break
                
                chunk_requests.append(requests[i])
                chunk_size_bytes += req_size_bytes
                i += 1
            
            if not chunk_requests:
                # ì²« ë²ˆì§¸ ìš”ì²­ í•˜ë‚˜ë„ í¬ê¸° ì œí•œì„ ì´ˆê³¼í•˜ë©´ ê²½ê³ í•˜ê³  ê±´ë„ˆëœ€
                if i < total_requests:
                    self.append_log(f"âš ï¸ ì²­í¬ {chunk_num}: ì²« ë²ˆì§¸ ìš”ì²­ì´ í¬ê¸° ì œí•œì„ ì´ˆê³¼í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    i += 1  # ë‹¤ìŒ ìš”ì²­ìœ¼ë¡œ ì´ë™
                break
            
            # ì²­í¬ JSONL íŒŒì¼ ìƒì„±
            chunk_jsonl_path = f"{base}_chunk{chunk_num:03d}{ext}"
            chunk_files_created.append(chunk_jsonl_path)
            
            with open(chunk_jsonl_path, "w", encoding="utf-8") as f:
                for req in chunk_requests:
                    f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            chunk_size_mb = os.path.getsize(chunk_jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] ì²­í¬ {chunk_num}: {len(chunk_requests)}ê°œ ìš”ì²­, {chunk_size_mb:.2f} MB")
            
            # ë°°ì¹˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 3
            retry_count = 0
            batch_created = False
            
            while retry_count < max_retries and not batch_created:
                try:
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹œë„ ì¤‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                    
                    # íŒŒì¼ ì—…ë¡œë“œ
                    with open(chunk_jsonl_path, "rb") as f:
                        batch_input_file = client.files.create(file=f, purpose="batch")
                    file_id = batch_input_file.id
                    
                    # ë°°ì¹˜ ìƒì„±
                    batch_job = client.batches.create(
                        input_file_id=file_id,
                        endpoint="/v1/responses",
                        completion_window="24h",
                    )
                    
                    batch_id = batch_job.id
                    batch_ids.append(batch_id)
                    self.append_log(f"âœ… ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_id}")
                    
                    batch_created = True
                    
                    # ì‘ì—… ì´ë ¥ ê¸°ë¡ (ê·¸ë£¹ ì •ë³´ í¬í•¨)
                    upsert_batch_job(
                        batch_id=batch_id,
                        src_excel=excel_path,
                        jsonl_path=chunk_jsonl_path,
                        model=model_name,
                        effort=effort,
                        status=batch_job.status,
                        output_file_id=None,
                        batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                        chunk_index=chunk_num,  # ì²­í¬ ë²ˆí˜¸
                        total_chunks=chunk_num,  # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì²­í¬ ìˆ˜ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨)
                    )
                except Exception as e:
                    error_str = str(e).lower()
                    is_token_limit_error = "enqueued token limit" in error_str or "token limit reached" in error_str
                    
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                        self.append_log(f"âš ï¸ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                        if is_token_limit_error:
                            self.append_log(f"[WARN] í† í° ì œí•œ ì˜¤ë¥˜ ê°ì§€. ë” ê¸´ ëŒ€ê¸° ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            wait_time = max(wait_time, 30)  # í† í° ì œí•œ ì˜¤ë¥˜ëŠ” ìµœì†Œ 30ì´ˆ ëŒ€ê¸°
                        self.append_log(f"[INFO] {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        import time
                        time.sleep(wait_time)
                    else:
                        self.append_log(f"âŒ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}")
                        if is_token_limit_error:
                            self.append_log(f"[INFO] í† í° ì œí•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì¼ë¶€ ë°°ì¹˜ê°€ ì™„ë£Œëœ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                            self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼: {chunk_jsonl_path}")
                            self.append_log(f"[INFO] ë‚˜ì¤‘ì— '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì´ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        import traceback
                        self.append_log(traceback.format_exc())
                        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìœ ì§€ (ìˆ˜ë™ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                        failed_chunk_files.append({
                            "chunk_num": chunk_num,
                            "chunk_file": chunk_jsonl_path,
                            "error": str(e),
                            "is_token_limit": is_token_limit_error,
                            "excel_path": excel_path,
                            "model_name": model_name,
                            "effort": effort,
                            "batch_group_id": batch_group_id,
                        })
                        # í•˜ì§€ë§Œ ë°°ì¹˜ IDëŠ” ì¶”ê°€ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ batch_idsì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ
        
        # ëª¨ë“  ì²­í¬ ìƒì„± ì™„ë£Œ í›„, total_chunksë¥¼ ì‹¤ì œ ìƒì„±ëœ ë°°ì¹˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        actual_total_chunks = len(batch_ids)
        if actual_total_chunks > 0:
            self.append_log(f"[INFO] ì´ {actual_total_chunks}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ. ì‘ì—… ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
            jobs = load_batch_jobs()
            updated_count = 0
            for j in jobs:
                if j.get("batch_group_id") == batch_group_id:
                    j["total_chunks"] = actual_total_chunks
                    updated_count += 1
            if updated_count > 0:
                save_batch_jobs(jobs)
                self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {actual_total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.append_log(f"âš ï¸ ìƒì„±ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì„±ê³µí•œ ë°°ì¹˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²­í¬ íŒŒì¼ ì •ë¦¬ ì˜µì…˜ ì œê³µ (í˜„ì¬ëŠ” ìœ ì§€)
        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìˆ˜ë™ ì¬ì‹œë„ë¥¼ ìœ„í•´ ìœ ì§€
        if actual_total_chunks < chunk_num:
            failed_chunks = chunk_num - actual_total_chunks
            self.append_log(f"âš ï¸ {failed_chunks}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²­í¬ íŒŒì¼ì€ ìœ ì§€ë©ë‹ˆë‹¤.")
            
            # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ì„ ëª…í™•íˆ í‘œì‹œ
            if failed_chunk_files:
                self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡:")
                for failed_info in failed_chunk_files:
                    self.append_log(f"  - ì²­í¬ {failed_info['chunk_num']}: {os.path.basename(failed_info['chunk_file'])}")
                    if failed_info['is_token_limit']:
                        self.append_log(f"    â†’ í† í° ì œí•œ ì˜¤ë¥˜. ì¼ë¶€ ë°°ì¹˜ ì™„ë£Œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                
                # ì‹¤íŒ¨ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                failed_info_path = f"{base}_failed_chunks.json"
                try:
                    with open(failed_info_path, "w", encoding="utf-8") as f:
                        json.dump(failed_chunk_files, f, ensure_ascii=False, indent=2)
                    self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ ì €ì¥: {os.path.basename(failed_info_path)}")
                    self.append_log(f"[INFO] ë‚˜ì¤‘ì— ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤íŒ¨í•œ ì²­í¬ë§Œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    # GUIì— ìë™ìœ¼ë¡œ ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ì•Œë¦¼
                    self.after(0, lambda: self._handle_failed_chunks(failed_info_path, failed_chunk_files))
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
        try:
            root_name = get_root_filename(excel_path)
            JobManager.update_status(root_name, text_msg="T1 (ì§„í–‰ì¤‘)")
            self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T1 (ì§„í–‰ì¤‘)")
        except Exception:
            pass
        
        return batch_ids

    # ----------------------------------------------------
    # Tab 2: Manage
    # ----------------------------------------------------
    def _init_tab_manage(self):
        container = ttk.Frame(self.tab_manage, padding=10)
        container.pack(fill='both', expand=True)
        sub_tabs = ttk.Notebook(container)
        sub_tabs.pack(fill='both', expand=True)
        
        self.sub_active = ttk.Frame(sub_tabs, padding=10)
        self.sub_archive = ttk.Frame(sub_tabs, padding=10)
        sub_tabs.add(self.sub_active, text=" â–¶ ì§„í–‰ì¤‘ / ì™„ë£Œ (Active) ")
        sub_tabs.add(self.sub_archive, text=" ğŸ—‘ íœ´ì§€í†µ (Archive) ")
        
        # Active UI
        f_ctrl = ttk.Frame(self.sub_active)
        f_ctrl.pack(fill='x', pady=(0, 10))
        
        # ìë™ ê°±ì‹  ì˜µì…˜
        f_auto = ttk.Frame(f_ctrl)
        f_auto.pack(side='left', padx=5)
        ttk.Checkbutton(f_auto, text="ìë™ ìƒíƒœ ê°±ì‹ ", variable=self.auto_refresh_var).pack(side='left')
        ttk.Spinbox(f_auto, from_=10, to=600, textvariable=self.refresh_interval_var, width=4).pack(side='left', padx=2)
        ttk.Label(f_auto, text="ì´ˆ").pack(side='left')
        
        ttk.Button(f_ctrl, text="ğŸ”„ ìˆ˜ë™ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active)).pack(side='left', padx=10)
        ttk.Button(f_ctrl, text="ğŸ“¥ ì¼ê´„ ë³‘í•©", command=self._merge_selected, style="Primary.TButton").pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ“Š ì •ì œ ë¦¬í¬íŠ¸", command=self._report_selected_unified, style="Success.TButton").pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ—‘ íœ´ì§€í†µ ì´ë™", command=self._archive_selected, style="Danger.TButton").pack(side='right', padx=2)
        
        # ì»¬ëŸ¼ ì •ì˜: batch_id | excel_name | group | memo | status | created | completed | model | effort | counts
        cols = ("batch_id", "excel_name", "group", "memo", "status", "created", "completed", "model", "effort", "counts")
        self.tree_active = ttk.Treeview(self.sub_active, columns=cols, show='headings', height=15, selectmode='extended')
        self.tree_active.tag_configure('odd', background=COLOR_WHITE)
        self.tree_active.tag_configure('even', background='#F2F7FF')
        self.tree_active.tag_configure('group_header', background='#E8F4FD', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_active.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_active.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_active.heading("group", text="ê·¸ë£¹")
        self.tree_active.heading("memo", text="ë©”ëª¨")
        self.tree_active.heading("status", text="ìƒíƒœ")
        self.tree_active.heading("created", text="ìƒì„±ì¼ì‹œ")
        self.tree_active.heading("completed", text="ì™„ë£Œì¼ì‹œ")
        self.tree_active.heading("model", text="ëª¨ë¸")
        self.tree_active.heading("effort", text="Effort")
        self.tree_active.heading("counts", text="ì™„ë£Œ/ì „ì²´")
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree_active.column("batch_id", width=180, anchor="w")
        self.tree_active.column("excel_name", width=200, anchor="w")
        self.tree_active.column("group", width=120, anchor="w")
        self.tree_active.column("memo", width=150, anchor="w")
        self.tree_active.column("status", width=80, anchor="center")
        self.tree_active.column("created", width=120, anchor="center")
        self.tree_active.column("completed", width=120, anchor="center")
        self.tree_active.column("model", width=80, anchor="center")
        self.tree_active.column("effort", width=60, anchor="center")
        self.tree_active.column("counts", width=80, anchor="center")
        
        self.tree_active.pack(fill='both', expand=True, padx=5, pady=5)
        
        self.menu_active = Menu(self, tearoff=0)
        self.menu_active.add_command(label="ìƒíƒœ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active))
        self.menu_active.add_command(label="ê²°ê³¼ ë³‘í•©", command=self._merge_selected)
        self.menu_active.add_command(label="ì •ì œ ë¦¬í¬íŠ¸ ìƒì„±", command=self._report_selected_unified)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="íœ´ì§€í†µìœ¼ë¡œ ì´ë™", command=self._archive_selected)
        self.tree_active.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_active, self.menu_active))
        self.tree_active.bind("<Double-1>", self._on_tree_double_click)

        # Archive UI
        f_arch_ctrl = ttk.Frame(self.sub_archive)
        f_arch_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_arch_ctrl, text="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected, style="Primary.TButton").pack(side='left')
        ttk.Button(f_arch_ctrl, text="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected, style="Danger.TButton").pack(side='right')
        self.tree_arch = ttk.Treeview(self.sub_archive, columns=cols, show='headings', height=15, selectmode='extended')
        self.tree_arch.tag_configure('odd', background=COLOR_WHITE)
        self.tree_arch.tag_configure('even', background='#FFF2F2')
        
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_arch.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_arch.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_arch.heading("memo", text="ë©”ëª¨")
        self.tree_arch.heading("status", text="ìƒíƒœ")
        self.tree_arch.heading("created", text="ìƒì„±ì¼ì‹œ")
        self.tree_arch.heading("completed", text="ì™„ë£Œì¼ì‹œ")
        self.tree_arch.heading("model", text="ëª¨ë¸")
        self.tree_arch.heading("effort", text="Effort")
        self.tree_arch.heading("counts", text="ì™„ë£Œ/ì „ì²´")
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree_arch.column("batch_id", width=180, anchor="w")
        self.tree_arch.column("excel_name", width=200, anchor="w")
        self.tree_arch.column("memo", width=150, anchor="w")
        self.tree_arch.column("status", width=80, anchor="center")
        self.tree_arch.column("created", width=120, anchor="center")
        self.tree_arch.column("completed", width=120, anchor="center")
        self.tree_arch.column("model", width=80, anchor="center")
        self.tree_arch.column("effort", width=60, anchor="center")
        self.tree_arch.column("counts", width=80, anchor="center")
        
        self.tree_arch.pack(fill='both', expand=True)
        
        # Archive ìš°í´ë¦­ ë©”ë‰´
        self.menu_arch = Menu(self, tearoff=0)
        self.menu_arch.add_command(label="ë³µêµ¬", command=self._restore_selected)
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_arch))
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected)
        self.tree_arch.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_arch, self.menu_arch))
        
        self._load_jobs_all()
        self._load_archive_list()

    def _auto_refresh_loop(self):
        """ìë™ ìƒíƒœ ê°±ì‹  ë£¨í”„"""
        if self.auto_refresh_var.get() and not self.is_refreshing:
            # merged, failed ë“± ì´ë¯¸ ëë‚œ ìƒíƒœëŠ” ì¡°íšŒ ëŒ€ìƒì—ì„œ ì œì™¸
            # expiredëŠ” ì œì™¸í•˜ì§€ ì•ŠìŒ (output_file_id í™•ì¸ì„ ìœ„í•´ ê°±ì‹  í•„ìš”)
            jobs = load_batch_jobs()
            active_ids = [
                j['batch_id'] for j in jobs 
                if not j.get('archived') and j.get('status') not in ['completed', 'failed', 'cancelled', 'merged']
            ]
            if active_ids:
                t = threading.Thread(target=self._run_refresh_ids, args=(active_ids, True))
                t.daemon = True
                t.start()
        
        interval = max(10, self.refresh_interval_var.get()) * 1000
        self.after(interval, self._auto_refresh_loop)

    def _show_context_menu(self, event, tree, menu):
        item = tree.identify_row(event.y)
        if item:
            if item not in tree.selection(): tree.selection_set(item)
            menu.post(event.x_root, event.y_root)

    def _get_selected_ids(self, tree):
        selection = tree.selection()
        ids = []
        for item in selection:
            vals = tree.item(item)['values']
            if not vals:
                continue
            
            # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
            if not vals[0] or vals[0] == "":
                # ê·¸ë£¹ í—¤ë”ì˜ ìì‹ë“¤(ë°°ì¹˜ë“¤) ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
                children = tree.get_children(item)
                for child in children:
                    child_vals = tree.item(child)['values']
                    if child_vals and child_vals[0]:
                        ids.append(child_vals[0])
            else:
                # ì¼ë°˜ ë°°ì¹˜ì¸ ê²½ìš°
                ids.append(vals[0])
        return ids
    
    def _edit_memo(self, tree):
        """ì„ íƒëœ ë°°ì¹˜ì˜ ë©”ëª¨ë¥¼ í¸ì§‘í•©ë‹ˆë‹¤."""
        selection = tree.selection()
        if not selection:
            messagebox.showinfo("ì•ˆë‚´", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        item = selection[0]
        vals = tree.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        if not batch_id:
            messagebox.showinfo("ì•ˆë‚´", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # í˜„ì¬ ë©”ëª¨ ê°€ì ¸ì˜¤ê¸°
        jobs = load_batch_jobs()
        current_memo = ""
        for j in jobs:
            if j["batch_id"] == batch_id:
                current_memo = j.get("memo", "") or ""
                break
        
        # ë©”ëª¨ í¸ì§‘ ë‹¤ì´ì–¼ë¡œê·¸
        dialog = tk.Toplevel(self)
        dialog.title("ë©”ëª¨ í¸ì§‘")
        dialog.geometry("500x200")
        dialog.transient(self)
        dialog.grab_set()
        
        # ë°°ì¹˜ ID í‘œì‹œ
        tk.Label(dialog, text=f"ë°°ì¹˜ ID: {batch_id[:30]}...", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(10, 5))
        
        # ë©”ëª¨ ì…ë ¥ í•„ë“œ
        tk.Label(dialog, text="ë©”ëª¨:", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(5, 0))
        memo_entry = tk.Text(dialog, height=5, width=60, font=("ë§‘ì€ ê³ ë”•", 9))
        memo_entry.pack(fill="both", expand=True, padx=10, pady=5)
        memo_entry.insert("1.0", current_memo)
        memo_entry.focus()
        
        # ë²„íŠ¼
        btn_frame = tk.Frame(dialog)
        btn_frame.pack(fill="x", padx=10, pady=10)
        
        def save_memo():
            new_memo = memo_entry.get("1.0", "end-1c").strip()
            upsert_batch_job(batch_id, memo=new_memo)
            self.append_log(f"[INFO] ë°°ì¹˜ {batch_id[:20]}... ë©”ëª¨ ì—…ë°ì´íŠ¸: {new_memo[:50]}...")
            self._load_jobs_all()
            self._load_archive_list()
            dialog.destroy()
            messagebox.showinfo("ì™„ë£Œ", "ë©”ëª¨ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        tk.Button(btn_frame, text="ì €ì¥", command=save_memo, bg="#4CAF50", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)
        tk.Button(btn_frame, text="ì·¨ì†Œ", command=dialog.destroy, bg="#f44336", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)

    def _load_jobs_all(self):
        if not hasattr(self, 'tree_active'): return
        for i in self.tree_active.get_children(): self.tree_active.delete(i)
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ë¶„ë¥˜
        groups = {}  # {group_id: [jobs]}
        ungrouped = []  # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤
        
        for j in jobs:
            if j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in groups:
                    groups[group_id] = []
                groups[group_id].append(j)
            else:
                ungrouped.append(j)
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬ (ê·¸ë£¹ ID ê¸°ì¤€)
        sorted_groups = sorted(groups.items())
        
        idx = 0
        # ê·¸ë£¹ë³„ë¡œ í‘œì‹œ
        for group_id, group_jobs in sorted_groups:
            # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ì„ chunk_index ìˆœìœ¼ë¡œ ì •ë ¬
            group_jobs.sort(key=lambda x: (x.get("chunk_index") or 999999, x.get("created_at") or ""))
            
            # ê·¸ë£¹ í—¤ë” ì¶”ê°€
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            is_collapsed = group_id in self.collapsed_groups
            collapse_icon = "â–¶" if is_collapsed else "â–¼"
            group_header = f"{collapse_icon} ê·¸ë£¹: {group_id[:12]}... ({len(group_jobs)}/{total_chunks}ê°œ ë°°ì¹˜)"
            header_item = self.tree_active.insert("", "end",
                text=group_header,
                values=("", "", group_id[:20], "", "", "", "", "", "", ""),
                tags=("group_header",))
            
            # ì ‘í˜€ìˆì§€ ì•Šìœ¼ë©´ ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ í‘œì‹œ
            if not is_collapsed:
                # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ í‘œì‹œ
                for j in group_jobs:
                    cnt = "-"
                    if "request_counts" in j and j["request_counts"]:
                        rc = j["request_counts"]
                        cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                    c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                    f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                    excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                    memo = j.get("memo", "") or "-"
                    chunk_idx = j.get("chunk_index", "")
                    group_display = f"ì²­í¬ {chunk_idx}" if chunk_idx else group_id[:12] if group_id else "-"
                    tag = 'even' if idx % 2 == 0 else 'odd'
                    self.tree_active.insert(header_item, "end",
                        text=j["batch_id"][:30],
                        values=(
                            j["batch_id"], excel_name, group_display, memo, j.get("status"),
                            c_at, f_at, j.get("model"), j.get("effort", "-"), cnt
                        ),
                        tags=(tag,))
                    idx += 1
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤ í‘œì‹œ
        for j in ungrouped:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_active.insert("", "end",
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, "-", memo, j.get("status"),
                    c_at, f_at, j.get("model"), j.get("effort", "-"), cnt
                ),
                tags=(tag,))
            idx += 1

    def _load_archive_list(self):
        if not hasattr(self, 'tree_arch'): return
        for i in self.tree_arch.get_children(): self.tree_arch.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        for j in jobs:
            if not j.get("archived", False): continue
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_arch.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), 
                    c_at, f_at, j.get("model"), j.get("effort", "-"), cnt
                ), 
                tags=(tag,))
            idx += 1

    # --- Actions ---
    def _refresh_selected(self, tree):
        ids = self._get_selected_ids(tree)
        if not ids: return
        jobs = load_batch_jobs()
        completed = [bid for bid in ids if next((x for x in jobs if x["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        if completed:
            if messagebox.askyesno("í™•ì¸", f"{len(completed)}ê±´ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\në¯¸ì™„ë£Œ ê±´ë§Œ ê°±ì‹ í• ê¹Œìš”?"):
                ids = [i for i in ids if i not in completed]
        if not ids: return
        t = threading.Thread(target=self._run_refresh_ids, args=(ids,))
        t.daemon = True
        t.start()

    def _run_refresh_ids(self, ids, silent=False):
        if self.is_refreshing: return
        self.is_refreshing = True
        
        key = self.api_key_var.get().strip()
        if not key:
            self.is_refreshing = False
            return
            
        if not silent: self.append_log(f"ì„ íƒëœ {len(ids)}ê±´ ê°±ì‹  ì¤‘...")
        
        try:
            client = OpenAI(api_key=key)
            for bid in ids:
                try:
                    remote = client.batches.retrieve(bid)
                    rc = None
                    if remote.request_counts:
                        rc = {"total": remote.request_counts.total, "completed": remote.request_counts.completed, "failed": remote.request_counts.failed}
                    
                    # expired ìƒíƒœë„ ê°±ì‹  ê°€ëŠ¥ (output_file_id í™•ì¸ì„ ìœ„í•´)
                    # output_file_id ì¶”ì¶œ: ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
                    output_file_id = getattr(remote, "output_file_id", None)
                    if not output_file_id:
                        # output_file ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                        output_file = getattr(remote, "output_file", None)
                        if output_file:
                            if isinstance(output_file, str):
                                output_file_id = output_file
                            else:
                                output_file_id = getattr(output_file, "id", None) or getattr(output_file, "file_id", None)
                    
                    # model_dump()ë¥¼ í†µí•œ ì¶”ê°€ í™•ì¸ (ê°±ì‹  ì‹œì—ë„ ì ìš©)
                    if not output_file_id and remote.status == "completed":
                        try:
                            if hasattr(remote, "model_dump"):
                                dump = remote.model_dump()
                                if "output_file_id" in dump and dump["output_file_id"]:
                                    output_file_id = dump["output_file_id"]
                                elif "output_file" in dump:
                                    of = dump["output_file"]
                                    if isinstance(of, str) and of:
                                        output_file_id = of
                                    elif isinstance(of, dict) and "id" in of:
                                        output_file_id = of["id"]
                        except Exception:
                            pass
                    
                    upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id, request_counts=rc)
                    
                    if not silent:
                        if remote.status == "expired" and output_file_id:
                            self.append_log(f"â„¹ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ì§€ë§Œ output_file_idê°€ ìˆìŠµë‹ˆë‹¤. (ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")
                        elif remote.status == "completed":
                            if output_file_id:
                                self.append_log(f"âœ… {bid}: {remote.status} (output_file_id: {output_file_id})")
                            else:
                                self.append_log(f"âš ï¸ {bid}: {remote.status} (output_file_id ì—†ìŒ - ë””ë²„ê¹… í•„ìš”)")
                        else:
                            self.append_log(f"âœ… {bid}: {remote.status}")
                except Exception as e:
                    if not silent: self.append_log(f"{bid} ê°±ì‹  ì‹¤íŒ¨: {e}")
        finally:
            self.is_refreshing = False
            self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])
            if not silent: self.append_log("ê°±ì‹  ì™„ë£Œ")

    def _merge_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        # completed ë˜ëŠ” expired ìƒíƒœì¸ ë°°ì¹˜ë„ í¬í•¨ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
        targets = [bid for bid in ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") in ["completed", "expired"]]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ë³‘í•©í•  ìˆ˜ ìˆëŠ” 'completed' ë˜ëŠ” 'expired' ìƒíƒœì˜ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if messagebox.askyesno("ë³‘í•©", f"ì„ íƒí•œ {len(targets)}ê±´ì„ ë³‘í•©í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_merge_multi, args=(targets,))
            t.daemon = True
            t.start()

    def _run_merge_multi(self, ids):
        """
        ì„ íƒëœ Batch ë“¤ì— ëŒ€í•´ ê²°ê³¼ JSONL ë‹¤ìš´ë¡œë“œ + ì—‘ì…€ ë³‘í•©ì„ ìˆ˜í–‰.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ í•˜ë‚˜ì˜ ì—‘ì…€ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.
        """
        key = self.api_key_var.get().strip()
        client = OpenAI(api_key=key)
        success_cnt = 0
        total_cost = 0.0
        
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ë°°ì¹˜ ë¶„ë¥˜
        groups_to_merge = {}  # {group_id: [batch_ids]}
        ungrouped_batches = []  # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤
        
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not job:
                continue
            
            group_id = job.get("batch_group_id")
            if group_id:
                if group_id not in groups_to_merge:
                    groups_to_merge[group_id] = []
                groups_to_merge[group_id].append(bid)
            else:
                ungrouped_batches.append(bid)
        
        # ê·¸ë£¹ë³„ ë³‘í•© ì²˜ë¦¬
        for group_id, batch_ids in groups_to_merge.items():
            self.append_log(f"--- ê·¸ë£¹ ë³‘í•© ì‹œì‘: {group_id} ({len(batch_ids)}ê°œ ë°°ì¹˜) ---")
            try:
                # ê·¸ë£¹ ë‚´ ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì›ë³¸ ì—‘ì…€ ê²½ë¡œ ì‚¬ìš©
                first_job = next((j for j in jobs if j["batch_id"] == batch_ids[0]), None)
                if not first_job:
                    continue
                
                src_path = first_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
                all_results_map = {}  # {custom_id: content}
                total_group_in = 0
                total_group_out = 0
                total_group_cost = 0.0
                total_group_cached = 0
                total_group_requests = 0
                total_group_cache_hits = 0
                model_name = first_job.get("model", "gpt-5-mini")
                
                # ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ (chunk_indexê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ)
                def get_chunk_index(bid):
                    job = next((j for j in jobs if j["batch_id"] == bid), None)
                    if job:
                        idx = job.get("chunk_index")
                        return idx if idx is not None else 999999
                    return 999999
                
                batch_ids_sorted = sorted(batch_ids, key=get_chunk_index)
                
                for bid in batch_ids_sorted:
                    self.append_log(f"  [ê·¸ë£¹] ë°°ì¹˜ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    try:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if not local_job:
                            continue
                        
                        # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                        if local_job.get("status") == "merged":
                            self.append_log(f"  â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        # Batch ìƒíƒœ í™•ì¸
                        remote = client.batches.retrieve(bid)
                        
                        # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                        output_file_id = getattr(remote, "output_file_id", None)
                        if not output_file_id:
                            output_ids = getattr(remote, "output_file_ids", None)
                            if output_ids and isinstance(output_ids, (list, tuple)) and len(output_ids) > 0:
                                output_file_id = output_ids[0]
                        
                        if remote.status == "expired":
                            if not output_file_id:
                                self.append_log(f"  âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                                upsert_batch_job(bid, status=remote.status, output_file_id=None)
                                continue
                            else:
                                self.append_log(f"  â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                        elif remote.status not in ["completed", "expired"]:
                            self.append_log(f"  âš ï¸ {bid}: ì•„ì§ completed ë˜ëŠ” expired ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                            upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id)
                            continue
                        
                        if not output_file_id:
                            self.append_log(f"  âš ï¸ {bid}: output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                        base_name, _ = os.path.splitext(os.path.basename(src_path))
                        base_dir = os.path.dirname(src_path)
                        out_jsonl = os.path.join(base_dir, f"{base_name}_stage1_batch_output_{bid}.jsonl")
                        
                        file_content = client.files.content(output_file_id)
                        if hasattr(file_content, "read"):
                            content_bytes = file_content.read()
                        elif hasattr(file_content, "iter_bytes"):
                            chunks = []
                            for ch in file_content.iter_bytes():
                                chunks.append(ch)
                            content_bytes = b"".join(chunks)
                        else:
                            content_bytes = file_content
                        
                        with open(out_jsonl, "wb") as f:
                            f.write(content_bytes)
                        
                        upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id, output_jsonl=out_jsonl)
                        
                        # JSONL íŒŒì¼ ì½ì–´ì„œ ê²°ê³¼ ìˆ˜ì§‘
                        batch_in_tok = 0
                        batch_out_tok = 0
                        batch_cached_tok = 0
                        batch_total_requests = 0
                        batch_cache_hits = 0
                        with open(out_jsonl, "r", encoding="utf-8") as f:
                            for line in f:
                                if not line.strip():
                                    continue
                                data = json.loads(line)
                                cid = data.get("custom_id")
                                resp = data.get("response")
                                error = data.get("error")
                                if error is not None or not resp or not cid:
                                    continue
                                
                                # core ì˜ íŒŒì„œ ì‚¬ìš© (Responses í¬ë§· ê¸°ì¤€)
                                refined = extract_text_from_response_dict(resp)
                                in_tok, out_tok, _ = extract_usage_from_response_dict(resp)
                                batch_in_tok += in_tok
                                batch_out_tok += out_tok
                                all_results_map[cid] = refined
                                
                                # ìºì‹± í†µê³„ ìˆ˜ì§‘
                                body = resp.get("body", {}) if isinstance(resp, dict) else {}
                                usage = body.get("usage", {})
                                input_tokens_details = usage.get("input_tokens_details", {})
                                cached_tokens = input_tokens_details.get("cached_tokens", 0)
                                batch_cached_tok += cached_tokens
                                batch_total_requests += 1
                                if cached_tokens > 0:
                                    batch_cache_hits += 1
                        
                        # ë°°ì¹˜ë³„ ìºì‹± í†µê³„ ì¶œë ¥
                        if batch_total_requests > 0:
                            cache_hit_rate = (batch_cache_hits / batch_total_requests * 100)
                            cache_savings_pct = (batch_cached_tok / batch_in_tok * 100) if batch_in_tok > 0 else 0
                            self.append_log(f"  [ìºì‹±] {bid}: ìš”ì²­ {batch_total_requests}ê±´, íˆíŠ¸ {batch_cache_hits}ê±´ ({cache_hit_rate:.1f}%), ìºì‹œ í† í° {batch_cached_tok:,} ({cache_savings_pct:.1f}%)")
                        
                        total_group_in += batch_in_tok
                        total_group_out += batch_out_tok
                        total_group_cached += batch_cached_tok
                        total_group_requests += batch_total_requests
                        total_group_cache_hits += batch_cache_hits
                        
                        # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                        pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0.25, "output": 2.0})
                        cost = ((batch_in_tok * pricing["input"] + batch_out_tok * pricing["output"]) / 1_000_000) * 0.5
                        total_group_cost += cost
                        
                        # ìºì‹œë¡œ ì ˆê°ëœ ë¹„ìš© ê³„ì‚°
                        cache_savings = (batch_cached_tok / 1_000_000) * pricing["input"] * 0.5
                        if cache_savings > 0:
                            self.append_log(f"  [ë¹„ìš©ì ˆê°] {bid}: ìºì‹±ìœ¼ë¡œ ${cache_savings:.4f} ì ˆê°")
                        
                    except Exception as e:
                        self.append_log(f"  âŒ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if not all_results_map:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ë³‘í•©í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ì˜ ì „ì²´ ì²­í¬ ìˆ˜ í™•ì¸ ë° ê²€ì¦
                expected_total_chunks = first_job.get("total_chunks")
                if expected_total_chunks:
                    downloaded_batch_ids = []
                    for bid in batch_ids_sorted:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if local_job and local_job.get("status") in ["completed", "expired"]:
                            out_jsonl = local_job.get("output_jsonl") or os.path.join(
                                os.path.dirname(src_path),
                                f"{os.path.splitext(os.path.basename(src_path))[0]}_stage1_batch_output_{bid}.jsonl"
                            )
                            if os.path.exists(out_jsonl):
                                downloaded_batch_ids.append(bid)
                    
                    if len(downloaded_batch_ids) < expected_total_chunks:
                        missing = expected_total_chunks - len(downloaded_batch_ids)
                        self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì˜ˆìƒ {expected_total_chunks}ê°œ ì¤‘ {len(downloaded_batch_ids)}ê°œë§Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({missing}ê°œ ëˆ„ë½ ê°€ëŠ¥)")
                
                # í†µí•© ê²°ê³¼ë¥¼ ì—‘ì…€ì— ë³‘í•©
                base_name, _ = os.path.splitext(os.path.basename(src_path))
                base_dir = os.path.dirname(src_path)
                
                # í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ 1ë‹¨ê³„ ì™„ë£Œ íŒŒì¼ëª…: T0 â†’ T1ë¡œ ë²„ì „ ì—…
                pattern = r"_T(\d+)_I(\d+)"
                match = re.search(pattern, base_name, re.IGNORECASE)
                if match:
                    current_t = int(match.group(1))
                    current_i = int(match.group(2))
                    original_name = re.sub(r"_T\d+_I\d+.*$", "", base_name, flags=re.IGNORECASE).rstrip("_")
                    new_t = current_t + 1
                    new_i = current_i
                    out_filename = f"{original_name}_T{new_t}_I{new_i}.xlsx"
                else:
                    out_filename = f"{base_name}_T1_I0.xlsx"
                out_excel = os.path.join(base_dir, out_filename)
                
                df = pd.read_excel(src_path)
                target_col = "ST1_ê²°ê³¼ìƒí’ˆëª…"
                if target_col not in df.columns:
                    df[target_col] = ""
                df[target_col] = df[target_col].astype(str)
                
                cnt = 0
                for cid, val in all_results_map.items():
                    try:
                        # custom_id í˜•ì‹: row-123  â†’ ì¸ë±ìŠ¤ 123
                        idx = int(str(cid).split("-")[1])
                        if 0 <= idx < len(df):
                            df.at[idx, target_col] = val
                            cnt += 1
                    except Exception:
                        continue
                
                # ì—‘ì…€ ì €ì¥
                if safe_save_excel(df, out_excel):
                    # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ë¥¼ merged ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                    for bid in batch_ids_sorted:
                        upsert_batch_job(bid, out_excel=out_excel, status="merged")
                    
                    # History ê¸°ë¡ (ì²« ë²ˆì§¸ ë°°ì¹˜ ê¸°ì¤€)
                    c_at_str = first_job.get("created_at", "")
                    if c_at_str:
                        c_at = datetime.fromisoformat(c_at_str)
                    else:
                        c_at = datetime.now()
                    finish_dt = datetime.now()
                    append_run_history(
                        stage="Stage 1 Batch",
                        model_name=model_name,
                        reasoning_effort=first_job.get("effort", "low"),
                        src_file=src_path,
                        out_file=out_excel,
                        total_rows=len(df),
                        api_rows=len(all_results_map),
                        elapsed_seconds=(finish_dt - c_at).total_seconds(),
                        total_in_tok=total_group_in,
                        total_out_tok=total_group_out,
                        total_reasoning_tok=0,
                        input_cost_usd=0,
                        output_cost_usd=0,
                        total_cost_usd=total_group_cost,
                        start_dt=c_at,
                        finish_dt=finish_dt,
                        api_type="batch",
                        batch_id=batch_ids[0],  # ì²« ë²ˆì§¸ ë°°ì¹˜ ID ì‚¬ìš©
                        success_rows=cnt,
                        fail_rows=len(all_results_map) - cnt,
                    )
                    
                    # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
                    try:
                        root_name = get_root_filename(src_path)
                        JobManager.update_status(root_name, text_msg="T1(ì™„ë£Œ)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T1(ì™„ë£Œ)")
                    except Exception as e:
                        self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")
                    
                    # ê·¸ë£¹ ì „ì²´ ìºì‹± í†µê³„ ì¶œë ¥
                    group_cache_hit_rate = (total_group_cache_hits / total_group_requests * 100) if total_group_requests > 0 else 0
                    group_cache_savings_pct = (total_group_cached / total_group_in * 100) if total_group_in > 0 else 0
                    group_cache_savings = (total_group_cached / 1_000_000) * pricing["input"] * 0.5
                    
                    self.append_log(f"  [ê·¸ë£¹] ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(out_excel)}")
                    self.append_log(f"  [ê·¸ë£¹ ìºì‹± í†µê³„] ìš”ì²­ {total_group_requests:,}ê±´, íˆíŠ¸ {total_group_cache_hits:,}ê±´ ({group_cache_hit_rate:.1f}%), ìºì‹œ í† í° {total_group_cached:,} ({group_cache_savings_pct:.1f}%)")
                    if group_cache_savings > 0:
                        self.append_log(f"  [ê·¸ë£¹ ë¹„ìš©ì ˆê°] ìºì‹±ìœ¼ë¡œ ì´ ${group_cache_savings:.4f} ì ˆê°")
                    
                    success_cnt += 1
                    total_cost += total_group_cost
                else:
                    self.append_log(f"  [ê·¸ë£¹] ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {out_excel}")
                
            except Exception as e:
                self.append_log(f"âŒ ê·¸ë£¹ {group_id} ë³‘í•© ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
                continue
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        for bid in ungrouped_batches:
            self.append_log(f"--- ë³‘í•© ì‹œì‘: {bid} ---")
            try:
                jobs = load_batch_jobs()
                local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                remote = client.batches.retrieve(bid)
                
                # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                if remote.status == "expired":
                    self.append_log(f"â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")

                # output_file_id / output_file_ids ì²˜ë¦¬ (ì‹ ë²„ì „ í˜¸í™˜)
                output_file_id = getattr(remote, "output_file_id", None)
                if not output_file_id:
                    output_ids = getattr(remote, "output_file_ids", None)
                    if output_ids and isinstance(output_ids, (list, tuple)) and len(output_ids) > 0:
                        output_file_id = output_ids[0]
                if not output_file_id:
                    if remote.status == "expired":
                        self.append_log(f"âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                    else:
                        self.append_log(f"âŒ output_file_id ì—†ìŒ: {bid}")
                    continue

                file_content = client.files.content(output_file_id)
                if hasattr(file_content, "read"):
                    content_bytes = file_content.read()
                elif hasattr(file_content, "iter_bytes"):
                    chunks = []
                    for ch in file_content.iter_bytes():
                        chunks.append(ch)
                    content_bytes = b"".join(chunks)
                else:
                    content_bytes = file_content  # type: ignore

                if local_job and local_job.get("src_excel"):
                    src_path = local_job["src_excel"]
                    base_name, _ = os.path.splitext(os.path.basename(src_path))
                    base_dir = os.path.dirname(src_path)
                    # JSONLì€ ì›ë³¸ê³¼ ê°™ì€ í´ë”ì— ì €ì¥
                    out_jsonl = os.path.join(base_dir, f"{base_name}_stage1_batch_output.jsonl")
                    # í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ 1ë‹¨ê³„ ì™„ë£Œ íŒŒì¼ëª…: T0 â†’ T1ë¡œ ë²„ì „ ì—… (stage1_api_ver_runner.pyì™€ ë™ì¼í•œ ë°©ì‹)
                    pattern = r"_T(\d+)_I(\d+)"
                    match = re.search(pattern, base_name, re.IGNORECASE)
                    if match:
                        current_t = int(match.group(1))
                        current_i = int(match.group(2))
                        # ì›ë³¸ëª… ì¶”ì¶œ (ë²„ì „ ì •ë³´ ì œê±°)
                        original_name = re.sub(r"_T\d+_I\d+.*$", "", base_name, flags=re.IGNORECASE).rstrip("_")
                        # T ë²„ì „ë§Œ +1 (IëŠ” ìœ ì§€)
                        new_t = current_t + 1
                        new_i = current_i
                        out_filename = f"{original_name}_T{new_t}_I{new_i}.xlsx"
                    else:
                        # ë²„ì „ ì •ë³´ê°€ ì—†ìœ¼ë©´ T1_I0ìœ¼ë¡œ ìƒì„±
                        out_filename = f"{base_name}_T1_I0.xlsx"
                    out_excel = os.path.join(base_dir, out_filename)
                else:
                    out_jsonl = os.path.join(BASE_DIR, f"output_{bid}.jsonl")
                    out_excel = os.path.join(BASE_DIR, f"output_{bid}.xlsx")
                    src_path = None

                with open(out_jsonl, "wb") as f:
                    f.write(content_bytes)

                results_map = {}
                batch_in_tok = 0
                batch_out_tok = 0
                batch_cached_tok = 0
                batch_total_requests = 0
                batch_cache_hits = 0

                with open(out_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip():
                            continue
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        resp = data.get("response")
                        error = data.get("error")
                        if error is not None or not resp or not cid:
                            continue
                        
                        # core ì˜ íŒŒì„œ ì‚¬ìš© (Responses í¬ë§· ê¸°ì¤€)
                        refined = extract_text_from_response_dict(resp)
                        in_tok, out_tok, _ = extract_usage_from_response_dict(resp)
                        batch_in_tok += in_tok
                        batch_out_tok += out_tok
                        results_map[cid] = refined
                        
                        # ìºì‹± í†µê³„ ìˆ˜ì§‘
                        body = resp.get("body", {}) if isinstance(resp, dict) else {}
                        usage = body.get("usage", {})
                        input_tokens_details = usage.get("input_tokens_details", {})
                        cached_tokens = input_tokens_details.get("cached_tokens", 0)
                        batch_cached_tok += cached_tokens
                        batch_total_requests += 1
                        if cached_tokens > 0:
                            batch_cache_hits += 1
                
                # ìºì‹± í†µê³„ ì¶œë ¥
                cache_hit_rate = (batch_cache_hits / batch_total_requests * 100) if batch_total_requests > 0 else 0
                cache_savings_pct = (batch_cached_tok / batch_in_tok * 100) if batch_in_tok > 0 else 0
                self.append_log(f"  [ìºì‹±] {bid}: ìš”ì²­ {batch_total_requests}ê±´, íˆíŠ¸ {batch_cache_hits}ê±´ ({cache_hit_rate:.1f}%), ìºì‹œ í† í° {batch_cached_tok:,} ({cache_savings_pct:.1f}%)")
                
                model_name = local_job.get("model", "gpt-5-mini") if local_job else "gpt-5-mini"
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0.25, "output": 2.0})
                # Batch í• ì¸(50%) ë°˜ì˜
                cost = ((batch_in_tok * pricing["input"] + batch_out_tok * pricing["output"]) / 1_000_000) * 0.5
                total_cost += cost
                
                # ìºì‹œë¡œ ì ˆê°ëœ ë¹„ìš© ê³„ì‚°
                cache_savings = (batch_cached_tok / 1_000_000) * pricing["input"] * 0.5
                if cache_savings > 0:
                    self.append_log(f"  [ë¹„ìš©ì ˆê°] {bid}: ìºì‹±ìœ¼ë¡œ ${cache_savings:.4f} ì ˆê°")

                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    target_col = "ST1_ê²°ê³¼ìƒí’ˆëª…"
                    if target_col not in df.columns:
                        df[target_col] = ""
                    df[target_col] = df[target_col].astype(str)
                    cnt = 0
                    for cid, val in results_map.items():
                        try:
                            # custom_id í˜•ì‹: row-123  â†’ ì¸ë±ìŠ¤ 123
                            idx = int(str(cid).split("-")[1])
                            if 0 <= idx < len(df):
                                df.at[idx, target_col] = val
                                cnt += 1
                        except Exception:
                            continue
                    # ì—‘ì…€ ì €ì¥ (ì—´ë ¤ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ ì €ì¥ ìœ í‹¸ ì‚¬ìš©)
                    if safe_save_excel(df, out_excel):
                        upsert_batch_job(bid, out_excel=out_excel, status="merged")

                        # History ê¸°ë¡ (Stage 1) - íƒ€ì„ì¡´ ì •ë³´ í˜¼í•© ë°©ì§€ë¥¼ ìœ„í•´ naive datetime ì‚¬ìš©
                        c_at_str = local_job.get("created_at", "")
                        if c_at_str:
                            # isoformat()ìœ¼ë¡œ ì €ì¥ëœ naive datetime ê°€ì •
                            c_at = datetime.fromisoformat(c_at_str)
                        else:
                            c_at = datetime.now()
                        finish_dt = datetime.now()
                        append_run_history(
                            stage="Stage 1 Batch",
                            model_name=model_name,
                            reasoning_effort=local_job.get("effort", "low"),
                            src_file=src_path,
                            out_file=out_excel,
                            total_rows=len(df),
                            api_rows=len(results_map),
                            elapsed_seconds=(finish_dt - c_at).total_seconds(),
                            total_in_tok=batch_in_tok,
                            total_out_tok=batch_out_tok,
                            total_reasoning_tok=0,
                            input_cost_usd=0,
                            output_cost_usd=0,
                            total_cost_usd=cost,
                            start_dt=c_at,
                            finish_dt=finish_dt,
                            api_type="batch",
                            batch_id=bid,
                            success_rows=cnt,
                            fail_rows=len(results_map) - cnt,
                        )

                        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage1 í…ìŠ¤íŠ¸(T1) ì™„ë£Œ ìƒíƒœ ê¸°ë¡ (img ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                        try:
                            root_name = get_root_filename(src_path)
                            JobManager.update_status(root_name, text_msg="T1(ì™„ë£Œ)")
                            self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T1(ì™„ë£Œ)")
                        except Exception as e:
                            self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")

                        self.append_log(f"âœ… ë³‘í•© ì™„ë£Œ: {os.path.basename(out_excel)}")
                        success_cnt += 1
                else:
                    self.append_log(f"âš ï¸ ì›ë³¸ ì—†ìŒ. JSONLë§Œ ì €ì¥.")
            except Exception as e:
                self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: {e}")
        
        self.append_log(f"=== ì¼ê´„ ë³‘í•© ë (ì„±ê³µ: {success_cnt}, ë¹„ìš©: ${total_cost:.4f}) ===")
        self._load_jobs_all()
        self._safe_msgbox("showinfo", "ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©: ${total_cost:.4f}")

    def _report_selected_unified(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        targets = [bid for bid in ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "merged"]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ìƒíƒœê°€ 'merged'ì¸ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if messagebox.askyesno("ë¦¬í¬íŠ¸", f"ì„ íƒí•œ {len(targets)}ê±´ì˜ ì •ì œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_report_unified, args=(targets,))
            t.daemon = True
            t.start()

    def _run_report_unified(self, ids):
        self.append_log(f"--- ì •ì œ ë¦¬í¬íŠ¸ ìƒì„± ({len(ids)}ê±´) ---")
        jobs = load_batch_jobs()
        all_reps = []
        for bid in ids:
            local_job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not local_job: continue
            out_path = local_job.get("out_excel")
            if not out_path or not os.path.exists(out_path):
                self.append_log(f"âŒ íŒŒì¼ ëˆ„ë½: {bid}")
                continue
            
            try:
                df = pd.read_excel(out_path)
                if "ST1_ê²°ê³¼ìƒí’ˆëª…" not in df.columns or "ì›ë³¸ìƒí’ˆëª…" not in df.columns: continue
                for idx, row in df.iterrows():
                    raw = safe_str(row.get("ì›ë³¸ìƒí’ˆëª…", ""))
                    res = safe_str(row.get("ST1_ê²°ê³¼ìƒí’ˆëª…", ""))
                    
                    is_changed = "O" if raw != res else "X"
                    len_diff = len(res) - len(raw)
                    
                    all_reps.append({
                        "Batch_ID": bid,
                        "í–‰ë²ˆí˜¸": idx+2,
                        "ìƒí’ˆì½”ë“œ": safe_str(row.get("ìƒí’ˆì½”ë“œ", "")),
                        "ì›ë³¸ìƒí’ˆëª…": raw,
                        "ì •ì œìƒí’ˆëª…": res,
                        "ë³€ê²½ì—¬ë¶€": is_changed,
                        "ê¸¸ì´ë³€í™”": f"{len(raw)} -> {len(res)} ({len_diff:+d})"
                    })
            except: pass

        if not all_reps:
            self._safe_msgbox("showinfo", "ì•Œë¦¼", "ë°ì´í„° ì—†ìŒ")
            return

        try:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            path = os.path.join(BASE_DIR, f"Stage1_Clean_Report_{ts}.xlsx")
            pd.DataFrame(all_reps).to_excel(path, index=False)
            self.append_log(f"ğŸ“Š ë¦¬í¬íŠ¸ ì™„ë£Œ: {os.path.basename(path)}")
            
            self.after(0, lambda: self._ask_open_file(path))
            
        except Exception as e: self._safe_msgbox("showerror", "ì˜¤ë¥˜", str(e))

    def _ask_open_file(self, path):
        if messagebox.askyesno("ì™„ë£Œ", "ë¦¬í¬íŠ¸ íŒŒì¼ì„ ì—¬ì‹œê² ìŠµë‹ˆê¹Œ?"):
            try: os.startfile(path)
            except: pass

    def _archive_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if ids and messagebox.askyesno("ë³´ê´€", f"{len(ids)}ê±´ íœ´ì§€í†µ ì´ë™?"):
            archive_batch_job(ids, True)
            self._load_jobs_all()
            self._load_archive_list()

    def _restore_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ë³µêµ¬", f"{len(ids)}ê±´ ë³µêµ¬?"):
            archive_batch_job(ids, False)
            self._load_jobs_all()
            self._load_archive_list()

    def _hard_delete_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ì‚­ì œ", "ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            hard_delete_batch_job(ids)
            self._load_archive_list()

    def _on_tree_double_click(self, event):
        sel = self.tree_active.selection()
        if not sel: return
        item = sel[0]
        item_tags = self.tree_active.item(item)['tags']
        item_values = self.tree_active.item(item)['values']
        
        # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° ì ‘ê¸°/í¼ì¹˜ê¸°
        if 'group_header' in item_tags:
            group_id = item_values[2] if len(item_values) > 2 else ""  # ê·¸ë£¹ IDëŠ” 3ë²ˆì§¸ ì»¬ëŸ¼
            if group_id:
                # ì ‘ê¸°/í¼ì¹˜ê¸° ìƒíƒœ í† ê¸€
                if group_id in self.collapsed_groups:
                    self.collapsed_groups.discard(group_id)
                else:
                    self.collapsed_groups.add(group_id)
                # ëª©ë¡ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ìƒíƒœ ë°˜ì˜
                self._load_jobs_all()
            return
        
        # ì¼ë°˜ ë°°ì¹˜ì¸ ê²½ìš° ê¸°ì¡´ ë™ì‘
        bid = item_values[0] if item_values else ""
        if bid:
            self.batch_id_var.set(bid)
            self.main_tabs.select(self.tab_merge)

    # ----------------------------------------------------
    # Tab 3: Manual
    # ----------------------------------------------------
    def _init_tab_merge(self):
        container = ttk.Frame(self.tab_merge, padding=20)
        container.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì„¹ì…˜
        f_retry = ttk.LabelFrame(container, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", padding=15)
        f_retry.pack(fill='x', pady=(0, 15))
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        f_list = ttk.Frame(f_retry)
        f_list.pack(fill='both', expand=True, pady=(0, 10))
        ttk.Label(f_list, text="ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(anchor='w')
        
        # Treeviewë¡œ ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        list_frame = ttk.Frame(f_list)
        list_frame.pack(fill='both', expand=True, pady=5)
        
        scrollbar = ttk.Scrollbar(list_frame)
        scrollbar.pack(side='right', fill='y')
        
        self.failed_chunks_tree = ttk.Treeview(list_frame, columns=("chunk_num", "file_name", "error_type"), 
                                                show='headings', height=4, yscrollcommand=scrollbar.set)
        scrollbar.config(command=self.failed_chunks_tree.yview)
        
        self.failed_chunks_tree.heading("chunk_num", text="ì²­í¬ ë²ˆí˜¸")
        self.failed_chunks_tree.heading("file_name", text="íŒŒì¼ëª…")
        self.failed_chunks_tree.heading("error_type", text="ì˜¤ë¥˜ ìœ í˜•")
        
        self.failed_chunks_tree.column("chunk_num", width=80, anchor="center")
        self.failed_chunks_tree.column("file_name", width=300, anchor="w")
        self.failed_chunks_tree.column("error_type", width=150, anchor="center")
        
        self.failed_chunks_tree.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ ì…ë ¥
        f_file = ttk.Frame(f_retry)
        f_file.pack(fill='x', pady=(10, 0))
        ttk.Label(f_file, text="ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼:", font=("ë§‘ì€ ê³ ë”•", 9)).pack(side='left')
        self.failed_chunks_file_var = tk.StringVar()
        ttk.Entry(f_file, textvariable=self.failed_chunks_file_var, width=50, font=("Consolas", 9)).pack(side='left', padx=5, fill='x', expand=True)
        btn_select = ttk.Button(f_file, text="ğŸ“‚ ì°¾ê¸°", command=self._select_failed_chunks_file)
        btn_select.pack(side='left', padx=5)
        btn_retry = ttk.Button(f_file, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", command=self._retry_failed_chunks, style="Success.TButton")
        btn_retry.pack(side='left', padx=5)
        
        f_in = ttk.LabelFrame(container, text="ê°œë³„ ì‘ì—…", padding=15)
        f_in.pack(fill='x', pady=(0, 15))
        ttk.Label(f_in, text="Batch ID:").pack(side='left')
        ttk.Entry(f_in, textvariable=self.batch_id_var, width=45, font=("Consolas", 10)).pack(side='left', padx=10)
        
        f_btn = ttk.Frame(container)
        f_btn.pack(fill='x', pady=20)
        ttk.Button(f_btn, text="1. ê²°ê³¼ ë³‘í•©", command=self._start_merge, style="Primary.TButton").pack(fill='x', pady=5)
        ttk.Button(f_btn, text="2. ë‹¨ì¼ ë¦¬í¬íŠ¸", command=self._start_diff_report).pack(fill='x', pady=5)

    def _start_merge(self):
        bid = self.batch_id_var.get().strip()
        if not bid:
            messagebox.showwarning("ê²½ê³ ", "Batch IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            return
        
        # ìƒíƒœ ì²´í¬ (ì•ˆì „ì¥ì¹˜)
        jobs = load_batch_jobs()
        job = next((j for j in jobs if j["batch_id"] == bid), None)
        if job and job.get("status") != "completed":
            if not messagebox.askyesno("ê²½ê³ ", f"í˜„ì¬ ìƒíƒœê°€ '{job.get('status')}'ì…ë‹ˆë‹¤.\nê·¸ë˜ë„ ë³‘í•©ì„ ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
                return

        t = threading.Thread(target=self._run_merge_multi, args=([bid],))
        t.daemon = True
        t.start()

    def _start_diff_report(self):
        t = threading.Thread(target=self._run_diff_report)
        t.daemon = True
        t.start()

    def _run_diff_report(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_report_unified([bid])
    
    def _handle_failed_chunks(self, failed_info_path, failed_chunk_files):
        """ì‹¤íŒ¨í•œ ì²­í¬ê°€ ìˆì„ ë•Œ GUIì— ìë™ìœ¼ë¡œ í‘œì‹œí•˜ê³  ì•Œë¦¼"""
        # ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ìë™ ì„¤ì •
        self.failed_chunks_file_var.set(failed_info_path)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ì„ Treeviewì— í‘œì‹œ
        for item in self.failed_chunks_tree.get_children():
            self.failed_chunks_tree.delete(item)
        
        for failed_info in failed_chunk_files:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
            file_name = os.path.basename(chunk_file)
            
            self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        
        # ì•Œë¦¼ ë©”ì‹œì§€ ë° íƒ­ ì „í™˜
        failed_count = len(failed_chunk_files)
        token_limit_count = sum(1 for f in failed_chunk_files if f.get("is_token_limit", False))
        
        msg = f"âš ï¸ {failed_count}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\n"
        if token_limit_count > 0:
            msg += f"â€¢ í† í° ì œí•œ ì˜¤ë¥˜: {token_limit_count}ê°œ\n"
        msg += f"â€¢ ì‹¤íŒ¨ ì •ë³´ê°€ ìë™ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        msg += f"â€¢ '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        self._safe_msgbox("showwarning", "ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨", msg)
        
        # ì¬ì‹œë„ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
        self.main_tabs.select(self.tab_merge)
    
    def _select_failed_chunks_file(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ"""
        path = filedialog.askopenfilename(
            title="ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ",
            filetypes=[("JSON íŒŒì¼", "*.json"), ("ëª¨ë“  íŒŒì¼", "*.*")]
        )
        if path:
            self.failed_chunks_file_var.set(path)
            # íŒŒì¼ì„ ì„ íƒí•˜ë©´ ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(path)
    
    def _load_failed_chunks_from_file(self, file_path):
        """ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ì„ ì½ì–´ì„œ ëª©ë¡ì— í‘œì‹œ"""
        if not os.path.exists(file_path):
            return
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ê¸°ì¡´ ëª©ë¡ ì‚­ì œ
            for item in self.failed_chunks_tree.get_children():
                self.failed_chunks_tree.delete(item)
            
            # ëª©ë¡ì— ì¶”ê°€
            for failed_info in failed_chunks:
                chunk_num = failed_info.get("chunk_num", 0)
                chunk_file = failed_info.get("chunk_file", "")
                error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
                file_name = os.path.basename(chunk_file)
                
                self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        except Exception as e:
            self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    def _retry_failed_chunks(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„"""
        failed_file = self.failed_chunks_file_var.get().strip()
        if not failed_file:
            self._safe_msgbox("showwarning", "ê²½ê³ ", "ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        if not os.path.exists(failed_file):
            self._safe_msgbox("showerror", "ì˜¤ë¥˜", f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{failed_file}")
            return
        
        if not self.api_key_var.get():
            self._safe_msgbox("showwarning", "ê²½ê³ ", "API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        try:
            with open(failed_file, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(failed_file)
        except Exception as e:
            self._safe_msgbox("showerror", "ì˜¤ë¥˜", f"JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨:\n{e}")
            return
        
        if not failed_chunks:
            self._safe_msgbox("showinfo", "ì•Œë¦¼", "ì¬ì‹œë„í•  ì‹¤íŒ¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if messagebox.askyesno("í™•ì¸", f"{len(failed_chunks)}ê°œ ì‹¤íŒ¨í•œ ì²­í¬ë¥¼ ì¬ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_retry_failed_chunks, args=(failed_chunks,))
            t.daemon = True
            t.start()
    
    def _run_retry_failed_chunks(self, failed_chunks):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì‹¤í–‰"""
        key = self.api_key_var.get().strip()
        import httpx
        timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
        
        self.append_log(f"[RETRY] ì‹¤íŒ¨í•œ ì²­í¬ {len(failed_chunks)}ê°œ ì¬ì‹œë„ ì‹œì‘...")
        
        retry_batch_ids = []
        for failed_info in failed_chunks:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            excel_path = failed_info.get("excel_path", "")
            model_name = failed_info.get("model_name", "gpt-5-mini")
            effort = failed_info.get("effort", "low")
            batch_group_id = failed_info.get("batch_group_id", "")
            
            if not os.path.exists(chunk_file):
                self.append_log(f"âš ï¸ ì²­í¬ {chunk_num}: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_file}")
                continue
            
            self.append_log(f"[RETRY] ì²­í¬ {chunk_num} ì¬ì‹œë„ ì¤‘... ({os.path.basename(chunk_file)})")
            
            try:
                # íŒŒì¼ ì—…ë¡œë“œ
                with open(chunk_file, "rb") as f:
                    batch_input_file = client.files.create(file=f, purpose="batch")
                file_id = batch_input_file.id
                
                # ë°°ì¹˜ ìƒì„±
                batch_job = client.batches.create(
                    input_file_id=file_id,
                    endpoint="/v1/responses",
                    completion_window="24h",
                )
                
                batch_id = batch_job.id
                retry_batch_ids.append(batch_id)
                self.append_log(f"âœ… ì²­í¬ {chunk_num} ì¬ì‹œë„ ì„±ê³µ: {batch_id}")
                
                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=excel_path,
                    jsonl_path=chunk_file,
                    model=model_name,
                    effort=effort,
                    status=batch_job.status,
                    output_file_id=None,
                    batch_group_id=batch_group_id,
                    chunk_index=chunk_num,
                )
                
            except Exception as e:
                self.append_log(f"âŒ ì²­í¬ {chunk_num} ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        if retry_batch_ids:
            self.append_log(f"âœ… ì¬ì‹œë„ ì™„ë£Œ: {len(retry_batch_ids)}ê°œ ë°°ì¹˜ ìƒì„±ë¨")
            # ë°°ì¹˜ ëª©ë¡ ê°±ì‹  ë° ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
            self.after(0, lambda: [
                self._load_jobs_all(),
                self._load_archive_list(),
                self.main_tabs.select(self.tab_manage),  # ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
                self._safe_msgbox("showinfo", "ì™„ë£Œ", f"{len(retry_batch_ids)}ê°œ ì²­í¬ ì¬ì‹œë„ ì„±ê³µ:\n{', '.join(retry_batch_ids[:5])}{'...' if len(retry_batch_ids) > 5 else ''}\n\në°°ì¹˜ ê´€ë¦¬ íƒ­ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.")
            ])
        else:
            self.append_log(f"âš ï¸ ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.after(0, lambda: self._safe_msgbox("showwarning", "ê²½ê³ ", "ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤."))

if __name__ == "__main__":
    app = Stage1BatchGUI()
    app.mainloop()