"""
stage2_batch_api.py

Stage 2 Batch API ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (GUI) - Final Version
- ê¸°ëŠ¥: ì—‘ì…€(ì´ë¯¸ì§€+í…ìŠ¤íŠ¸) -> Batch JSONL ìƒì„±(Vision API) -> ì—…ë¡œë“œ -> ì‹¤í–‰ -> ë³‘í•© -> JSON ë¶„ì„ ë¦¬í¬íŠ¸
- íŠ¹ì§•: GPT-5/4o ëª¨ë¸ ì§€ì›, ì´ë¯¸ì§€ ì˜µì…˜ ì²˜ë¦¬, ìƒì„¸ íˆ´íŒ ë° ê°€ì´ë“œ í¬í•¨
"""

import os
import sys
import json
import threading
import subprocess
import re
from datetime import datetime

import pandas as pd
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, Menu
from tkinter.scrolledtext import ScrolledText

from openai import OpenAI


# ========================================================
# ë©”ì¸ ëŸ°ì²˜ ì—°ë™ìš© JobManager & íŒŒì¼ëª… ìœ í‹¸ (Stage2 ì „ìš©)
# ========================================================
def get_root_filename(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´(_T*_I* ë˜ëŠ” _t*_i*) ë° ê¸°íƒ€ ê¼¬ë¦¬í‘œë¥¼ ë–¼ê³  ì›ë³¸ëª…(Key)ë§Œ ì¶”ì¶œ
    ì˜ˆ: ìƒí’ˆ_T0_I0.xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T2_I1.xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T1_I0(ì—…ì™„).xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T1_I0_T2_I1.xlsx -> ìƒí’ˆ.xlsx (ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ ì œê±°)
    """
    name = os.path.basename(filename)
    base, ext = os.path.splitext(name)

    # 1. ë²„ì „ íŒ¨í„´ (_Tìˆ«ì_Iìˆ«ì ë˜ëŠ” _tìˆ«ì_iìˆ«ì) ë°˜ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    # íŒ¨í„´ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì œê±°
    while True:
        new_base = re.sub(r"_[Tt]\d+_[Ii]\d+", "", base, flags=re.IGNORECASE)
        if new_base == base:
            break
        base = new_base
    
    # 2. ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„), (ì™„ë£Œ) ë“±)
    base = re.sub(r"\([^)]*\)", "", base)
    
    # 3. ê¸°íƒ€ êµ¬í˜• ê¼¬ë¦¬í‘œ ì œê±° (í˜¸í™˜ì„± ìœ ì§€)
    suffixes = ["_stage1_mapping", "_stage1_img_mapping", "_with_images"]
    for s in suffixes:
        base = base.replace(s, "")
    
    # 4. ëì— ë‚¨ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    base = base.rstrip("_")

    return base + ext


def get_next_version_path(current_path: str, task_type: str = "text") -> str:
    """
    í˜„ì¬ íŒŒì¼ëª…ì„ ë¶„ì„í•´ì„œ ë‹¤ìŒ ë‹¨ê³„ì˜ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    íŒŒì¼ëª… í˜•ì‹: ì›ë³¸ëª…_T{ìˆ«ì}_I{ìˆ«ì}.xlsx
    - task_type='text'  â†’ T ë²„ì „ +1 (Stage1: T1, Stage2: T2, ...)
    - task_type='image' â†’ I ë²„ì „ +1
    
    ì£¼ì˜: íŒŒì¼ëª…ì— ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ íŒ¨í„´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    dir_name = os.path.dirname(current_path)
    base_name = os.path.basename(current_path)
    name_only, ext = os.path.splitext(base_name)

    # ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„))
    name_only_clean = re.sub(r"\([^)]*\)", "", name_only)
    
    # ë§ˆì§€ë§‰ _T*_I* íŒ¨í„´ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ, ì—¬ëŸ¬ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ ê²ƒë§Œ)
    all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
    
    if all_matches:
        # ë§ˆì§€ë§‰ ë§¤ì¹­ ì‚¬ìš©
        match = all_matches[-1]
        current_t = int(match.group(2))
        current_i = int(match.group(4))
        # ì›ë³¸ëª…ì€ ë§ˆì§€ë§‰ íŒ¨í„´ ì´ì „ê¹Œì§€
        original_name = name_only_clean[: match.start()].rstrip("_")
    else:
        # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì›ë³¸ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì œê±° í›„ ì‚¬ìš©
        original_name = name_only_clean
        # ê¸°ì¡´ ë²„ì „ íŒ¨í„´ ì œê±°
        while True:
            new_name = re.sub(r"_[Tt]\d+_[Ii]\d+", "", original_name, flags=re.IGNORECASE)
            if new_name == original_name:
                break
            original_name = new_name
        original_name = original_name.rstrip("_")
        current_t = 0
        current_i = 0

    if task_type == "text":
        new_t = current_t + 1
        new_i = current_i
    elif task_type == "image":
        new_t = current_t
        new_i = current_i + 1
    else:
        return current_path

    new_filename = f"{original_name}_T{new_t}_I{new_i}{ext}"
    return os.path.join(dir_name, new_filename)


class JobManager:
    DB_FILE = None

    @classmethod
    def find_db_path(cls):
        if cls.DB_FILE and os.path.exists(cls.DB_FILE):
            return cls.DB_FILE

        current_dir = os.path.dirname(os.path.abspath(__file__))
        search_dirs = [
            current_dir,
            os.path.abspath(os.path.join(current_dir, "..")),
            os.path.abspath(os.path.join(current_dir, "..", "..")),
        ]

        for d in search_dirs:
            target = os.path.join(d, "job_history.json")
            if os.path.exists(target):
                cls.DB_FILE = target
                print(f"[JobManager] DB Found: {target}")
                return target

        default_path = os.path.abspath(os.path.join(current_dir, "..", "job_history.json"))
        cls.DB_FILE = default_path
        return default_path

    @classmethod
    def load_jobs(cls):
        db_path = cls.find_db_path()
        if not os.path.exists(db_path):
            return {}
        try:
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    @classmethod
    def update_status(cls, filename, text_msg=None, img_msg=None):
        """ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ìƒíƒœ ì—…ë°ì´íŠ¸ (Stage1/Stage2 ê³µìš©)."""
        db_path = cls.find_db_path()
        data = cls.load_jobs()
        now = datetime.now().strftime("%m-%d %H:%M")

        if filename not in data:
            data[filename] = {
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "text_status": "ëŒ€ê¸°",
                "text_time": "-",
                "image_status": "ëŒ€ê¸°",
                "image_time": "-",
                "memo": "",
            }

        if text_msg:
            data[filename]["text_status"] = text_msg
            data[filename]["text_time"] = now
        if img_msg:
            data[filename]["image_status"] = img_msg
            data[filename]["image_time"] = now

        data[filename]["last_update"] = now

        try:
            with open(db_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"[JobManager Error] {e}")


def safe_save_excel(df: pd.DataFrame, path: str) -> bool:
    """ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆì–´ ì €ì¥ì´ ì•ˆ ë  ë•Œ ì¬ì‹œë„ë¥¼ ìœ ë„í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            df.to_excel(path, index=False)
            return True
        except PermissionError:
            if not messagebox.askretrycancel(
                "ì €ì¥ ì‹¤íŒ¨",
                f"ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤!\n[{os.path.basename(path)}]\n\n"
                "íŒŒì¼ì„ ë‹«ê³  'ë‹¤ì‹œ ì‹œë„'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
            ):
                return False
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False


# [í•„ìˆ˜ ì˜ì¡´ì„±] stage2_core.py / stage2_run_history.py
try:
    from stage2_core import (
        safe_str,
        build_stage2_request_from_row, # Row -> Request ê°ì²´(í”„ë¡¬í”„íŠ¸+ì´ë¯¸ì§€ê²½ë¡œ) ë³€í™˜
    )
    from stage2_run_history import append_run_history
except ImportError:
    # ì˜ì¡´ì„± íŒŒì¼ ë¶€ì¬ ì‹œ ë¹„ìƒìš© ë”ë¯¸
    def safe_str(x): return str(x) if x is not None else ""
    def build_stage2_request_from_row(*args, **kwargs): return None
    def append_run_history(*args, **kwargs): pass

# ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ê¸°ì¡´ Stage2 Batch ì½”ì–´ ë¡œì§ ì¬ì‚¬ìš©
try:
    from stage2_batch_api_ê¸°ì¡´gpt import (
        create_stage2_batch_input_jsonl,
        create_batch_from_jsonl,
        download_batch_output_if_ready,
        merge_batch_output_to_excel,
    )
except ImportError:
    # êµ¬ë²„ì „ í™˜ê²½ì—ì„œëŠ” ì´ ëª¨ë“ˆì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ ê²½ìš°ì—ëŠ” ì•„ë˜ ìƒˆ ë¡œì§ë§Œ ì‚¬ìš©
    create_stage2_batch_input_jsonl = None  # type: ignore
    create_batch_from_jsonl = None  # type: ignore
    download_batch_output_if_ready = None  # type: ignore
    merge_batch_output_to_excel = None  # type: ignore

# === ê¸°ë³¸ ì„¤ì • ===
API_KEY_FILE = ".openai_api_key_stage2_batch"
BATCH_JOBS_FILE = os.path.join(os.path.dirname(__file__), "stage2_batch_jobs.json")

# Stage 2ìš© Batch ëª¨ë¸/ê°€ê²© (stage2_core ì™€ ë™ì¼í•œ gpt-5 ê³„ì—´ë§Œ ì‚¬ìš©)
MODEL_PRICING_USD_PER_MTOK = {
    "gpt-5": {
        "input": 1.25,
        "output": 10.0,
    },
    "gpt-5-mini": {
        "input": 0.25,
        "output": 2.00,
    },
    "gpt-5-nano": {
        "input": 0.05,
        "output": 0.40,
    },
}

# --- UI ìƒ‰ìƒ íŒ”ë ˆíŠ¸ (Modern Blue) ---
COLOR_BG = "#F8F9FA"
COLOR_WHITE = "#FFFFFF"
COLOR_PRIMARY = "#4A90E2"
COLOR_PRIMARY_HOVER = "#357ABD"
COLOR_SUCCESS = "#28A745"
COLOR_SUCCESS_HOVER = "#218838"
COLOR_DANGER = "#DC3545"
COLOR_DANGER_HOVER = "#C82333"
COLOR_TEXT = "#333333"
COLOR_HEADER = "#E9ECEF"

def get_seoul_now():
    try:
        from pytz import timezone
        return datetime.now(timezone("Asia/Seoul"))
    except:
        return datetime.now()

def load_api_key_from_file(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f: return f.read().strip()
    return ""

def save_api_key_to_file(key, path):
    with open(path, "w", encoding="utf-8") as f: f.write(key)

# ========================================================
# íˆ´íŒ í´ë˜ìŠ¤ (ìµœìƒë‹¨ ì •ì˜)
# ========================================================
class ToolTip:
    def __init__(self, widget, text, wraplength=400):
        self.widget = widget
        self.text = text
        self.wraplength = wraplength
        self.tipwindow = None
        self.widget.bind("<Enter>", self.show_tip)
        self.widget.bind("<Leave>", self.hide_tip)

    def show_tip(self, event=None):
        if self.tipwindow or not self.text: return
        x = self.widget.winfo_rootx() + 20
        y = self.widget.winfo_rooty() + 20
        self.tipwindow = tw = tk.Toplevel(self.widget)
        tw.wm_overrideredirect(True)
        tw.wm_geometry(f"+{x}+{y}")
        label = tk.Label(tw, text=self.text, justify='left', background="#ffffe0", relief='solid', borderwidth=1, font=("ë§‘ì€ ê³ ë”•", 9), wraplength=self.wraplength)
        label.pack(ipadx=4, ipady=2)

    def hide_tip(self, event=None):
        if self.tipwindow:
            self.tipwindow.destroy()
            self.tipwindow = None

# ========================================================
# ë°°ì¹˜ ì¡ ê´€ë¦¬ (JSON DB)
# ========================================================
def load_batch_jobs():
    if not os.path.exists(BATCH_JOBS_FILE): return []
    try:
        with open(BATCH_JOBS_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return []

def save_batch_jobs(jobs):
    try:
        with open(BATCH_JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
    except Exception as e: print(f"[Error] ì¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def upsert_batch_job(batch_id, **kwargs):
    jobs = load_batch_jobs()
    found = False
    now_str = datetime.now().isoformat()
    for j in jobs:
        if j["batch_id"] == batch_id:
            if kwargs.get("status") == "completed" and j.get("status") != "completed":
                if "completed_at" not in kwargs: j["completed_at"] = now_str
            j.update(kwargs)
            j["updated_at"] = now_str
            found = True
            break
    if not found:
        new_job = {
            "batch_id": batch_id, "created_at": now_str, "updated_at": now_str,
            "completed_at": "", "archived": False, **kwargs
        }
        jobs.insert(0, new_job)
    save_batch_jobs(jobs)

def archive_batch_job(batch_ids, archive=True):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    for j in jobs:
        if j["batch_id"] in batch_ids: j["archived"] = archive
    save_batch_jobs(jobs)

def hard_delete_batch_job(batch_ids):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    jobs = [j for j in jobs if j["batch_id"] not in batch_ids]
    save_batch_jobs(jobs)

# ========================================================
# Payload Builder (Stage 2 ì „ìš©)
# ========================================================
def build_stage2_batch_payload(row_index, row, model, effort, use_thumb, allow_url):
    """
    Stage 2 Coreì˜ ë¡œì§ì„ í™œìš©í•˜ì—¬ Batch Payload ìƒì„±
    - use_thumb: ì¸ë„¤ì¼(ì´ë¯¸ì§€ëŒ€) í¬í•¨ ì—¬ë¶€
    - allow_url: URL ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í—ˆìš© ì—¬ë¶€ (Core ë¡œì§ì— ë”°ë¼ ë™ì‘)
    """
    try:
        # Core í•¨ìˆ˜ í˜¸ì¶œ (Coreê°€ ì˜µì…˜ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ë¡œì§ ìˆ˜í–‰)
        # ì—¬ê¸°ì„œëŠ” Coreê°€ row ì „ì²´ë¥¼ ë°›ì•„ íŒë‹¨í•œë‹¤ê³  ê°€ì •í•˜ê³ , ì˜µì…˜ì— ë”°ë¼ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•„í„°ë§í•  ìˆ˜ë„ ìˆìŒ
        # í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ Core êµ¬í˜„ìƒ rowì— ìˆëŠ” ì •ë³´ë¥¼ ë‹¤ ê°€ì ¸ì˜¤ë¯€ë¡œ, 
        # Core ìˆ˜ì • ì—†ì´ ì—¬ê¸°ì„œ request ê°ì²´ì˜ image_pathsë¥¼ ì¡°ì‘í•˜ëŠ” ê²ƒì´ ì•ˆì „í•¨.
        req = build_stage2_request_from_row(row)
    except Exception:
        return None

    if not req: return None

    # 1. ì´ë¯¸ì§€ í•„í„°ë§ ë¡œì§ (ì˜µì…˜ ì ìš©)
    final_messages = []
    
    # Prompt ë©”ì‹œì§€ (System/User text)
    # Coreê°€ ë§Œë“  messages ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì´ë¯¸ì§€ ë¶€ë¶„ë§Œ í•„í„°ë§
    for msg in req.messages:
        if not isinstance(msg.get('content'), list):
            final_messages.append(msg) # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            continue
            
        new_content = []
        for item in msg['content']:
            if item['type'] == 'text':
                new_content.append(item)
            elif item['type'] == 'image_url':
                url = item['image_url']['url']
                
                # ì¸ë„¤ì¼ ì œì™¸ ë¡œì§ (ì´ë¯¸ì§€ëŒ€ ì»¬ëŸ¼ê°’ê³¼ ë¹„êµ)
                thumb_val = safe_str(row.get("ì´ë¯¸ì§€ëŒ€", ""))
                if not use_thumbnail and thumb_val and url == thumb_val:
                    continue # ì¸ë„¤ì¼ì´ë©´ ê±´ë„ˆëœ€
                
                # URL í—ˆìš© ì—¬ë¶€ ë¡œì§
                if not allow_url and (url.startswith("http://") or url.startswith("https://")):
                    continue # URL ë¹„í—ˆìš©ì´ë©´ ê±´ë„ˆëœ€
                
                new_content.append(item)
        
        if new_content:
            final_messages.append({"role": msg['role'], "content": new_content})

    # 2. Body êµ¬ì„±
    body = {
        "model": model,
        "messages": final_messages,
        "response_format": {"type": "json_object"} # JSON ì¶œë ¥ ê°•ì œ
    }
    
    # 3. ì¶”ë¡  ëª¨ë¸ íŒŒë¼ë¯¸í„°
    is_reasoning = any(x in model for x in ["gpt-5", "o1", "o3"])
    if is_reasoning and effort in ["low", "medium", "high"]:
        body["reasoning_effort"] = effort
    elif not is_reasoning:
        body["temperature"] = 0.0 # ì •ë°€ë„ ìš°ì„ 

    # 4. Batch Request ê°ì²´
    request_obj = {
        "custom_id": f"row_{row_index}",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": body
    }
    return request_obj

# ========================================================
# GUI Class
# ========================================================
class Stage2BatchGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Stage 2: Batch API Manager (Multimodal & Analytics)")
        self.geometry("1280x950")
        
        self.api_key_var = tk.StringVar()
        self.src_file_var = tk.StringVar()
        
        # ê¸°ë³¸ê°’
        self.model_var = tk.StringVar(value="gpt-5-mini") 
        self.effort_var = tk.StringVar(value="medium")
        self.skip_exist_var = tk.BooleanVar(value=True)
        
        # Stage 2 ì˜µì…˜
        self.use_thumbnail_var = tk.BooleanVar(value=False)  # ê¸°ë³¸ê°’: ì¸ë„¤ì¼ ì œì™¸ (ì„±ëŠ¥ ìµœì í™”)
        self.allow_url_var = tk.BooleanVar(value=False)

        self.batch_id_var = tk.StringVar()
        
        self._configure_styles()
        self._init_ui()
        self._load_key()

    def _configure_styles(self):
        style = ttk.Style()
        try: style.theme_use('clam')
        except: pass
        
        self.configure(background=COLOR_BG)
        
        style.configure("TFrame", background=COLOR_BG)
        style.configure("TLabel", background=COLOR_BG, foreground=COLOR_TEXT, font=("ë§‘ì€ ê³ ë”•", 10))
        style.configure("Header.TLabel", font=("ë§‘ì€ ê³ ë”•", 11, "bold"), foreground="#444")
        
        style.configure("TLabelframe", background=COLOR_BG, bordercolor="#D0D7DE")
        style.configure("TLabelframe.Label", background=COLOR_BG, foreground="#0056b3", font=("ë§‘ì€ ê³ ë”•", 10, "bold"))

        style.configure("TNotebook", background=COLOR_BG, borderwidth=0)
        style.configure("TNotebook.Tab", background="#E1E4E8", padding=[12, 5], font=("ë§‘ì€ ê³ ë”•", 10))
        style.map("TNotebook.Tab", background=[("selected", COLOR_WHITE)], foreground=[("selected", COLOR_PRIMARY)])
        
        style.configure("Treeview", background=COLOR_WHITE, fieldbackground=COLOR_WHITE, font=("ë§‘ì€ ê³ ë”•", 9), rowheight=28)
        style.configure("Treeview.Heading", background=COLOR_HEADER, foreground="#333", font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        style.map("Treeview", background=[('selected', '#CCE5FF')], foreground=[('selected', 'black')])

        style.configure("TButton", font=("ë§‘ì€ ê³ ë”•", 9), padding=5, borderwidth=1)
        style.configure("Primary.TButton", background=COLOR_PRIMARY, foreground="white", bordercolor=COLOR_PRIMARY)
        style.map("Primary.TButton", background=[("active", COLOR_PRIMARY_HOVER)])
        style.configure("Success.TButton", background=COLOR_SUCCESS, foreground="white", bordercolor=COLOR_SUCCESS)
        style.map("Success.TButton", background=[("active", COLOR_SUCCESS_HOVER)])
        style.configure("Danger.TButton", background=COLOR_DANGER, foreground="white", bordercolor=COLOR_DANGER)
        style.map("Danger.TButton", background=[("active", COLOR_DANGER_HOVER)])

    def _init_ui(self):
        main_container = ttk.Frame(self, padding=15)
        main_container.pack(fill='both', expand=True)

        # 1. ìƒë‹¨ API
        f_top = ttk.LabelFrame(main_container, text="ğŸ”‘ API ì„¤ì •", padding=10)
        f_top.pack(fill='x', pady=(0, 10))
        ttk.Label(f_top, text="Batch API Key:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(side='left')
        entry_key = ttk.Entry(f_top, textvariable=self.api_key_var, show="*", width=50, font=("Consolas", 10))
        entry_key.pack(side='left', padx=10)
        btn_save = ttk.Button(f_top, text="ì €ì¥", command=self._save_key, style="Primary.TButton")
        btn_save.pack(side='left')
        ToolTip(btn_save, "ì…ë ¥í•œ API Keyë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.")

        btn_help = ttk.Button(f_top, text="â“ ì‚¬ìš©ë²• / ì›Œí¬í”Œë¡œìš°", command=self._show_help_dialog)
        btn_help.pack(side='right')

        # 2. ë©”ì¸ íƒ­
        self.main_tabs = ttk.Notebook(main_container)
        self.main_tabs.pack(fill='both', expand=True, pady=5)
        
        self.tab_create = ttk.Frame(self.main_tabs)
        self.tab_manage = ttk.Frame(self.main_tabs) 
        self.tab_merge = ttk.Frame(self.main_tabs)
        
        self.main_tabs.add(self.tab_create, text=" 1. ë°°ì¹˜ ìƒì„± & ì—…ë¡œë“œ ")
        self.main_tabs.add(self.tab_manage, text=" 2. ë°°ì¹˜ ê´€ë¦¬ (ëª©ë¡/ë³‘í•©/ë¦¬í¬íŠ¸) ")
        self.main_tabs.add(self.tab_merge, text=" 3. ê°œë³„ ë³‘í•© (ìˆ˜ë™) ")
        
        self._init_tab_create()
        self._init_tab_manage()
        self._init_tab_merge()
        
        # 3. ë¡œê·¸
        f_log = ttk.LabelFrame(main_container, text="ğŸ“‹ ì‹œìŠ¤í…œ ë¡œê·¸", padding=10)
        f_log.pack(fill='both', expand=True, pady=(10, 0))
        self.log_widget = ScrolledText(f_log, height=12, state='disabled', font=("Consolas", 9), bg="#F1F3F5")
        self.log_widget.pack(fill='both', expand=True)

    def _load_key(self):
        loaded = load_api_key_from_file(API_KEY_FILE)
        if loaded: self.api_key_var.set(loaded)

    def _save_key(self):
        k = self.api_key_var.get().strip()
        if k:
            save_api_key_to_file(k, API_KEY_FILE)
            messagebox.showinfo("ì €ì¥", "API Key ì €ì¥ ì™„ë£Œ")

    def append_log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        self.log_widget.config(state='normal')
        self.log_widget.insert('end', f"[{ts}] {msg}\n")
        self.log_widget.see('end')
        self.log_widget.config(state='disabled')

    def _show_help_dialog(self):
        msg = (
            "[Stage 2 Batch API ì›Œí¬í”Œë¡œìš°]\n\n"
            "1. [ìƒì„± íƒ­]: ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ê³  'Start Batch'ë¥¼ í´ë¦­.\n"
            "   - ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ê²½ìš° Vision API ìš”ì²­ì„ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.\n"
            "   - ë¹„ìš©ì€ ì‹¤ì‹œê°„ API ëŒ€ë¹„ 50% ì ˆê°ë©ë‹ˆë‹¤.\n\n"
            "2. [ê´€ë¦¬ íƒ­]: ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ê³  'ì™„ë£Œ(completed)' ì‹œ ë³‘í•©í•©ë‹ˆë‹¤.\n"
            "   - [ì„ íƒ ê°±ì‹ ]: OpenAI ì„œë²„ì—ì„œ ìµœì‹  ìƒíƒœë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n"
            "   - [ì„ íƒ ë³‘í•©]: ê²°ê³¼ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì›ë³¸ ì—‘ì…€(ST2_JSON)ì— ì €ì¥í•©ë‹ˆë‹¤.\n"
            "   - [ë¶„ì„ ë¦¬í¬íŠ¸]: JSON íŒŒì‹± ì„±ê³µë¥ , í‚¤ì›Œë“œ ìˆ˜ ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n\n"
            "â€» ì´ë¯¸ì§€ê°€ ì—†ëŠ” í–‰ì€ í…ìŠ¤íŠ¸ë§Œ ë¶„ì„í•˜ê±°ë‚˜, ì„¤ì •ì— ë”°ë¼ ê±´ë„ˆëœë‹ˆë‹¤."
        )
        messagebox.showinfo("ì‚¬ìš©ë²•", msg)

    # ----------------------------------------------------
    # Tab 1: Create
    # ----------------------------------------------------
    def _init_tab_create(self):
        container = ttk.Frame(self.tab_create, padding=20)
        container.pack(fill='both', expand=True)
        
        # Step 1: íŒŒì¼
        f_file = ttk.LabelFrame(container, text="1. ì‘ì—… ëŒ€ìƒ íŒŒì¼ ì„ íƒ", padding=15)
        f_file.pack(fill='x', pady=(0, 15))
        ttk.Entry(f_file, textvariable=self.src_file_var, font=("ë§‘ì€ ê³ ë”•", 10)).pack(side='left', fill='x', expand=True)
        btn_file = ttk.Button(f_file, text="ğŸ“‚ íŒŒì¼ ì°¾ê¸°", command=self._select_src_file)
        btn_file.pack(side='right', padx=5)
        ToolTip(btn_file, "Stage 2ë¥¼ ìˆ˜í–‰í•  ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.\nì´ë¯¸ì§€ ì»¬ëŸ¼ì´ ì—†ì–´ë„ í…ìŠ¤íŠ¸ ë¶„ì„ì´ ê°€ëŠ¥í•˜ë©´ ì§„í–‰ë©ë‹ˆë‹¤.")
        
        # Step 2: ì˜µì…˜
        f_opt = ttk.LabelFrame(container, text="2. ë°°ì¹˜ ì˜µì…˜ ì„¤ì •", padding=15)
        f_opt.pack(fill='x', pady=5)
        
        # ëª¨ë¸/Effort
        fr1 = ttk.Frame(f_opt)
        fr1.pack(fill='x', pady=5)
        ttk.Label(fr1, text="ëª¨ë¸ (Model):", width=12).pack(side='left')
        models = list(MODEL_PRICING_USD_PER_MTOK.keys())
        cb_model = ttk.Combobox(fr1, textvariable=self.model_var, values=models, state="readonly", width=20)
        cb_model.pack(side='left', padx=5)
        ToolTip(cb_model, "ì´ë¯¸ì§€ ë¶„ì„(Vision) ì„±ëŠ¥ì´ ë›°ì–´ë‚œ GPT-5 ê³„ì—´ ëª¨ë¸ ê¶Œì¥.")
        
        ttk.Label(fr1, text="ì¶”ë¡  ê°•ë„:", width=10).pack(side='left', padx=(20, 5))
        cb_effort = ttk.Combobox(fr1, textvariable=self.effort_var, values=["low", "medium", "high"], state="readonly", width=12)
        cb_effort.pack(side='left', padx=5)
        ToolTip(cb_effort, "ì´ë¯¸ì§€ ë¶„ì„ì˜ ê¹Šì´ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\nMedium ê¶Œì¥.")
        
        # Stage 2 ì „ìš© ì˜µì…˜
        fr2 = ttk.Frame(f_opt)
        fr2.pack(fill='x', pady=5)
        chk_skip = ttk.Checkbutton(fr2, text=" ì´ë¯¸ ST2_JSONì´ ìˆëŠ” í–‰ì€ ê±´ë„ˆë›°ê¸° (Skip)", variable=self.skip_exist_var)
        chk_skip.pack(side='left', padx=5)
        ToolTip(chk_skip, "ì¤‘ë³µ ê³¼ê¸ˆ ë°©ì§€ë¥¼ ìœ„í•´ ì´ë¯¸ ê²°ê³¼ê°€ ìˆëŠ” í–‰ì€ ì œì™¸í•©ë‹ˆë‹¤.")

        chk_thumb = ttk.Checkbutton(fr2, text=" ì¸ë„¤ì¼(ì´ë¯¸ì§€ëŒ€) í¬í•¨", variable=self.use_thumbnail_var)
        chk_thumb.pack(side='left', padx=20)
        ToolTip(chk_thumb, "ì²´í¬ ì‹œ: ëŒ€í‘œ ì´ë¯¸ì§€(ì´ë¯¸ì§€ëŒ€)ë„ AIì—ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.\ní•´ì œ ì‹œ: ìƒì„¸ í˜ì´ì§€ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")

        chk_url = ttk.Checkbutton(fr2, text=" URL ì´ë¯¸ì§€ í—ˆìš©", variable=self.allow_url_var)
        chk_url.pack(side='left', padx=20)
        ToolTip(chk_url, "ì²´í¬ ì‹œ: ì›¹ ë§í¬(http) ì´ë¯¸ì§€ë„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.\ní•´ì œ ì‹œ: ë¡œì»¬ íŒŒì¼ ê²½ë¡œë§Œ ì¸ì‹í•©ë‹ˆë‹¤.")

        # Step 3: ì‹¤í–‰
        f_step3 = ttk.LabelFrame(container, text="3. ì‹¤í–‰", padding=15)
        f_step3.pack(fill='x', pady=15)
        btn_run = ttk.Button(f_step3, text="ğŸš€ JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (Start Batch)", command=self._start_create_batch, style="Success.TButton")
        btn_run.pack(fill='x', ipady=8)
        ToolTip(btn_run, "1. ì—‘ì…€ ì½ê¸° (ì´ë¯¸ì§€ í¬í•¨)\n2. JSONL ìƒì„±\n3. ë°°ì¹˜ ì‹œì‘ ìš”ì²­ (24ì‹œê°„ ë‚´ ì™„ë£Œ)")
        
        ttk.Label(container, text="â€» ë°°ì¹˜ APIëŠ” ê²°ê³¼ ìˆ˜ì‹ ê¹Œì§€ ìµœëŒ€ 24ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. (ë¹„ìš© 50% ì ˆê°)", foreground="#666").pack()

    def _select_src_file(self):
        p = filedialog.askopenfilename(
            title="Stage2 ì—‘ì…€ ì„ íƒ (T1 ë²„ì „ë§Œ ê°€ëŠ¥)",
            filetypes=[("Excel", "*.xlsx;*.xls")]
        )
        if p:
            # T1 í¬í•¨ ì—¬ë¶€ ê²€ì¦
            base_name = os.path.splitext(os.path.basename(p))[0]
            if not re.search(r"_T1_[Ii]\d+", base_name, re.IGNORECASE):
                messagebox.showerror(
                    "ì˜¤ë¥˜", 
                    f"ì´ ë„êµ¬ëŠ” T1 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                    f"ì„ íƒí•œ íŒŒì¼: {os.path.basename(p)}\n"
                    f"íŒŒì¼ëª…ì— '_T1_I*' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
                )
                return
            self.src_file_var.set(p)

    def _start_create_batch(self):
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Key í•„ìš”")
            return
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ ì„ íƒ í•„ìš”")
            return
        
        # T1 í¬í•¨ ì—¬ë¶€ ê²€ì¦
        src = self.src_file_var.get().strip()
        base_name = os.path.splitext(os.path.basename(src))[0]
        if not re.search(r"_T1_[Ii]\d+", base_name, re.IGNORECASE):
            messagebox.showerror(
                "ì˜¤ë¥˜", 
                f"ì´ ë„êµ¬ëŠ” T1 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                f"ì„ íƒí•œ íŒŒì¼: {os.path.basename(src)}\n"
                f"íŒŒì¼ëª…ì— '_T1_I*' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
            )
            return
        
        t = threading.Thread(target=self._run_create_batch)
        t.daemon = True
        t.start()

    def _run_create_batch(self):
        """
        1ë‹¨ê³„: ì—‘ì…€ â†’ /v1/responsesìš© JSONL ìƒì„± (ê¸°ì¡´ ì•ˆì • ë²„ì „ ì½”ì–´ ì‚¬ìš©)
        2ë‹¨ê³„: JSONL ì—…ë¡œë“œ â†’ Batch ìƒì„±
        3ë‹¨ê³„: í˜„ì¬ GUIì˜ ì‘ì—… ì´ë ¥ì— ê¸°ë¡
        """
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        model = self.model_var.get().strip() or "gpt-5-mini"
        effort = self.effort_var.get().strip() or "medium"

        use_thumb = self.use_thumbnail_var.get()
        allow_url = self.allow_url_var.get()

        try:
            if create_stage2_batch_input_jsonl is None or create_batch_from_jsonl is None:
                raise RuntimeError("stage2_batch_api_ê¸°ì¡´gpt ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ Batch ì½”ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            self.append_log(f"[RUN] Stage2: JSONL ìƒì„± + Batch ìƒì„± ì‹œì‘ â†’ {os.path.basename(src)}")

            # 1) ì—‘ì…€ â†’ JSONL ìƒì„±
            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_stage2_batch_input.jsonl"

            info = create_stage2_batch_input_jsonl(
                excel_path=src,
                jsonl_path=jsonl_path,
                model_name=model,
                effort=effort,
                skip_filled=self.skip_exist_var.get(),
                use_thumbnail=use_thumb,
                allow_url=allow_url,
                log_func=self.append_log,
            )

            self.append_log(
                f"[DONE] ìš”ì²­ JSONL ìƒì„±: total_rows={info['total_rows']}, "
                f"target_rows={info['target_rows']}, num_requests={info['num_requests']}"
            )

            # 2) ë°°ì¹˜ íŒŒì¼ í¬ê¸° í™•ì¸ ë° ë¶„í•  ì²˜ë¦¬
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ìš”ì²­ ìˆ˜: {info['num_requests']}ê°œ")
            
            # 190MB ì´ìƒì´ê±°ë‚˜ ìš”ì²­ì´ 500ê°œ ì´ìƒì´ë©´ ë¶„í•  ì²˜ë¦¬ (OpenAI Batch API ì œí•œ: 200MB)
            MAX_FILE_SIZE_MB = 190
            MAX_REQUESTS_PER_BATCH = 500
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB or info['num_requests'] > MAX_REQUESTS_PER_BATCH:
                reason = []
                if jsonl_size_mb > MAX_FILE_SIZE_MB:
                    reason.append(f"íŒŒì¼ í¬ê¸° ({jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB)")
                if info['num_requests'] > MAX_REQUESTS_PER_BATCH:
                    reason.append(f"ìš”ì²­ ìˆ˜ ({info['num_requests']}ê°œ > {MAX_REQUESTS_PER_BATCH}ê°œ)")
                self.append_log(f"[INFO] {' ë° '.join(reason)}ë¡œ ì¸í•´ ë¶„í•  ì²˜ë¦¬í•©ë‹ˆë‹¤... (OpenAI ì œí•œ: 200MB)")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client_with_timeout = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch_ids = self._create_batch_chunks(
                    client=client_with_timeout,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model,
                    effort=effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=MAX_REQUESTS_PER_BATCH,
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}")
            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch = create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model,
                    log_func=self.append_log,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # 3) ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src,
                    jsonl_path=jsonl_path,
                    model=model,
                    effort=effort,
                    status=batch.status,
                    output_file_id=None,
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage2(Text) ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡: T2 (ì§„í–‰ì¤‘) (img ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                try:
                    root_name = get_root_filename(src)
                    JobManager.update_status(root_name, text_msg="T2 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T2 (ì§„í–‰ì¤‘)")
                except Exception:
                    # ëŸ°ì²˜ë‚˜ job_history.json ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ìš©íˆ ë¬´ì‹œ
                    pass
                messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {batch_id}")
            
            self._load_jobs_all()
            self._load_archive_list()

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            messagebox.showerror("ì—ëŸ¬", str(e))
    
    def _create_batch_chunks(self, client, jsonl_path, excel_path, model_name, effort, max_size_mb=190, max_requests=500):
        """
        í° JSONL íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ batch_group_idë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        """
        import json
        import uuid
        
        # ë°°ì¹˜ ê·¸ë£¹ ID ìƒì„± (ê°™ì€ ì—‘ì…€ì—ì„œ ë¶„í• ëœ ë°°ì¹˜ë“¤ì„ ë¬¶ìŒ)
        batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
        
        # JSONL íŒŒì¼ ì½ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ê³ ë ¤, í•˜ì§€ë§Œ í˜„ì¬ëŠ” ì „ì²´ ë¡œë“œ)
        requests = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    requests.append(json.loads(line))
        
        total_requests = len(requests)
        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (íŒŒì¼ í¬ê¸°ì™€ ìš”ì²­ ìˆ˜ ëª¨ë‘ ê³ ë ¤)
        original_file_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
        estimated_chunks_by_size = max(1, int(original_file_size_mb / max_size_mb) + 1)
        estimated_chunks_by_count = (total_requests + max_requests - 1) // max_requests
        estimated_total_chunks = max(estimated_chunks_by_size, estimated_chunks_by_count)
        
        self.append_log(f"[INFO] ì´ {total_requests}ê°œ ìš”ì²­ì„ ë¶„í• í•©ë‹ˆë‹¤... (ì˜ˆìƒ: ì•½ {estimated_total_chunks}ê°œ ì²­í¬, ê·¸ë£¹ ID: {batch_group_id})")
        
        batch_ids = []
        chunk_num = 0
        chunk_files_created = []  # ìƒì„±ëœ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì •ë¦¬ìš©)
        
        i = 0
        while i < total_requests:
            chunk_num += 1
            chunk_requests = []
            chunk_size_bytes = 0  # ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì •í™•íˆ ê³„ì‚°
            
            # ì²­í¬ ìƒì„± (í¬ê¸° ë˜ëŠ” ê°œìˆ˜ ì œí•œ)
            # ì‹¤ì œ íŒŒì¼ í¬ê¸°ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ JSON ì§ë ¬í™” + ì¤„ë°”ê¿ˆ ë¬¸ì ê³ ë ¤
            while i < total_requests and len(chunk_requests) < max_requests:
                req_json = json.dumps(requests[i], ensure_ascii=False)
                req_size_bytes = len(req_json.encode('utf-8')) + 1  # +1 for newline
                
                # ë‹¤ìŒ ìš”ì²­ì„ ì¶”ê°€í•˜ë©´ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ ë§ˆì§„ 5% í¬í•¨)
                if chunk_size_bytes + req_size_bytes > max_size_mb * 1024 * 1024 * 0.95:
                    break
                
                chunk_requests.append(requests[i])
                chunk_size_bytes += req_size_bytes
                i += 1
            
            if not chunk_requests:
                break
            
            # ì²­í¬ JSONL íŒŒì¼ ìƒì„±
            base, ext = os.path.splitext(jsonl_path)
            chunk_jsonl_path = f"{base}_chunk{chunk_num:03d}{ext}"
            chunk_files_created.append(chunk_jsonl_path)
            
            with open(chunk_jsonl_path, "w", encoding="utf-8") as f:
                for req in chunk_requests:
                    f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            chunk_size_mb = os.path.getsize(chunk_jsonl_path) / (1024 * 1024)
            # ì‹¤ì œ ìƒì„±ëœ ì²­í¬ ìˆ˜ë¡œ í‘œì‹œ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë  ìˆ˜ ìˆìŒ)
            self.append_log(f"[INFO] ì²­í¬ {chunk_num}: {len(chunk_requests)}ê°œ ìš”ì²­, {chunk_size_mb:.2f} MB")
            
            # ë°°ì¹˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 3
            retry_count = 0
            batch_created = False
            
            while retry_count < max_retries and not batch_created:
                try:
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹œë„ ì¤‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                    batch = create_batch_from_jsonl(
                        client=client,
                        jsonl_path=chunk_jsonl_path,
                        excel_path=excel_path,
                        model_name=model_name,
                        log_func=self.append_log,
                    )
                    
                    batch_id = batch.id
                    batch_ids.append(batch_id)
                    self.append_log(f"âœ… ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_id}")
                    batch_created = True
                    
                    # ì‘ì—… ì´ë ¥ ê¸°ë¡ (ê·¸ë£¹ ì •ë³´ í¬í•¨)
                    # total_chunksëŠ” ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë˜ë¯€ë¡œ ì¼ë‹¨ chunk_numìœ¼ë¡œ ì„¤ì •
                    upsert_batch_job(
                        batch_id=batch_id,
                        src_excel=excel_path,
                        jsonl_path=chunk_jsonl_path,
                        model=model_name,
                        effort=effort,
                        status=batch.status,
                        output_file_id=None,
                        batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                        chunk_index=chunk_num,  # ì²­í¬ ë²ˆí˜¸
                        total_chunks=chunk_num,  # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì²­í¬ ìˆ˜ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨)
                    )
                except Exception as e:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                        self.append_log(f"âš ï¸ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                        self.append_log(f"[INFO] {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        import time
                        time.sleep(wait_time)
                    else:
                        self.append_log(f"âŒ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}")
                        import traceback
                        self.append_log(traceback.format_exc())
                        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìœ ì§€ (ìˆ˜ë™ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                        # í•˜ì§€ë§Œ ë°°ì¹˜ IDëŠ” ì¶”ê°€ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ batch_idsì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ
        
        # ëª¨ë“  ì²­í¬ ìƒì„± ì™„ë£Œ í›„, total_chunksë¥¼ ì‹¤ì œ ìƒì„±ëœ ë°°ì¹˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        actual_total_chunks = len(batch_ids)
        if actual_total_chunks > 0:
            self.append_log(f"[INFO] ì´ {actual_total_chunks}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ. ì‘ì—… ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
            # ê²½ìŸ ì¡°ê±´ ë°©ì§€ë¥¼ ìœ„í•´ ì›ìì  ì—…ë°ì´íŠ¸
            jobs = load_batch_jobs()
            updated_count = 0
            for j in jobs:
                if j.get("batch_group_id") == batch_group_id:
                    j["total_chunks"] = actual_total_chunks
                    updated_count += 1
            if updated_count > 0:
                save_batch_jobs(jobs)
                self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {actual_total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.append_log(f"âš ï¸ ìƒì„±ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì„±ê³µí•œ ë°°ì¹˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²­í¬ íŒŒì¼ ì •ë¦¬ ì˜µì…˜ ì œê³µ (í˜„ì¬ëŠ” ìœ ì§€)
        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìˆ˜ë™ ì¬ì‹œë„ë¥¼ ìœ„í•´ ìœ ì§€
        if actual_total_chunks < chunk_num:
            failed_chunks = chunk_num - actual_total_chunks
            self.append_log(f"âš ï¸ {failed_chunks}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²­í¬ íŒŒì¼ì€ ìœ ì§€ë©ë‹ˆë‹¤.")
        
        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
        try:
            root_name = get_root_filename(excel_path)
            JobManager.update_status(root_name, text_msg="T2 (ì§„í–‰ì¤‘)")
            self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T2 (ì§„í–‰ì¤‘)")
        except Exception:
            pass
        
        self._load_jobs_all()
        self._load_archive_list()
        
        return batch_ids

    # ----------------------------------------------------
    # Tab 2: Manage
    # ----------------------------------------------------
    def _init_tab_manage(self):
        container = ttk.Frame(self.tab_manage, padding=10)
        container.pack(fill='both', expand=True)
        sub_tabs = ttk.Notebook(container)
        sub_tabs.pack(fill='both', expand=True)
        
        self.sub_active = ttk.Frame(sub_tabs, padding=10)
        self.sub_archive = ttk.Frame(sub_tabs, padding=10)
        sub_tabs.add(self.sub_active, text=" â–¶ ì§„í–‰ì¤‘ / ì™„ë£Œ (Active) ")
        sub_tabs.add(self.sub_archive, text=" ğŸ—‘ íœ´ì§€í†µ (Archive) ")
        
        # Active UI
        f_ctrl = ttk.Frame(self.sub_active)
        f_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_ctrl, text="ğŸ”„ ì„ íƒ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ“¥ ì„ íƒ ì¼ê´„ ë³‘í•©", command=self._merge_selected, style="Primary.TButton").pack(side='left', padx=10)
        ttk.Button(f_ctrl, text="ğŸ“Š ì„ íƒ ì¼ê´„ ë¶„ì„ ë¦¬í¬íŠ¸", command=self._report_selected_unified, style="Success.TButton").pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ—‘ íœ´ì§€í†µ ì´ë™", command=self._archive_selected, style="Danger.TButton").pack(side='right', padx=2)
        
        cols = ("batch_id", "status", "created", "completed", "model", "effort", "counts", "group")
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš© (íŠ¸ë¦¬ ì•„ì´ì½˜ + ì»¬ëŸ¼ í—¤ë”)
        self.tree_active = ttk.Treeview(self.sub_active, columns=cols, show='tree headings', height=15, selectmode='extended')
        self.tree_active.tag_configure('odd', background=COLOR_WHITE)
        self.tree_active.tag_configure('even', background='#F2F7FF')
        self.tree_active.tag_configure('group', background='#E8F5E9')  # ê·¸ë£¹ ë°°ì¹˜ ê°•ì¡°
        self.tree_active.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))  # ê·¸ë£¹ í—¤ë” ê°•ì¡°
        for c in cols: self.tree_active.heading(c, text=c.capitalize())
        self.tree_active.column("#0", width=250, anchor="w")  # íŠ¸ë¦¬ ì»¬ëŸ¼ (ê·¸ë£¹ëª… í‘œì‹œ)
        self.tree_active.column("batch_id", width=200)
        self.tree_active.column("effort", width=80, anchor="center")
        self.tree_active.column("group", width=120, anchor="center")
        self.tree_active.pack(fill='both', expand=True, padx=5, pady=5)
        
        self.menu_active = Menu(self, tearoff=0)
        self.menu_active.add_command(label="ìƒíƒœ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active))
        self.menu_active.add_command(label="ê²°ê³¼ ë³‘í•©", command=self._merge_selected)
        self.menu_active.add_command(label="ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±", command=self._report_selected_unified)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="íœ´ì§€í†µìœ¼ë¡œ ì´ë™", command=self._archive_selected)
        self.tree_active.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_active, self.menu_active))
        self.tree_active.bind("<Double-1>", self._on_tree_double_click)

        # Archive UI
        f_arch_ctrl = ttk.Frame(self.sub_archive)
        f_arch_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_arch_ctrl, text="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected, style="Primary.TButton").pack(side='left')
        ttk.Button(f_arch_ctrl, text="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected, style="Danger.TButton").pack(side='right')
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš©
        self.tree_arch = ttk.Treeview(self.sub_archive, columns=cols, show='tree headings', height=15, selectmode='extended')
        self.tree_arch.tag_configure('odd', background=COLOR_WHITE)
        self.tree_arch.tag_configure('even', background='#FFF2F2')
        self.tree_arch.tag_configure('group', background='#FFE8E8')  # ê·¸ë£¹ ë°°ì¹˜ ê°•ì¡°
        self.tree_arch.tag_configure('group_header', background='#FFCDD2', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))  # ê·¸ë£¹ í—¤ë” ê°•ì¡°
        for c in cols: self.tree_arch.heading(c, text=c.capitalize())
        self.tree_arch.column("#0", width=250, anchor="w")  # íŠ¸ë¦¬ ì»¬ëŸ¼ (ê·¸ë£¹ëª… í‘œì‹œ)
        self.tree_arch.column("batch_id", width=200)
        self.tree_arch.column("effort", width=80, anchor="center")
        self.tree_arch.column("group", width=120, anchor="center")
        self.tree_arch.pack(fill='both', expand=True)
        
        self._load_jobs_all()
        self._load_archive_list()

    def _show_context_menu(self, event, tree, menu):
        item = tree.identify_row(event.y)
        if item:
            if item not in tree.selection(): tree.selection_set(item)
            menu.post(event.x_root, event.y_root)

    def _get_selected_ids(self, tree):
        selection = tree.selection()
        ids = []
        for item in selection:
            vals = tree.item(item)['values']
            if vals: ids.append(vals[0])
        return ids

    def _load_jobs_all(self):
        if not hasattr(self, 'tree_active'): return
        for i in self.tree_active.get_children(): self.tree_active.delete(i)
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ì„ ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ)
            group_header_text = f"ğŸ“¦ ê·¸ë£¹ {group_id[:8]}... ({total_chunks}ê°œ ë°°ì¹˜) - {status_summary}"
            group_node = self.tree_active.insert("", "end", 
                text=group_header_text,
                values=("", "", "", "", "", "", "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                tag = 'group'
                self.tree_active.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            tag = 'even'
            self.tree_active.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))

    def _load_archive_list(self):
        if not hasattr(self, 'tree_arch'): return
        for i in self.tree_arch.get_children(): self.tree_arch.delete(i)
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if not j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs)) if group_jobs else len(group_jobs)
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ)
            group_header_text = f"ğŸ“¦ ê·¸ë£¹ {group_id[:8]}... ({total_chunks}ê°œ ë°°ì¹˜) - {status_summary}"
            group_node = self.tree_arch.insert("", "end", 
                text=group_header_text,
                values=("", "", "", "", "", "", "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                tag = 'group'
                self.tree_arch.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            tag = 'even'
            self.tree_arch.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))

    # --- Actions ---
    def _refresh_selected(self, tree):
        ids = self._get_selected_ids(tree)
        if not ids: return
        jobs = load_batch_jobs()
        completed = [bid for bid in ids if next((x for x in jobs if x["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        if completed:
            if messagebox.askyesno("í™•ì¸", f"{len(completed)}ê±´ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\në¯¸ì™„ë£Œ ê±´ë§Œ ê°±ì‹ í• ê¹Œìš”?"):
                ids = [i for i in ids if i not in completed]
        if not ids: return
        t = threading.Thread(target=self._run_refresh_ids, args=(ids,))
        t.daemon = True
        t.start()

    def _run_refresh_ids(self, ids):
        key = self.api_key_var.get().strip()
        client = OpenAI(api_key=key)
        self.append_log(f"ì„ íƒëœ {len(ids)}ê±´ ê°±ì‹  ì¤‘...")
        for bid in ids:
            try:
                remote = client.batches.retrieve(bid)
                rc = None
                if remote.request_counts:
                    rc = {"total": remote.request_counts.total, "completed": remote.request_counts.completed, "failed": remote.request_counts.failed}
                upsert_batch_job(bid, status=remote.status, output_file_id=remote.output_file_id, request_counts=rc)
            except Exception as e:
                self.append_log(f"{bid} ê°±ì‹  ì‹¤íŒ¨: {e}")
        self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])
        self.append_log("ê°±ì‹  ì™„ë£Œ")

    def _merge_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        
        # ì„ íƒëœ ë°°ì¹˜ë“¤ì˜ ê·¸ë£¹ ì •ë³´ í™•ì¸
        selected_jobs = [j for j in jobs if j["batch_id"] in ids]
        group_ids = set()
        for j in selected_jobs:
            group_id = j.get("batch_group_id")
            if group_id:
                group_ids.add(group_id)
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        all_target_ids = set(ids)
        if group_ids:
            for group_id in group_ids:
                group_batches = [j for j in jobs if j.get("batch_group_id") == group_id and j.get("status") == "completed"]
                for j in group_batches:
                    all_target_ids.add(j["batch_id"])
            
            if len(all_target_ids) > len(ids):
                group_info = f"\n\nê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ {len(all_target_ids) - len(ids)}ê°œê°€ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤."
            else:
                group_info = ""
        else:
            group_info = ""
        
        targets = [bid for bid in all_target_ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "completed"]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ë³‘í•©í•  ìˆ˜ ìˆëŠ” 'completed' ìƒíƒœì˜ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if messagebox.askyesno("ë³‘í•©", f"ì´ {len(targets)}ê±´ì„ ë³‘í•©í•˜ì‹œê² ìŠµë‹ˆê¹Œ?{group_info}"):
            t = threading.Thread(target=self._run_merge_multi, args=(list(targets),))
            t.daemon = True
            t.start()

    def _run_merge_multi(self, ids):
        """
        ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ê¸°ì¡´ Stage2 Batch ì½”ì–´(download_batch_output_if_ready + merge_batch_output_to_excel)ë¥¼
        ì´ìš©í•´ ì„ íƒëœ Batch ë“¤ì— ëŒ€í•´ ê²°ê³¼ JSONL ë‹¤ìš´ë¡œë“œ + ì—‘ì…€ ë³‘í•©ì„ ìˆ˜í–‰.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ í•˜ë‚˜ì˜ ì—‘ì…€ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.
        """
        key = self.api_key_var.get().strip()
        client = OpenAI(api_key=key)
        success_cnt = 0
        total_cost = 0.0
        
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ë°°ì¹˜ ë¶„ë¥˜
        groups_to_merge = {}  # {group_id: [batch_ids]}
        ungrouped_batches = []  # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤
        
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not job:
                continue
            
            group_id = job.get("batch_group_id")
            if group_id:
                if group_id not in groups_to_merge:
                    groups_to_merge[group_id] = []
                groups_to_merge[group_id].append(bid)
            else:
                ungrouped_batches.append(bid)
        
        # ê·¸ë£¹ë³„ ë³‘í•© ì²˜ë¦¬
        for group_id, batch_ids in groups_to_merge.items():
            self.append_log(f"--- ê·¸ë£¹ ë³‘í•© ì‹œì‘: {group_id} ({len(batch_ids)}ê°œ ë°°ì¹˜) ---")
            try:
                # ê·¸ë£¹ ë‚´ ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì›ë³¸ ì—‘ì…€ ê²½ë¡œ ì‚¬ìš©
                first_job = next((j for j in jobs if j["batch_id"] == batch_ids[0]), None)
                if not first_job:
                    continue
                
                src_path = first_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ì„ì‹œ JSONLì— ìˆ˜ì§‘
                all_output_lines = []
                model_name = first_job.get("model", "gpt-5-mini")
                total_group_cost = 0.0
                
                # ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ (chunk_indexê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ)
                def get_chunk_index(bid):
                    job = next((j for j in jobs if j["batch_id"] == bid), None)
                    if job:
                        idx = job.get("chunk_index")
                        return idx if idx is not None else 999999  # chunk_indexê°€ ì—†ìœ¼ë©´ ë§¨ ë’¤ë¡œ
                    return 999999
                
                batch_ids_sorted = sorted(batch_ids, key=get_chunk_index)
                
                for bid in batch_ids_sorted:
                    self.append_log(f"  [ê·¸ë£¹] ë°°ì¹˜ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    try:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if not local_job:
                            continue
                        
                        # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                        if local_job.get("status") == "merged":
                            self.append_log(f"  â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        base_dir = os.path.dirname(src_path)
                        base_name, _ = os.path.splitext(os.path.basename(src_path))
                        out_jsonl = os.path.join(base_dir, f"{base_name}_stage2_batch_output_{bid}.jsonl")
                        
                        if download_batch_output_if_ready is None:
                            raise RuntimeError("stage2_batch_api_ê¸°ì¡´gpt ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ Batch ë³‘í•© ì½”ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # ë°°ì¹˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                        ok, status = download_batch_output_if_ready(
                            client=client,
                            batch_id=bid,
                            output_jsonl_path=out_jsonl,
                            log_func=self.append_log,
                        )
                        
                        upsert_batch_job(
                            batch_id=bid,
                            status=status,
                            output_jsonl=out_jsonl if ok else local_job.get("output_jsonl", ""),
                        )
                        
                        if not ok or status != "completed":
                            self.append_log(f"  âš ï¸ {bid}: ì•„ì§ completed ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤. (status={status})")
                            continue
                        
                        # JSONL íŒŒì¼ ì½ì–´ì„œ ìˆ˜ì§‘
                        if os.path.exists(out_jsonl):
                            with open(out_jsonl, "r", encoding="utf-8") as f:
                                for line in f:
                                    line = line.strip()
                                    if line:
                                        all_output_lines.append(line)
                        
                    except Exception as e:
                        self.append_log(f"  âŒ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if not all_output_lines:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ë³‘í•©í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ì˜ ì „ì²´ ì²­í¬ ìˆ˜ í™•ì¸ ë° ê²€ì¦
                expected_total_chunks = first_job.get("total_chunks")
                if expected_total_chunks:
                    # ì‹¤ì œë¡œ ê²°ê³¼ë¥¼ ë‹¤ìš´ë¡œë“œí•œ ë°°ì¹˜ ìˆ˜ ê³„ì‚°
                    downloaded_batch_ids = []
                    for bid in batch_ids_sorted:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if local_job and local_job.get("status") == "completed":
                            out_jsonl = local_job.get("output_jsonl") or os.path.join(
                                os.path.dirname(src_path),
                                f"{os.path.splitext(os.path.basename(src_path))[0]}_stage2_batch_output_{bid}.jsonl"
                            )
                            if os.path.exists(out_jsonl):
                                downloaded_batch_ids.append(bid)
                    
                    if len(downloaded_batch_ids) < expected_total_chunks:
                        missing = expected_total_chunks - len(downloaded_batch_ids)
                        self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì˜ˆìƒ {expected_total_chunks}ê°œ ì¤‘ {len(downloaded_batch_ids)}ê°œë§Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({missing}ê°œ ëˆ„ë½ ê°€ëŠ¥)")
                
                # ì„ì‹œ í†µí•© JSONL íŒŒì¼ ìƒì„±
                base_dir = os.path.dirname(src_path)
                base_name, _ = os.path.splitext(os.path.basename(src_path))
                merged_jsonl = os.path.join(base_dir, f"{base_name}_stage2_batch_output_merged_{group_id}.jsonl")
                
                with open(merged_jsonl, "w", encoding="utf-8") as f:
                    for line in all_output_lines:
                        f.write(line + "\n")
                
                self.append_log(f"  [ê·¸ë£¹] í†µí•© JSONL ìƒì„±: {len(all_output_lines)}ê°œ ê²°ê³¼")
                
                # í†µí•© JSONLì„ ì—‘ì…€ì— ë³‘í•©
                if merge_batch_output_to_excel is None:
                    raise RuntimeError("stage2_batch_api_ê¸°ì¡´gpt ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ Batch ë³‘í•© ì½”ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                info = merge_batch_output_to_excel(
                    excel_path=src_path,
                    output_jsonl_path=merged_jsonl,
                    model_name=model_name,
                    skip_filled=self.skip_exist_var.get(),
                    log_func=self.append_log,
                )
                
                total_group_cost += info.get("total_cost_usd") or 0.0
                total_cost += total_group_cost
                
                # Stage2 ìµœì¢… íŒŒì¼ëª…: *_T2_... í˜•ì‹ìœ¼ë¡œ ë²„ì „ ì—…
                core_out_path = info["out_excel_path"]
                final_out_path = None
                try:
                    df_done = pd.read_excel(core_out_path)
                    
                    # ST2_JSONì´ ìˆëŠ” í–‰ê³¼ ì—†ëŠ” í–‰ ë¶„ë¦¬
                    if "ST2_JSON" in df_done.columns:
                        # ST2_JSONì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì¸ í–‰ ì°¾ê¸°
                        df_with_st2 = df_done[df_done["ST2_JSON"].notna() & (df_done["ST2_JSON"] != '')].copy()
                        df_no_st2 = df_done[df_done["ST2_JSON"].isna() | (df_done["ST2_JSON"] == '')].copy()
                    else:
                        # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  í–‰ì´ ST2_JSON ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
                        df_with_st2 = pd.DataFrame()
                        df_no_st2 = df_done.copy()
                    
                    # ST2_JSONì´ ì—†ëŠ” í–‰ë“¤ì„ T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³„ë„ íŒŒì¼ ì €ì¥
                    no_st2_path = None
                    if len(df_no_st2) > 0:
                        base_dir = os.path.dirname(src_path)
                        base_name, ext = os.path.splitext(os.path.basename(src_path))
                        
                        # í˜„ì¬ íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: _T1_I0)
                        # T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³€ê²½
                        name_only_clean = re.sub(r"\([^)]*\)", "", base_name)  # ê¸°ì¡´ ê´„í˜¸ ì œê±°
                        all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
                        
                        if all_matches:
                            # ë§ˆì§€ë§‰ ë²„ì „ íŒ¨í„´ ì‚¬ìš©
                            match = all_matches[-1]
                            original_name = name_only_clean[: match.start()].rstrip("_")
                            current_i = int(match.group(4))
                            # T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ìƒì„±
                            new_filename = f"{original_name}_T2-2(ì‹¤íŒ¨)_I{current_i}{ext}"
                        else:
                            # ë²„ì „ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ T2-2(ì‹¤íŒ¨)_I0ë¡œ ìƒì„±
                            new_filename = f"{base_name}_T2-2(ì‹¤íŒ¨)_I0{ext}"
                        
                        no_st2_path = os.path.join(base_dir, new_filename)
                        df_no_st2.to_excel(no_st2_path, index=False)
                        
                        self.append_log(f"  [ê·¸ë£¹] T2-2(ì‹¤íŒ¨) ë¶„ë¦¬ íŒŒì¼: {os.path.basename(no_st2_path)} ({len(df_no_st2)}ê°œ í–‰)")
                        self.append_log(f"         â€» ì´ íŒŒì¼ì€ T2-1 ë‹¨ê³„ê¹Œì§€ë§Œ ì‘ì—… ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        
                        # ë¶„ë¦¬ëœ íŒŒì¼ì˜ ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                        try:
                            no_st2_root_name = get_root_filename(no_st2_path)
                            JobManager.update_status(no_st2_root_name, text_msg="T2-2(ì‹¤íŒ¨)")
                            self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸: {no_st2_root_name} -> T2-2(ì‹¤íŒ¨)")
                        except Exception as e:
                            self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                    
                    # ST2_JSONì´ ìˆëŠ” í–‰ë“¤ë§Œ ì €ì¥
                    if len(df_with_st2) > 0:
                        df_done = df_with_st2
                    else:
                        self.append_log(f"  âš ï¸ ê·¸ë£¹ {group_id}: ST2_JSONì´ ìˆëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                    
                    final_out_path = get_next_version_path(src_path, task_type="text")
                    
                    if safe_save_excel(df_done, final_out_path):
                        info["out_excel_path"] = final_out_path
                        # T2 ë²„ì „ íŒŒì¼ ì €ì¥ ì„±ê³µ ì‹œ, ì½”ì–´ê°€ ìƒì„±í•œ ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
                        if core_out_path != final_out_path and os.path.exists(core_out_path):
                            try:
                                os.remove(core_out_path)
                                self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(core_out_path)}")
                            except Exception as e:
                                self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                    else:
                        final_out_path = core_out_path
                except Exception as e:
                    final_out_path = core_out_path
                    self.append_log(f"[WARN] T2 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ë¥¼ merged ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                for bid in batch_ids:
                    upsert_batch_job(
                        batch_id=bid,
                        out_excel=final_out_path,
                        status="merged",
                    )
                
                # ì‹¤í–‰ ì´ë ¥ ê¸°ë¡
                try:
                    if first_job:
                        c_at_str = first_job.get("created_at", "")
                        if c_at_str:
                            c_at = datetime.fromisoformat(c_at_str)
                        else:
                            c_at = datetime.now()
                        finish_dt = datetime.now()
                        elapsed = (finish_dt - c_at).total_seconds()
                        
                        append_run_history(
                            stage="Stage 2 Batch (Grouped)",
                            model_name=model_name,
                            reasoning_effort=first_job.get("effort", "medium"),
                            src_file=src_path,
                            out_file=final_out_path,
                            total_rows=info["total_rows"],
                            api_rows=info["merged"],
                            elapsed_seconds=elapsed,
                            total_in_tok=info["total_in_tok"],
                            total_out_tok=info["total_out_tok"],
                            total_reasoning_tok=info["total_reasoning_tok"],
                            input_cost_usd=info["input_cost_usd"],
                            output_cost_usd=info["output_cost_usd"],
                            total_cost_usd=total_group_cost,
                            start_dt=c_at,
                            finish_dt=finish_dt,
                            api_type="batch",
                            batch_id=f"{group_id} ({len(batch_ids)} batches)",
                            success_rows=info["merged"],
                            fail_rows=info["missing"],
                        )
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨: {e}")
                
                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
                try:
                    root_name = get_root_filename(src_path)
                    JobManager.update_status(root_name, text_msg="T2-2(ë¶„ì„ì™„ë£Œ)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T2-2(ë¶„ì„ì™„ë£Œ)")
                except Exception as e:
                    self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")
                
                self.append_log(f"âœ… ê·¸ë£¹ ë³‘í•© ì™„ë£Œ: {os.path.basename(final_out_path)} ({len(batch_ids)}ê°œ ë°°ì¹˜)")
                success_cnt += 1
                
            except Exception as e:
                self.append_log(f"âŒ ê·¸ë£¹ {group_id} ë³‘í•© ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ ê°œë³„ ë³‘í•© (ê¸°ì¡´ ë¡œì§)
        for bid in ungrouped_batches:
            self.append_log(f"--- ë³‘í•© ì‹œì‘: {bid} ---")
            try:
                jobs = load_batch_jobs()
                local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                if not local_job:
                    self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì‘ì—… ì´ë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë³‘í•© ë°©ì§€)
                if local_job.get("status") == "merged":
                    self.append_log(f"â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                src_path = local_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ {bid}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                base_dir = os.path.dirname(src_path)
                base_name, _ = os.path.splitext(os.path.basename(src_path))
                out_jsonl = os.path.join(base_dir, f"{base_name}_stage2_batch_output.jsonl")

                if download_batch_output_if_ready is None or merge_batch_output_to_excel is None:
                    raise RuntimeError("stage2_batch_api_ê¸°ì¡´gpt ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ Batch ë³‘í•© ì½”ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                # 1) Batch ê²°ê³¼ JSONL ë‹¤ìš´ë¡œë“œ
                ok, status = download_batch_output_if_ready(
                    client=client,
                    batch_id=bid,
                    output_jsonl_path=out_jsonl,
                    log_func=self.append_log,
                )

                upsert_batch_job(
                    batch_id=bid,
                    status=status,
                    output_jsonl=out_jsonl if ok else local_job.get("output_jsonl", ""),
                )

                if not ok or status != "completed":
                    self.append_log(f"âš ï¸ {bid}: ì•„ì§ completed ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤. (status={status})")
                    continue

                # 2) JSONL â†’ ì—‘ì…€ ë³‘í•© + ë¹„ìš©/í† í° ê³„ì‚° (ê¸°ì¡´ ì½”ì–´ ì‚¬ìš©)
                model_name = local_job.get("model", "gpt-5-mini")
                info = merge_batch_output_to_excel(
                    excel_path=src_path,
                    output_jsonl_path=out_jsonl,
                    model_name=model_name,
                    skip_filled=self.skip_exist_var.get(),
                    log_func=self.append_log,
                )

                total_cost += info.get("total_cost_usd") or 0.0

                # 3) Stage2 ìµœì¢… íŒŒì¼ëª…: *_T2_... í˜•ì‹ìœ¼ë¡œ ë²„ì „ ì—…
                core_out_path = info["out_excel_path"]
                final_out_path = None
                try:
                    # ì½”ì–´ê°€ ë§Œë“  ì™„ë£Œ íŒŒì¼ì„ ë‹¤ì‹œ ì½ì–´ì™€ì„œ T2 ë²„ì „ íŒŒì¼ë¡œ ì €ì¥
                    df_done = pd.read_excel(core_out_path)
                    
                    # ST2_JSONì´ ìˆëŠ” í–‰ê³¼ ì—†ëŠ” í–‰ ë¶„ë¦¬
                    if "ST2_JSON" in df_done.columns:
                        # ST2_JSONì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì¸ í–‰ ì°¾ê¸°
                        df_with_st2 = df_done[df_done["ST2_JSON"].notna() & (df_done["ST2_JSON"] != '')].copy()
                        df_no_st2 = df_done[df_done["ST2_JSON"].isna() | (df_done["ST2_JSON"] == '')].copy()
                    else:
                        # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  í–‰ì´ ST2_JSON ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
                        df_with_st2 = pd.DataFrame()
                        df_no_st2 = df_done.copy()
                    
                    # ST2_JSONì´ ì—†ëŠ” í–‰ë“¤ì„ T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³„ë„ íŒŒì¼ ì €ì¥
                    no_st2_path = None
                    if len(df_no_st2) > 0:
                        base_dir = os.path.dirname(src_path)
                        base_name, ext = os.path.splitext(os.path.basename(src_path))
                        
                        # í˜„ì¬ íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: _T1_I0)
                        # T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³€ê²½
                        name_only_clean = re.sub(r"\([^)]*\)", "", base_name)  # ê¸°ì¡´ ê´„í˜¸ ì œê±°
                        all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
                        
                        if all_matches:
                            # ë§ˆì§€ë§‰ ë²„ì „ íŒ¨í„´ ì‚¬ìš©
                            match = all_matches[-1]
                            original_name = name_only_clean[: match.start()].rstrip("_")
                            current_i = int(match.group(4))
                            # T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ìƒì„±
                            new_filename = f"{original_name}_T2-2(ì‹¤íŒ¨)_I{current_i}{ext}"
                        else:
                            # ë²„ì „ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ T2-2(ì‹¤íŒ¨)_I0ë¡œ ìƒì„±
                            new_filename = f"{base_name}_T2-2(ì‹¤íŒ¨)_I0{ext}"
                        
                        no_st2_path = os.path.join(base_dir, new_filename)
                        df_no_st2.to_excel(no_st2_path, index=False)
                        
                        self.append_log(f"  T2-2(ì‹¤íŒ¨) ë¶„ë¦¬ íŒŒì¼: {os.path.basename(no_st2_path)} ({len(df_no_st2)}ê°œ í–‰)")
                        self.append_log(f"  â€» ì´ íŒŒì¼ì€ T2-1 ë‹¨ê³„ê¹Œì§€ë§Œ ì‘ì—… ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        
                        # ë¶„ë¦¬ëœ íŒŒì¼ì˜ ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                        try:
                            no_st2_root_name = get_root_filename(no_st2_path)
                            JobManager.update_status(no_st2_root_name, text_msg="T2-2(ì‹¤íŒ¨)")
                            self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸: {no_st2_root_name} -> T2-2(ì‹¤íŒ¨)")
                        except Exception as e:
                            self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                    
                    # ST2_JSONì´ ìˆëŠ” í–‰ë“¤ë§Œ ì €ì¥
                    if len(df_with_st2) > 0:
                        df_done = df_with_st2
                    else:
                        self.append_log(f"âš ï¸ {bid}: ST2_JSONì´ ìˆëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                    
                    final_out_path = get_next_version_path(src_path, task_type="text")

                    if safe_save_excel(df_done, final_out_path):
                        info["out_excel_path"] = final_out_path
                        # T2 ë²„ì „ íŒŒì¼ ì €ì¥ ì„±ê³µ ì‹œ, ì½”ì–´ê°€ ìƒì„±í•œ ì¤‘ê°„ íŒŒì¼(_stage2_batch_ì™„ë£Œ) ì‚­ì œ
                        if core_out_path != final_out_path and os.path.exists(core_out_path):
                            try:
                                os.remove(core_out_path)
                                self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(core_out_path)}")
                            except Exception as e:
                                self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                    else:
                        # ì €ì¥ ì‹¤íŒ¨ ì‹œ, ì½”ì–´ ì™„ë£Œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        final_out_path = core_out_path
                except Exception as e:
                    final_out_path = core_out_path
                    self.append_log(f"[WARN] T2 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

                upsert_batch_job(
                    batch_id=bid,
                    out_excel=final_out_path,
                    status="merged",
                )

                # ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ (naive datetime ê¸°ì¤€)
                try:
                    c_at_str = local_job.get("created_at", "")
                    if c_at_str:
                        c_at = datetime.fromisoformat(c_at_str)
                    else:
                        c_at = datetime.now()
                    finish_dt = datetime.now()
                    elapsed = (finish_dt - c_at).total_seconds()

                    append_run_history(
                        stage="Stage 2 Batch",
                        model_name=model_name,
                        reasoning_effort=local_job.get("effort", "medium"),
                        src_file=src_path,
                        out_file=info["out_excel_path"],
                        total_rows=info["total_rows"],
                        api_rows=info["merged"],
                        elapsed_seconds=elapsed,
                        total_in_tok=info["total_in_tok"],
                        total_out_tok=info["total_out_tok"],
                        total_reasoning_tok=info["total_reasoning_tok"],
                        input_cost_usd=info["input_cost_usd"],
                        output_cost_usd=info["output_cost_usd"],
                        total_cost_usd=info["total_cost_usd"],
                        start_dt=c_at,
                        finish_dt=finish_dt,
                        api_type="batch",
                        batch_id=bid,
                        success_rows=info["merged"],
                        fail_rows=info["missing"],
                    )
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨: {e}")

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage2(Text) ì™„ë£Œ ìƒíƒœ ê¸°ë¡: T2-2(ë¶„ì„ì™„ë£Œ) (img ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                try:
                    root_name = get_root_filename(src_path)
                    JobManager.update_status(root_name, text_msg="T2-2(ë¶„ì„ì™„ë£Œ)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T2-2(ë¶„ì„ì™„ë£Œ)")
                except Exception as e:
                    self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")

                self.append_log(f"âœ… ë³‘í•© ì™„ë£Œ: {os.path.basename(final_out_path)}")
                success_cnt += 1
            except Exception as e:
                self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: {e}")

        self.append_log(f"=== ì¼ê´„ ë³‘í•© ë (ì„±ê³µ: {success_cnt}, ì´ ë¹„ìš© ì¶”ì •: ${total_cost:.4f}) ===")
        self._load_jobs_all()
        messagebox.showinfo("ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©(ì¶”ì •): ${total_cost:.4f}")

    def _report_selected_unified(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        targets = [bid for bid in ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "merged"]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ìƒíƒœê°€ 'merged'ì¸ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if messagebox.askyesno("ë¦¬í¬íŠ¸", f"ì„ íƒí•œ {len(targets)}ê±´ì˜ JSON ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_report_unified, args=(targets,))
            t.daemon = True
            t.start()

    def _run_report_unified(self, ids):
        self.append_log(f"--- JSON ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ({len(ids)}ê±´) ---")
        jobs = load_batch_jobs()
        all_reps = []
        for bid in ids:
            local_job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not local_job: continue
            out_path = local_job.get("out_excel")
            if not out_path or not os.path.exists(out_path):
                self.append_log(f"âŒ íŒŒì¼ ëˆ„ë½: {bid}")
                continue
            
            try:
                df = pd.read_excel(out_path)
                if "ST2_JSON" not in df.columns: continue
                for idx, row in df.iterrows():
                    st2 = safe_str(row.get("ST2_JSON", ""))
                    parsed = "âŒ ì‹¤íŒ¨"
                    kw_cnt = 0
                    if st2.strip().startswith("{"):
                        try:
                            js = json.loads(st2)
                            kw_cnt = len(js.get("search_keywords", []))
                            parsed = "âœ… ì„±ê³µ"
                        except: pass
                    
                    all_reps.append({
                        "Batch_ID": bid,
                        "í–‰ë²ˆí˜¸": idx+2,
                        "ìƒí’ˆì½”ë“œ": safe_str(row.get("ìƒí’ˆì½”ë“œ", "")),
                        "JSONìƒíƒœ": parsed,
                        "í‚¤ì›Œë“œìˆ˜": kw_cnt
                    })
            except: pass

        if not all_reps:
            messagebox.showinfo("ì•Œë¦¼", "ë°ì´í„° ì—†ìŒ")
            return

        try:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            path = os.path.join(os.path.dirname(__file__), f"Stage2_Analysis_Report_{ts}.xlsx")
            pd.DataFrame(all_reps).to_excel(path, index=False)
            self.append_log(f"ğŸ“Š ë¦¬í¬íŠ¸ ì™„ë£Œ: {os.path.basename(path)}")
            if messagebox.askyesno("ì™„ë£Œ", "íŒŒì¼ì„ ì—¬ì‹œê² ìŠµë‹ˆê¹Œ?"): os.startfile(path)
        except Exception as e: messagebox.showerror("ì˜¤ë¥˜", str(e))

    def _archive_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if ids and messagebox.askyesno("ë³´ê´€", f"{len(ids)}ê±´ íœ´ì§€í†µ ì´ë™?"):
            archive_batch_job(ids, True)
            self._load_jobs_all()
            self._load_archive_list()

    def _restore_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ë³µêµ¬", f"{len(ids)}ê±´ ë³µêµ¬?"):
            archive_batch_job(ids, False)
            self._load_jobs_all()
            self._load_archive_list()

    def _hard_delete_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ì‚­ì œ", "ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            hard_delete_batch_job(ids)
            self._load_archive_list()

    def _on_tree_double_click(self, event):
        sel = self.tree_active.selection()
        if not sel: return
        bid = self.tree_active.item(sel[0])['values'][0]
        self.batch_id_var.set(bid)
        self.main_tabs.select(self.tab_merge)

    # ----------------------------------------------------
    # Tab 3: Manual
    # ----------------------------------------------------
    def _init_tab_merge(self):
        container = ttk.Frame(self.tab_merge, padding=20)
        container.pack(fill='both', expand=True)
        f_in = ttk.LabelFrame(container, text="ê°œë³„ ì‘ì—…", padding=15)
        f_in.pack(fill='x')
        ttk.Label(f_in, text="Batch ID:").pack(side='left')
        ttk.Entry(f_in, textvariable=self.batch_id_var, width=45, font=("Consolas", 10)).pack(side='left', padx=10)
        
        f_btn = ttk.Frame(container)
        f_btn.pack(fill='x', pady=20)
        ttk.Button(f_btn, text="1. ê²°ê³¼ ë³‘í•©", command=self._start_merge, style="Primary.TButton").pack(fill='x', pady=5)
        ttk.Button(f_btn, text="2. ë‹¨ì¼ ë¦¬í¬íŠ¸", command=self._start_diff_report).pack(fill='x', pady=5)

    def _start_merge(self):
        t = threading.Thread(target=self._run_merge)
        t.daemon = True
        t.start()

    def _run_merge(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_merge_multi([bid])

    def _start_diff_report(self):
        t = threading.Thread(target=self._run_diff_report)
        t.daemon = True
        t.start()

    def _run_diff_report(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_report_unified([bid])

if __name__ == "__main__":
    app = Stage2BatchGUI()
    app.mainloop()