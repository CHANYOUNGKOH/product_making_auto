"""
stage2_batch_api_Cachever.py

Stage 2 Batch API ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (GUI) - ìºì‹± ìµœì í™” ë²„ì „
- ê¸°ëŠ¥: ì—‘ì…€(ì´ë¯¸ì§€+í…ìŠ¤íŠ¸) -> Batch JSONL ìƒì„±(Vision API) -> ì—…ë¡œë“œ -> ì‹¤í–‰ -> ë³‘í•© -> JSON ë¶„ì„ ë¦¬í¬íŠ¸
- íŠ¹ì§•: GPT-5/4o ëª¨ë¸ ì§€ì›, ì´ë¯¸ì§€ ì˜µì…˜ ì²˜ë¦¬, ìƒì„¸ íˆ´íŒ ë° ê°€ì´ë“œ í¬í•¨
- ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™”: OpenAI Prompt Caching ê°€ì´ë“œì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì¬êµ¬ì„±
  * ì •ì  ì½˜í…ì¸ (ì—­í• , ì œì•½, ìŠ¤í‚¤ë§ˆ)ë¥¼ ì•ìª½ì— ë°°ì¹˜
  * ë™ì  ì½˜í…ì¸ (ì…ë ¥ ë°ì´í„°)ë¥¼ ë’¤ìª½ì— ë°°ì¹˜
  * prompt_cache_key ì‚¬ìš©ìœ¼ë¡œ ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ (í† í° ë¹„ìš© ìµœëŒ€ 90% ì ˆê° ê°€ëŠ¥)
"""

import os
import sys
import json
import threading
import subprocess
import re
from datetime import datetime

import pandas as pd
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, Menu
from tkinter.scrolledtext import ScrolledText

from openai import OpenAI


# ========================================================
# ë©”ì¸ ëŸ°ì²˜ ì—°ë™ìš© JobManager & íŒŒì¼ëª… ìœ í‹¸ (Stage2 ì „ìš©)
# ========================================================
def get_root_filename(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´(_T*_I* ë˜ëŠ” _t*_i*, _I*(ì—…ì™„) í¬í•¨) ë° ê¸°íƒ€ ê¼¬ë¦¬í‘œë¥¼ ë–¼ê³  ì›ë³¸ëª…(Key)ë§Œ ì¶”ì¶œ
    ì˜ˆ: ìƒí’ˆ_T0_I0.xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T2_I1.xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T1_I0(ì—…ì™„).xlsx -> ìƒí’ˆ.xlsx
    ì˜ˆ: ìƒí’ˆ_T1_I0_T2_I1.xlsx -> ìƒí’ˆ.xlsx (ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ ì œê±°)
    ì˜ˆ: ìƒí’ˆ_T1_I5(ì—…ì™„).xlsx -> ìƒí’ˆ.xlsx
    """
    name = os.path.basename(filename)
    base, ext = os.path.splitext(name)

    # 1. ë²„ì „ íŒ¨í„´ (_Tìˆ«ì_Iìˆ«ì(ê´„í˜¸)? ë˜ëŠ” _tìˆ«ì_iìˆ«ì(ê´„í˜¸)?) ë°˜ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    # íŒ¨í„´ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì œê±°, ê´„í˜¸ê°€ ë¶™ì€ ê²½ìš°ë„ í¬í•¨
    while True:
        new_base = re.sub(r"_[Tt]\d+_[Ii]\d+(\([^)]+\))?", "", base, flags=re.IGNORECASE)
        if new_base == base:
            break
        base = new_base
    
    # 2. ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„), (ì™„ë£Œ) ë“±) - ë²„ì „ íŒ¨í„´ì˜ ê´„í˜¸ëŠ” ì´ë¯¸ ì œê±°ë¨
    base = re.sub(r"\([^)]*\)", "", base)
    
    # 3. ê¸°íƒ€ êµ¬í˜• ê¼¬ë¦¬í‘œ ì œê±° (í˜¸í™˜ì„± ìœ ì§€)
    suffixes = ["_stage1_mapping", "_stage1_img_mapping", "_with_images"]
    for s in suffixes:
        base = base.replace(s, "")
    
    # 4. ëì— ë‚¨ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    base = base.rstrip("_")

    return base + ext

def get_excel_name_from_path(excel_path: str) -> str:
    """
    ì—‘ì…€ íŒŒì¼ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
    ì˜ˆ: C:/Users/.../ìƒí’ˆ_T1_I0.xlsx -> ìƒí’ˆ_T1_I0.xlsx
    """
    if not excel_path:
        return "-"
    return os.path.basename(excel_path)


def get_next_version_path(current_path: str, task_type: str = "text") -> str:
    """
    í˜„ì¬ íŒŒì¼ëª…ì„ ë¶„ì„í•´ì„œ ë‹¤ìŒ ë‹¨ê³„ì˜ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    íŒŒì¼ëª… í˜•ì‹: ì›ë³¸ëª…_T{ìˆ«ì}_I{ìˆ«ì}.xlsx ë˜ëŠ” ì›ë³¸ëª…_T{ìˆ«ì}_I{ìˆ«ì}(ê´„í˜¸).xlsx
    - task_type='text'  â†’ T ë²„ì „ +1 (Stage1: T1, Stage2: T2, ...)
    - task_type='image' â†’ I ë²„ì „ +1
    
    ì£¼ì˜: íŒŒì¼ëª…ì— ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ íŒ¨í„´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì˜ˆ: ìƒí’ˆ_T1_I5(ì—…ì™„).xlsx -> ìƒí’ˆ_T2_I5(ì—…ì™„).xlsx (text) ë˜ëŠ” ìƒí’ˆ_T1_I6(ì—…ì™„).xlsx (image)
    """
    dir_name = os.path.dirname(current_path)
    base_name = os.path.basename(current_path)
    name_only, ext = os.path.splitext(base_name)

    # ë§ˆì§€ë§‰ _T*_I*(ê´„í˜¸)? íŒ¨í„´ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ, ì—¬ëŸ¬ íŒ¨í„´ì´ ìˆì–´ë„ ë§ˆì§€ë§‰ ê²ƒë§Œ)
    # ê´„í˜¸ê°€ ë¶™ì€ ê²½ìš°ë„ ì¸ì‹ (ì˜ˆ: _I5(ì—…ì™„))
    all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)(\([^)]+\))?", name_only, re.IGNORECASE))
    
    if all_matches:
        # ë§ˆì§€ë§‰ ë§¤ì¹­ ì‚¬ìš©
        match = all_matches[-1]
        current_t = int(match.group(2))
        current_i = int(match.group(4))
        i_suffix = match.group(5) or ""  # ê´„í˜¸ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ìœ ì§€ (ì˜ˆ: (ì—…ì™„))
        # ì›ë³¸ëª…ì€ ë§ˆì§€ë§‰ íŒ¨í„´ ì´ì „ê¹Œì§€
        original_name = name_only[: match.start()].rstrip("_")
    else:
        # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì›ë³¸ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì œê±° í›„ ì‚¬ìš©
        original_name = name_only
        # ê¸°ì¡´ ë²„ì „ íŒ¨í„´ ì œê±° (ê´„í˜¸ í¬í•¨)
        while True:
            new_name = re.sub(r"_[Tt]\d+_[Ii]\d+(\([^)]+\))?", "", original_name, flags=re.IGNORECASE)
            if new_name == original_name:
                break
            original_name = new_name
        original_name = original_name.rstrip("_")
        current_t = 0
        current_i = 0
        i_suffix = ""

    if task_type == "text":
        new_t = current_t + 1
        new_i = current_i
    elif task_type == "image":
        new_t = current_t
        new_i = current_i + 1
    else:
        return current_path

    # ê´„í˜¸ ë¶€ë¶„ ìœ ì§€ (ì˜ˆ: (ì—…ì™„))
    new_filename = f"{original_name}_T{new_t}_I{new_i}{i_suffix}{ext}"
    return os.path.join(dir_name, new_filename)


class JobManager:
    DB_FILE = None

    @classmethod
    def find_db_path(cls):
        if cls.DB_FILE and os.path.exists(cls.DB_FILE):
            return cls.DB_FILE

        current_dir = os.path.dirname(os.path.abspath(__file__))
        search_dirs = [
            current_dir,
            os.path.abspath(os.path.join(current_dir, "..")),
            os.path.abspath(os.path.join(current_dir, "..", "..")),
        ]

        for d in search_dirs:
            target = os.path.join(d, "job_history.json")
            if os.path.exists(target):
                cls.DB_FILE = target
                print(f"[JobManager] DB Found: {target}")
                return target

        default_path = os.path.abspath(os.path.join(current_dir, "..", "job_history.json"))
        cls.DB_FILE = default_path
        return default_path

    @classmethod
    def load_jobs(cls):
        db_path = cls.find_db_path()
        if not os.path.exists(db_path):
            return {}
        try:
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    @classmethod
    def update_status(cls, filename, text_msg=None, img_msg=None):
        """ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ìƒíƒœ ì—…ë°ì´íŠ¸ (Stage1/Stage2 ê³µìš©)."""
        db_path = cls.find_db_path()
        data = cls.load_jobs()
        now = datetime.now().strftime("%m-%d %H:%M")

        if filename not in data:
            data[filename] = {
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "text_status": "ëŒ€ê¸°",
                "text_time": "-",
                "image_status": "ëŒ€ê¸°",
                "image_time": "-",
                "memo": "",
            }

        if text_msg:
            data[filename]["text_status"] = text_msg
            data[filename]["text_time"] = now
        if img_msg:
            data[filename]["image_status"] = img_msg
            data[filename]["image_time"] = now

        data[filename]["last_update"] = now

        try:
            with open(db_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"[JobManager Error] {e}")


def safe_save_excel(df: pd.DataFrame, path: str) -> bool:
    """ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆì–´ ì €ì¥ì´ ì•ˆ ë  ë•Œ ì¬ì‹œë„ë¥¼ ìœ ë„í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            df.to_excel(path, index=False)
            return True
        except PermissionError:
            if not messagebox.askretrycancel(
                "ì €ì¥ ì‹¤íŒ¨",
                f"ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤!\n[{os.path.basename(path)}]\n\n"
                "íŒŒì¼ì„ ë‹«ê³  'ë‹¤ì‹œ ì‹œë„'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
            ):
                return False
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False


# [í•„ìˆ˜ ì˜ì¡´ì„±] stage2_core_Cache.py / stage2_run_history.py
# ìºì‹± ìµœì í™” ë²„ì „ ê°•ì œ ì‚¬ìš© (stage2_core_Cache.py)
try:
    from stage2_core_Cache import (
        safe_str,
        build_stage2_request_from_row, # Row -> Request ê°ì²´(í”„ë¡¬í”„íŠ¸+ì´ë¯¸ì§€ê²½ë¡œ) ë³€í™˜ (ìºì‹± ìµœì í™”)
    )
    CACHE_MODE_CORE = True
except ImportError:
    # ìºì‹± ë²„ì „ì´ í•„ìˆ˜ì´ë¯€ë¡œ ì˜¤ë¥˜ ë°œìƒ
    raise ImportError(
        "stage2_core_Cache.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
        "ìºì‹± ìµœì í™” ë²„ì „ì„ ì‚¬ìš©í•˜ë ¤ë©´ stage2_core_Cache.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
    )

try:
    from stage2_run_history import append_run_history
except ImportError:
    def append_run_history(*args, **kwargs): pass

# ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ê¸°ì¡´ Stage2 Batch ì½”ì–´ ë¡œì§ ì¬ì‚¬ìš©
# ì£¼ì˜: Batch JSONL ìƒì„± ë¡œì§(create_stage2_batch_input_jsonl)ì€ ì´ íŒŒì¼(Cachever)ì—ì„œ ë³„ë„ë¡œ êµ¬í˜„í•˜ì—¬
#       í”„ë¡¬í”„íŠ¸ ìºì‹± êµ¬ì¡°(system/user ë¶„ë¦¬, meta JSON, prompt_cache_key ê³ ì •)ë¥¼ ê°•ì œí•©ë‹ˆë‹¤.
try:
    from stage2_batch_api_ê¸°ì¡´gpt import (
        create_batch_from_jsonl,
        download_batch_output_if_ready,
        merge_batch_output_to_excel,
        build_image_nodes_from_paths,
        CACHE_MODE as BATCH_CACHE_MODE,  # stage2_batch_api_ê¸°ì¡´gpt.pyì˜ CACHE_MODE ê°€ì ¸ì˜¤ê¸°
    )
    CACHE_MODE_BATCH = BATCH_CACHE_MODE
    if not CACHE_MODE_BATCH:
        # CACHE_MODEê°€ Falseì¸ ê²½ìš° ê²½ê³ 
        print("[WARN] stage2_batch_api_ê¸°ì¡´gpt.pyì˜ CACHE_MODEê°€ Falseì…ë‹ˆë‹¤. ìºì‹±ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
except ImportError:
    # êµ¬ë²„ì „ í™˜ê²½ì—ì„œëŠ” ì´ ëª¨ë“ˆì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ ê²½ìš°ì—ëŠ” ì•„ë˜ ìƒˆ ë¡œì§ë§Œ ì‚¬ìš©
    create_batch_from_jsonl = None  # type: ignore
    download_batch_output_if_ready = None  # type: ignore
    merge_batch_output_to_excel = None  # type: ignore
    build_image_nodes_from_paths = None  # type: ignore
    CACHE_MODE_BATCH = False
except AttributeError:
    # CACHE_MODEê°€ ì—†ëŠ” ê²½ìš° (êµ¬ë²„ì „)
    CACHE_MODE_BATCH = False
    print("[WARN] stage2_batch_api_ê¸°ì¡´gpt.pyì— CACHE_MODEê°€ ì—†ìŠµë‹ˆë‹¤. ìºì‹±ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# === ê¸°ë³¸ ì„¤ì • ===
API_KEY_FILE = ".openai_api_key_stage2_batch"
BATCH_JOBS_FILE = os.path.join(os.path.dirname(__file__), "stage2_batch_jobs.json")
DEFAULT_SETTINGS_FILE = os.path.join(os.path.dirname(__file__), ".stage2_batch_defaults.json")

# Stage 2ìš© Batch ëª¨ë¸/ê°€ê²© (stage2_core ì™€ ë™ì¼í•œ gpt-5 ê³„ì—´ë§Œ ì‚¬ìš©)
MODEL_PRICING_USD_PER_MTOK = {
    "gpt-5": {
        "input": 1.25,
        "output": 10.0,
    },
    "gpt-5-mini": {
        "input": 0.25,
        "output": 2.00,
    },
    "gpt-5-nano": {
        "input": 0.05,
        "output": 0.40,
    },
}

# --- UI ìƒ‰ìƒ íŒ”ë ˆíŠ¸ (Modern Blue) ---
COLOR_BG = "#F8F9FA"
COLOR_WHITE = "#FFFFFF"
COLOR_PRIMARY = "#4A90E2"
COLOR_PRIMARY_HOVER = "#357ABD"
COLOR_SUCCESS = "#28A745"
COLOR_SUCCESS_HOVER = "#218838"
COLOR_DANGER = "#DC3545"
COLOR_DANGER_HOVER = "#C82333"
COLOR_TEXT = "#333333"
COLOR_HEADER = "#E9ECEF"


# =========================================================
# ì—‘ì…€ â†’ Batch ìš”ì²­ JSONL ìƒì„± (Cachever ì „ìš© êµ¬í˜„)
#  - í”„ë¡¬í”„íŠ¸ ìºì‹± êµ¬ì¡°ë¥¼ ì´ íŒŒì¼ì—ì„œ ê°•ì œ:
#    * system / user ë©”ì‹œì§€ ë¶„ë¦¬
#    * metaë¥¼ JSONìœ¼ë¡œë§Œ ì „ë‹¬
#    * detail_images_block í…ìŠ¤íŠ¸ ì œê±° (ì´ë¯¸ì§€ëŠ” ì²¨ë¶€ë¡œë§Œ ì‚¬ìš©)
#    * prompt_cache_key ê³ ì •ê°’("stage2_v1") ì‚¬ìš©
# =========================================================

def create_stage2_batch_input_jsonl(
    excel_path: str,
    jsonl_path: str,
    model_name: str,
    effort: str,
    skip_filled: bool,
    use_thumbnail: bool,
    allow_url: bool,
    max_detail_images: int = 10,
    resize_mode: str = "A",
    log_func=None,
):
    """
    Stage2 ì—‘ì…€ì„ ì½ì–´ì„œ Batch APIìš© ìš”ì²­ JSONLì„ ë§Œë“ ë‹¤. (Cachever ì „ìš©)
    - ST2_JSON ì´ ë¹„ì–´ ìˆëŠ” í–‰ë§Œ ëŒ€ìƒìœ¼ë¡œ í•œë‹¤(skip_filled=Trueì¼ ë•Œ).
    - stage2_core_Cache.Stage2Request(system_prompt + user_prompt + image_paths)ë¥¼ ì´ìš©í•´ ìš”ì²­ì„ êµ¬ì„±.
    - custom_id ëŠ” "row-{index}-{resize_mode}" í˜•ì‹ (ê²°ê³¼ ë¹„êµë¥¼ ìœ„í•´ resize_mode í¬í•¨).
    - OpenAI Prompt Caching ê°€ì´ë“œì— ë§ê²Œ system/user ë¶„ë¦¬ ë° prompt_cache_key ê³ ì •.
    
    Args:
        resize_mode: ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ. "A"(ê¸°ë³¸/ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨), "B"(ê°€ë¡œ 512px), "C"(ê°€ë¡œ 448px)
    """

    if build_image_nodes_from_paths is None:
        raise RuntimeError("build_image_nodes_from_pathsë¥¼ stage2_batch_api_ê¸°ì¡´gpt ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def log(msg: str):
        if log_func:
            log_func(msg)
        else:
            print(msg)

    df = pd.read_excel(excel_path)

    # ST2_JSON ì»¬ëŸ¼ ì—†ìœ¼ë©´ ë§Œë“¤ì–´ ë‘”ë‹¤ (ë³‘í•© ì‹œ ì“°ì„)
    if "ST2_JSON" not in df.columns:
        df["ST2_JSON"] = ""

    # ST2_í”„ë¡¬í”„íŠ¸ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒì„± (ë””ë²„ê¹…ìš©)
    if "ST2_í”„ë¡¬í”„íŠ¸" not in df.columns:
        df["ST2_í”„ë¡¬í”„íŠ¸"] = ""

    # ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼ë“¤ ì •ë ¬ ë° ê°œìˆ˜ ì œí•œ
    detail_cols = [c for c in df.columns if str(c).startswith("ìƒì„¸ì´ë¯¸ì§€_")]
    if detail_cols:
        def sort_key(c):
            try:
                return int(str(c).split("_")[1])
            except Exception:
                return 9999
        detail_cols.sort(key=sort_key)

        # max_detail_images ê°œìˆ˜ë§Œí¼ë§Œ ì‚¬ìš© (ì˜ˆ: 10ì´ë©´ ìƒì„¸ì´ë¯¸ì§€_1~ìƒì„¸ì´ë¯¸ì§€_10ê¹Œì§€ë§Œ)
        if max_detail_images > 0:
            detail_cols = detail_cols[:max_detail_images]
            log(f"[INFO] ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼ ì œí•œ ì ìš©: {max_detail_images}ê°œê¹Œì§€ë§Œ ì‚¬ìš©")
        else:
            log(f"[INFO] ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼ ì œí•œ ì—†ìŒ: ëª¨ë“  ì»¬ëŸ¼ ì‚¬ìš©")

    log(f"[INFO] ì‚¬ìš©í•  ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼: {detail_cols}")

    image_cache: dict[str, str] = {}

    requests = []
    total_rows = len(df)
    target_rows = 0

    # ì¸ë„¤ì¼ ì œì™¸ í†µê³„ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìš”ì•½ ë¡œê·¸ìš©)
    thumbnail_exclude_count = 0
    thumbnail_exclude_logs: list[str] = []  # ì²˜ìŒ ëª‡ ê°œë§Œ ì €ì¥ (ë””ë²„ê¹…ìš©)

    # ì¤‘ë³µ ìš”ì²­ ë°©ì§€: custom_id ì¶”ì 
    seen_custom_ids: set[str] = set()
    duplicate_count = 0

    # ë¨¼ì € ì „ì²´ ëŒ€ìƒ ìš”ì²­ ìˆ˜ë¥¼ ê³„ì‚° (ë²„í‚· ìˆ˜ ê²°ì •ìš©)
    for idx, row in df.iterrows():
        existing_json = safe_str(row.get("ST2_JSON", ""))
        existing_json_clean = existing_json.strip().lower() if existing_json else ""
        if not (skip_filled and existing_json_clean and existing_json_clean not in ("", "nan", "none", "null")):
            target_rows += 1

    # ë²„í‚· ìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚° (ëª¨ë“  ìš”ì²­ì— ë™ì¼í•˜ê²Œ ì ìš©)
    if CACHE_MODE_CORE and target_rows > 0:
        # [ë²„í‚· ìˆ˜ ê³„ì‚° ì „ëµ - ì£¼ì˜: OpenAI ê³µì‹ ê¸°ì¤€ì´ ì•„ë‹Œ ì¶”ì •ì¹˜ì…ë‹ˆë‹¤]
        # 
        # [OpenAI ê³µì‹ ë¬¸ì„œ ê¸°ì¤€]
        # - ì¼ë°˜ API(ë™ê¸° ìš”ì²­): ê°™ì€ prefix + prompt_cache_key ì¡°í•©ì´ ë¶„ë‹¹ ì•½ 15ê±´ì„ ì´ˆê³¼í•˜ë©´
        #   ì¼ë¶€ê°€ ì¶”ê°€ ë¨¸ì‹ ìœ¼ë¡œ overflowë˜ì–´ ìºì‹œ íš¨ìœ¨ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
        #   (ì°¸ê³ : https://platform.openai.com/docs/guides/prompt-caching)
        # - Batch API: ê³µì‹ ë¬¸ì„œì— prompt_cache_key ë²„í‚· ë¶„ë°° ê¸°ì¤€ì´ ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŒ
        #
        # [í˜„ì¬ êµ¬í˜„ ì „ëµ (ì¶”ì •ì¹˜)]
        # - Batch APIëŠ” 24ì‹œê°„ì— ê±¸ì³ ì²˜ë¦¬ë˜ë¯€ë¡œ, ì‹¤ì œ ì²˜ë¦¬ ì‹œì ì—ëŠ” ë” ë¶„ì‚°ë  ìˆ˜ ìˆìŒ
        # - ì¼ë°˜ API ê¸°ì¤€(ë¶„ë‹¹ 15ê±´)ì„ ì°¸ê³ í•˜ì—¬, ì•ˆì „ ë§ˆì§„ì„ í¬í•¨í•˜ì—¬ ë¶„ë‹¹ 10ê±´ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        # - ëŒ€ëŸ‰ ë°°ì¹˜(5000~10000ê°œ)ë¥¼ ê³ ë ¤í•˜ì—¬ ë²„í‚· ìˆ˜ë¥¼ ì¶©ë¶„íˆ í™•ë³´
        # - ì‹¤ì œ overflow ê°€ëŠ¥ì„±ì€ Batch APIì˜ ì²˜ë¦¬ ë¶„ì‚° íŠ¹ì„±ìƒ ë‚®ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ
        #
        # [ë²„í‚· ìˆ˜ ê³„ì‚° ê°œì„ ]
        # - ë¶„ë‹¹ 15ê±´ ì œí•œì„ ê³ ë ¤í•˜ì—¬, ê° ë²„í‚·ë‹¹ ë¶„ë‹¹ 10ê±´ ì´í•˜ê°€ ë˜ë„ë¡ ê³„ì‚° (ì•ˆì „ ë§ˆì§„ í¬í•¨)
        # - ëŒ€ëŸ‰ ë°°ì¹˜(5000~10000ê°œ)ì˜ ê²½ìš° ë²„í‚·ë‹¹ ìš”ì²­ë¥ ì„ ë‚®ì¶”ê¸° ìœ„í•´ ë” ë§ì€ ë²„í‚· í•„ìš”
        # - ìµœëŒ€ ë²„í‚· ìˆ˜ë¥¼ 500ê°œë¡œ í™•ëŒ€ (5000ê°œ ìš”ì²­: 500ê°œ ë²„í‚·, 10000ê°œ ìš”ì²­: 500ê°œ ë²„í‚·)
        #
        # [ì°¸ê³ ]
        # - ì´ ë²„í‚· ë¶„ì‚°ì€ overflow ë°©ì§€ë¥¼ ìœ„í•œ ê²ƒì´ë©°, í”„ë¡¬í”„íŠ¸ ìºì‹± ìì²´ëŠ” system í”„ë¡¬í”„íŠ¸ì˜
        #   ë™ì¼ì„±ì— ì˜ì¡´í•˜ë¯€ë¡œ ìš”ì²­ ìˆ˜ì™€ ë¬´ê´€í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤
        # - "ëŒ€ëŸ‰ì´ë©´ ë²„í‚· 10ê°œë¡œ ë¶€ì¡±í•  ë•Œê°€ ë§ì•„ì„œ 20~50ê°œë¡œ ëŠ˜ë¦¬ëŠ” ê²Œ ì‹¤ì œë¡œ ì²´ê°ì— ë„ì›€ë©ë‹ˆë‹¤"
        #   â†’ 5ì²œ~1ë§Œê°œ ìš”ì²­ì˜ ê²½ìš° í›¨ì”¬ ë” ë§ì€ ë²„í‚·ì´ í•„ìš”
        # [í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ì „ëµ - ì ë‹¹íˆ ìƒ¤ë”©]
        # stage2ëŠ” ìš©ëŸ‰ ë¬¸ì œë¡œ ì²­í¬ë¥¼ ì—¬ëŸ¬ ê°œë¡œ ë¶„ë°°í•˜ë¯€ë¡œ, ì ë‹¹íˆ ìƒ¤ë”©í•˜ì—¬ ìºì‹œ íš¨ìœ¨ í–¥ìƒ
        # ìµœëŒ€ 8ê°œ ë²„í‚·ìœ¼ë¡œ ì œí•œ (ì²­í¬ë³„ë¡œ ë‹¤ë¥¸ í‚¤ë¥¼ ì‚¬ìš©í•˜ë˜, ê³¼ë„í•œ ë¶„ì‚° ë°©ì§€)
        # 500ê±´ ì´í•˜: 1ê°œ, 500~1000ê±´: 2ê°œ, 1000~2000ê±´: 4ê°œ, 2000ê±´ ì´ìƒ: 8ê°œ
        if target_rows <= 500:
            PROMPT_CACHE_BUCKETS = 1
        elif target_rows <= 1000:
            PROMPT_CACHE_BUCKETS = 2
        elif target_rows <= 2000:
            PROMPT_CACHE_BUCKETS = 4
        else:
            PROMPT_CACHE_BUCKETS = 8
        
        avg_per_bucket = target_rows // PROMPT_CACHE_BUCKETS if PROMPT_CACHE_BUCKETS > 0 else target_rows
        log(f"[INFO] í”„ë¡¬í”„íŠ¸ ìºì‹±: ì ë‹¹íˆ ìƒ¤ë”© ì „ëµ ì‚¬ìš© (ë²„í‚· ìˆ˜: {PROMPT_CACHE_BUCKETS}ê°œ)")
        log(f"[INFO] ì˜ˆìƒ ìš”ì²­ ìˆ˜: {target_rows}ê°œ, ê° ë²„í‚·ë‹¹ í‰ê·  ~{avg_per_bucket}ê±´")
    else:
        PROMPT_CACHE_BUCKETS = 1

    # target_rows ì´ˆê¸°í™” (ì‹¤ì œ ì²˜ë¦¬ ì‹œ ë‹¤ì‹œ ê³„ì‚°)
    target_rows = 0

    for idx, row in df.iterrows():
        # ST2_JSON ì¤‘ë³µ ì²´í¬ (skip_filled ì˜µì…˜)
        existing_json = safe_str(row.get("ST2_JSON", ""))
        # ë¹ˆ ë¬¸ìì—´, "nan", None ë“±ì„ ëª¨ë‘ ë¹ˆ ê°’ìœ¼ë¡œ ì²˜ë¦¬
        existing_json_clean = existing_json.strip().lower() if existing_json else ""
        if skip_filled and existing_json_clean and existing_json_clean not in ("", "nan", "none", "null"):
            log(f"[SKIP] idx={idx}: ì´ë¯¸ ST2_JSON ê°’ì´ ìˆì–´ ê±´ë„ˆëœ€.")
            continue

        target_rows += 1

        try:
            # stage2_core_Cache ì˜ ìºì‹± ìµœì í™” í”„ë¡¬í”„íŠ¸ ë¹Œë” ì‚¬ìš©
            req = build_stage2_request_from_row(row, detail_cols)
        except Exception as e:
            log(f"[ERROR] idx={idx}: Stage2 í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨ â†’ ìŠ¤í‚µ. ({e})")
            continue

        system_prompt = safe_str(getattr(req, "system_prompt", ""))
        user_prompt = safe_str(getattr(req, "user_prompt", ""))

        if not system_prompt or not user_prompt:
            log(f"[SKIP] idx={idx}: Stage2 í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ ìˆì–´ ê±´ë„ˆëœ€.")
            continue

        # ë””ë²„ê¹…ìš© ST2_í”„ë¡¬í”„íŠ¸ ê¸°ë¡ (system + user ê²°í•©)
        full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{user_prompt}"
        df.at[idx, "ST2_í”„ë¡¬í”„íŠ¸"] = full_prompt

        image_paths = list(getattr(req, "image_paths", []) or [])

        # ì¸ë„¤ì¼(ì´ë¯¸ì§€ëŒ€) ì œì™¸ ì˜µì…˜ (ì„±ëŠ¥ ìµœì í™”: ì¸ë„¤ì¼ ì œì™¸ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì²´í¬)
        if not use_thumbnail:
            thumb_val = safe_str(row.get("ì´ë¯¸ì§€ëŒ€", ""))
            if thumb_val:  # ì´ë¯¸ì§€ëŒ€ ê°’ì´ ìˆì„ ë•Œë§Œ í•„í„°ë§
                before_len = len(image_paths)
                if before_len > 0:  # ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œë§Œ í•„í„°ë§
                    image_paths = [p for p in image_paths if safe_str(p) != thumb_val]
                    if len(image_paths) != before_len:
                        thumbnail_exclude_count += 1
                        # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸ ì €ì¥ (ë””ë²„ê¹…ìš©)
                        if thumbnail_exclude_count <= 5:
                            thumbnail_exclude_logs.append(f"idx={idx}: {thumb_val[:50]}...")

        # stage2_batch_api_ê¸°ì¡´gpt ì˜ ì´ë¯¸ì§€ ì¸ì½”ë”©/ìºì‹œ ë¡œì§ ì¬ì‚¬ìš©
        # resize_modeì— ë”°ë¼ ê°€ë¡œ ê¸°ì¤€ ë¦¬ì‚¬ì´ì¦ˆ ì ìš©
        image_nodes = build_image_nodes_from_paths(
            image_paths,
            log_func=log,
            cache=image_cache,
            allow_url=allow_url,
            resize_mode=resize_mode,
        )

        # System ë©”ì‹œì§€ (í…ìŠ¤íŠ¸ë§Œ, ì •ì )
        system_content = [{"type": "input_text", "text": system_prompt}]

        # User ë©”ì‹œì§€ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€, ë™ì )
        user_content = [{"type": "input_text", "text": user_prompt}]
        user_content.extend(image_nodes)

        body = {
            "model": model_name,
            "input": [
                {
                    "role": "system",
                    "content": system_content,
                },
                {
                    "role": "user",
                    "content": user_content,
                }
            ],
        }

        # reasoning.effort
        if effort in ("low", "medium", "high"):
            body["reasoning"] = {"effort": effort}

        # custom_idì— resize_mode í¬í•¨ (ê²°ê³¼ ë¹„êµë¥¼ ìœ„í•´)
        custom_id = f"row-{idx}-{resize_mode}"

        # Prompt Caching ìµœì í™” (ìºì‹± ëª¨ë“œì¼ ë•Œë§Œ)
        if CACHE_MODE_CORE:
            # prompt_cache_key: ë²„í‚· ë¶„ì‚°ìœ¼ë¡œ ë¼ìš°íŒ… íš¨ìœ¨ í–¥ìƒ
            # 
            # [ë²„í‚· ë¶„ì‚°ì˜ ëª©ì ]
            # - ê°™ì€ prefix+key ì¡°í•©ì´ ë¶„ë‹¹ ~15ê±´ì„ ë„˜ìœ¼ë©´ overflowë¡œ ë¼ìš°íŒ…ì´ í¼ì ¸ ìºì‹œ íš¨ìœ¨ì´ ê¸‰ê°
            # - ë°°ì¹˜ APIëŠ” ì‹œê°„ì— ê±¸ì³ ì²˜ë¦¬ë˜ì§€ë§Œ, ì‹¤ì œ ì²˜ë¦¬ ì‹œì ì— ë¶„ë‹¹ 15ê±´ ì œí•œì´ ì ìš©ë¨
            # - ë²„í‚·ìœ¼ë¡œ ë¶„ì‚°í•˜ë©´ ê° ë²„í‚·ë‹¹ ìš”ì²­ ìˆ˜ê°€ ì¤„ì–´ë“¤ì–´ overflow ë°©ì§€
            #
            # [ë²„í‚· ìˆ˜ ê²°ì • - ì£¼ì˜: OpenAI ê³µì‹ ê¸°ì¤€ì´ ì•„ë‹Œ ì¶”ì •ì¹˜ì…ë‹ˆë‹¤]
            # - ì˜ˆìƒ ìš”ì²­ ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ë²„í‚· ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ê³„ì‚° (ìœ„ì—ì„œ ë¯¸ë¦¬ ê³„ì‚°ë¨)
            # - ê° ë²„í‚·ë‹¹ ë¶„ë‹¹ 10ê±´ ì´í•˜ê°€ ë˜ë„ë¡ ì„¤ì • (ì¼ë°˜ API ê¸°ì¤€ 15ê±´ì˜ ì•ˆì „ ë§ˆì§„ í¬í•¨)
            # - ìµœì†Œ 1ê°œ, ìµœëŒ€ 200ê°œ ë²„í‚· (ëŒ€ëŸ‰ ë°°ì¹˜ ëŒ€ì‘: 1000~10000ê°œ ìš”ì²­)
            # - stage2_v2: system í”„ë¡¬í”„íŠ¸ì— meta ë³µì‚¬ ëª…ì‹œ ì¶”ê°€ (2024-12-15)
            #
            # [í”„ë¡¬í”„íŠ¸ ìºì‹± ì‘ë™ ì—¬ë¶€]
            # - í”„ë¡¬í”„íŠ¸ ìºì‹±ì€ ìš”ì²­ ìˆ˜ì™€ ë¬´ê´€í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤ (system í”„ë¡¬í”„íŠ¸ê°€ ë™ì¼í•˜ë©´ ìºì‹œ íˆíŠ¸)
            # - ë²„í‚· ë¶„ì‚°ì€ overflow ë°©ì§€ë¥¼ ìœ„í•œ ê²ƒì´ë©°, ìºì‹± ìì²´ëŠ” system í”„ë¡¬í”„íŠ¸ì˜ ë™ì¼ì„±ì— ì˜ì¡´
            # - ë°°ì¹˜ APIëŠ” 24ì‹œê°„ì— ê±¸ì³ ì²˜ë¦¬ë˜ë¯€ë¡œ, ì‹¤ì œ ì²˜ë¦¬ ì‹œì ì—ëŠ” ë” ë¶„ì‚°ë˜ì–´ overflow ê°€ëŠ¥ì„± ê°ì†Œ
            #
            # [ì¤‘ìš”: í”„ë¡¬í”„íŠ¸ ìºì‹±ì´ ê²°ê³¼ê°’ì— ë¯¸ì¹˜ëŠ” ì˜í–¥]
            # - í”„ë¡¬í”„íŠ¸ ìºì‹± ìì²´ëŠ” ê²°ê³¼ê°’ì„ ë³€ê²½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (ë¹„ìš©/ì§€ì—°ë§Œ ì˜í–¥)
            # - ë‹¤ë§Œ ì´ë²ˆ ìˆ˜ì •ì—ì„œ "system/user ë¶„ë¦¬" + "metaë¥¼ user JSONì—ì„œ ë³µì‚¬" ê°™ì€ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ë³€í™”ëŠ”
            #   ëª¨ë¸ í–‰ë™(íŠ¹íˆ meta ëˆ„ë½/ë³µì‚¬ ì •í™•ë„)ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
            # - system í”„ë¡¬í”„íŠ¸ì— "meta(JSON)ë¥¼ í‚¤/ê°’ ì™„ì „íˆ ë™ì¼ ë³µì‚¬" ë¬¸êµ¬ê°€ í¬í•¨ë˜ì–´ ìˆì–´
            #   meta ëˆ„ë½ ë¦¬ìŠ¤í¬ëŠ” ë§ì´ ì¤„ì–´ë“  ìƒíƒœì…ë‹ˆë‹¤
            #
            # [ì°¸ê³ : OpenAI ê³µì‹ ë¬¸ì„œ]
            # - ì¼ë°˜ API: ê°™ì€ prefix + prompt_cache_key ì¡°í•©ì´ ë¶„ë‹¹ ì•½ 15ê±´ ì´ˆê³¼ ì‹œ overflow ê°€ëŠ¥
            # - Batch API: ê³µì‹ ë¬¸ì„œì— prompt_cache_key ë²„í‚· ë¶„ë°° ê¸°ì¤€ì´ ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŒ
            # - í˜„ì¬ êµ¬í˜„ì€ ì¼ë°˜ API ê¸°ì¤€ì„ ì°¸ê³ í•œ ì¶”ì •ì¹˜ì´ë©°, ì‹¤ì œ Batch API ë™ì‘ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
            
            # prompt_cache_key: ì ë‹¹íˆ ìƒ¤ë”© (ì²­í¬ ë¶„ë°° ê³ ë ¤)
            bucket_num = hash(custom_id) % PROMPT_CACHE_BUCKETS
            body["prompt_cache_key"] = f"stage2_v2_b{bucket_num:02d}"
            
            # prompt_cache_retention: ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
            # Extended retention ì§€ì› ëª¨ë¸: gpt-5.1, gpt-5.1-codex, gpt-5.1-codex-mini, gpt-5.1-chat-latest, gpt-5, gpt-5-codex, gpt-4.1
            # gpt-5-mini, gpt-5-nanoëŠ” prompt_cache_retention íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
            if model_name in ["gpt-5.1", "gpt-5.1-codex", "gpt-5.1-codex-mini", "gpt-5.1-chat-latest", "gpt-5", "gpt-5-codex", "gpt-4.1"]:
                body["prompt_cache_retention"] = "extended"  # 24ì‹œê°„ retention
            elif model_name not in ["gpt-5-mini", "gpt-5-nano"]:
                # ê¸°íƒ€ ëª¨ë¸ì€ in-memory ì‚¬ìš© (5~10ë¶„ inactivity, ìµœëŒ€ 1ì‹œê°„)
                body["prompt_cache_retention"] = "in_memory"
        
        # Responses API: text.formatìœ¼ë¡œ JSON ëª¨ë“œ ê°•ì œ (Structured Outputs)
        # í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œ JSON ê°•ì œí•˜ëŠ” ëŒ€ì‹ , text.formatìœ¼ë¡œ íŒŒì‹± ì•ˆì •ì„± í–¥ìƒ
        # ë¬¸ì„œ ìŠ¤í™ì— ë§ì¶° formatì„ ê°ì²´ í˜•íƒœë¡œ ì„¤ì • (í–¥í›„ í˜¸í™˜ì„± ë³´ì¥)
        body["text"] = {
            "format": {
                "type": "json_object"  # JSON ëª¨ë“œ ê°•ì œ (JSON SchemaëŠ” í•„ìš” ì‹œ ì¶”ê°€ ê°€ëŠ¥)
            }
        }

        # ì¤‘ë³µ custom_id ì²´í¬
        if custom_id in seen_custom_ids:
            duplicate_count += 1
            log(f"[WARN] ì¤‘ë³µ ìš”ì²­ ê°ì§€: custom_id={custom_id} (idx={idx}) - ê±´ë„ˆëœ€.")
            continue

        seen_custom_ids.add(custom_id)

        # ìš©ëŸ‰ ë¶„ì„ìš©: ê° ìš”ì²­ì˜ ì´ë¯¸ì§€ ë°ì´í„° í¬ê¸° ì¶”ì •
        image_data_size = 0
        for node in image_nodes:
            if node.get("type") == "input_image" and "image_url" in node:
                img_url = node["image_url"]
                if img_url.startswith("data:"):
                    # Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€: data:image/jpeg;base64,{base64_string}
                    # Base64 ë¬¸ìì—´ ê¸¸ì´ë¡œ í¬ê¸° ì¶”ì • (ì•½ 4/3 ë°°ìœ¨)
                    base64_part = img_url.split(",", 1)[1] if "," in img_url else ""
                    image_data_size += len(base64_part)  # Base64 ë¬¸ìì—´ ê¸¸ì´ (ë°”ì´íŠ¸ ë‹¨ìœ„)

        requests.append(
            {
                "custom_id": custom_id,
                "method": "POST",
                "url": "/v1/responses",
                "body": body,
                "_size_estimate": {  # ë””ë²„ê¹…ìš© (ì‹¤ì œ JSONLì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ)
                    "image_data_bytes": image_data_size,
                    "image_count": len(image_nodes),
                }
            }
        )

    # ì¸ë„¤ì¼ ì œì™¸ ìš”ì•½ ë¡œê·¸ (ì„±ëŠ¥ ìµœì í™”)
    if not use_thumbnail and thumbnail_exclude_count > 0:
        if thumbnail_exclude_count <= 10:
            # 10ê°œ ì´í•˜ë©´ ê°œë³„ ë¡œê·¸ ì¶œë ¥
            for log_msg in thumbnail_exclude_logs:
                log(f"[INFO] {log_msg}")
            if thumbnail_exclude_count > len(thumbnail_exclude_logs):
                log(f"[INFO] ... ì™¸ {thumbnail_exclude_count - len(thumbnail_exclude_logs)}ê°œ í–‰ì—ì„œ ì¸ë„¤ì¼ ì œì™¸ë¨")
        else:
            # 10ê°œ ì´ˆê³¼ë©´ ìš”ì•½ ë¡œê·¸ë§Œ
            if thumbnail_exclude_logs:
                for log_msg in thumbnail_exclude_logs[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                    log(f"[INFO] {log_msg}")
            log(f"[INFO] ì´ {thumbnail_exclude_count}ê°œ í–‰ì—ì„œ ì¸ë„¤ì¼(ì´ë¯¸ì§€ëŒ€)ì´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ì˜µì…˜: ì¸ë„¤ì¼ ì œì™¸)")

    # ì¤‘ë³µ ìš”ì²­ ê°ì§€ ë¡œê·¸
    if duplicate_count > 0:
        log(f"[WARN] âš ï¸ ì¤‘ë³µ ìš”ì²­ {duplicate_count}ê°œê°€ ê°ì§€ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ê°™ì€ í–‰ì´ ì—¬ëŸ¬ ë²ˆ ìš”ì²­ë˜ëŠ” ê²ƒì„ ë°©ì§€)")
        log(f"[WARN] âš ï¸ ì¤‘ë³µ ìš”ì²­ìœ¼ë¡œ ì¸í•´ ì‹¤ì œ ìš”ì²­ ìˆ˜ê°€ {len(requests)}ê°œì…ë‹ˆë‹¤. (ì˜ˆìƒ: {target_rows}ê°œ)")

    # ìµœì¢… í†µê³„ ë¡œê·¸
    log(f"[INFO] ìµœì¢… ìš”ì²­ í†µê³„:")
    log(f"  - ì „ì²´ í–‰ ìˆ˜: {total_rows}ê°œ")
    log(f"  - ëŒ€ìƒ í–‰ ìˆ˜ (ST2_JSON ë¹„ì–´ìˆìŒ): {target_rows}ê°œ")
    log(f"  - ì‹¤ì œ ìƒì„±ëœ ìš”ì²­ ìˆ˜: {len(requests)}ê°œ")
    if duplicate_count > 0:
        log(f"  - ì¤‘ë³µ ì œì™¸: {duplicate_count}ê°œ")
    if target_rows != len(requests):
        log(f"  - âš ï¸ ì°¨ì´: {target_rows - len(requests)}ê°œ (ì¤‘ë³µ ì œì™¸ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨)")

    if not requests:
        raise RuntimeError("Batch ìš”ì²­ì— ì‚¬ìš©í•  ìœ íš¨í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ìš©ëŸ‰ ë¶„ì„: ì´ë¯¸ì§€ ë°ì´í„° í¬ê¸° í†µê³„
    total_image_data_bytes = 0
    total_image_count = 0
    requests_with_images = 0
    for req in requests:
        size_est = req.get("_size_estimate", {})
        if size_est:
            total_image_data_bytes += size_est.get("image_data_bytes", 0)
            total_image_count += size_est.get("image_count", 0)
            if size_est.get("image_count", 0) > 0:
                requests_with_images += 1
    
    # JSONL íŒŒì¼ ìƒì„± (ìš©ëŸ‰ ë¶„ì„ìš© í•„ë“œ ì œê±°)
    import json as json_module
    with open(jsonl_path, "w", encoding="utf-8") as f:
        for item in requests:
            # _size_estimate í•„ë“œëŠ” ì‹¤ì œ JSONLì— í¬í•¨í•˜ì§€ ì•ŠìŒ
            item_clean = {k: v for k, v in item.items() if k != "_size_estimate"}
            f.write(json_module.dumps(item_clean, ensure_ascii=False) + "\n")
    
    # ìš©ëŸ‰ ë¶„ì„ ë¡œê·¸
    jsonl_size_bytes = os.path.getsize(jsonl_path)
    jsonl_size_mb = jsonl_size_bytes / (1024 * 1024)
    image_data_mb = total_image_data_bytes / (1024 * 1024)
    image_data_ratio = (total_image_data_bytes / jsonl_size_bytes * 100) if jsonl_size_bytes > 0 else 0
    
    log(f"[INFO] ğŸ“Š JSONL ìš©ëŸ‰ ë¶„ì„:")
    log(f"  - ì „ì²´ íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB ({jsonl_size_bytes:,} bytes)")
    log(f"  - Base64 ì´ë¯¸ì§€ ë°ì´í„°: {image_data_mb:.2f} MB ({total_image_data_bytes:,} bytes, {image_data_ratio:.1f}%)")
    log(f"  - í…ìŠ¤íŠ¸/ë©”íƒ€ë°ì´í„°: {jsonl_size_mb - image_data_mb:.2f} MB ({(100 - image_data_ratio):.1f}%)")
    log(f"  - ì´ ì´ë¯¸ì§€ ê°œìˆ˜: {total_image_count}ê°œ (í‰ê·  {total_image_count / len(requests):.1f}ê°œ/ìš”ì²­)")
    log(f"  - ì´ë¯¸ì§€ í¬í•¨ ìš”ì²­: {requests_with_images}ê°œ / {len(requests)}ê°œ")
    if total_image_data_bytes > 0:
        avg_image_size_mb = (total_image_data_bytes / total_image_count) / (1024 * 1024) if total_image_count > 0 else 0
        log(f"  - í‰ê·  ì´ë¯¸ì§€ í¬ê¸°: {avg_image_size_mb:.2f} MB (Base64 ì¸ì½”ë”© í›„)")
        log(f"[INFO] ğŸ’¡ ì°¸ê³ : Base64 ì¸ì½”ë”©ì€ ì›ë³¸ ì´ë¯¸ì§€ë³´ë‹¤ ì•½ 33% í¬ê¸°ê°€ ì¦ê°€í•©ë‹ˆë‹¤.")

    # ST2_í”„ë¡¬í”„íŠ¸ ê¸°ë¡ì„ ìœ„í•´ ì—‘ì…€ ë®ì–´ì“°ê¸° (ì—´ë ¤ìˆìœ¼ë©´ ì‹¤íŒ¨í•´ë„ ë¬´ë°©)
    try:
        df.to_excel(excel_path, index=False)
        log(f"[INFO] ì—‘ì…€ì— ST2_í”„ë¡¬í”„íŠ¸ ê°±ì‹  ì™„ë£Œ: {excel_path}")
    except Exception as e:
        log(f"[WARN] ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨(ì—´ë ¤ìˆì„ ìˆ˜ ìˆìŒ): {e}")

    log(
        f"[DONE] Batch ì…ë ¥ JSONL ìƒì„± ì™„ë£Œ: {jsonl_path} "
        f"(ì „ì²´ {total_rows}í–‰ ì¤‘ ëŒ€ìƒ {target_rows}í–‰, ìš”ì²­ {len(requests)}ê°œ)"
    )

    return {
        "total_rows": total_rows,
        "target_rows": target_rows,
        "num_requests": len(requests),
    }

def get_seoul_now():
    try:
        from pytz import timezone
        return datetime.now(timezone("Asia/Seoul"))
    except:
        return datetime.now()

def load_api_key_from_file(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f: return f.read().strip()
    return ""

def save_api_key_to_file(key, path):
    with open(path, "w", encoding="utf-8") as f: f.write(key)

def load_default_settings():
    """ê¸°ë³¸ ì„¤ì •ê°’ ë¶ˆëŸ¬ì˜¤ê¸°"""
    default_settings = {
        "model": "gpt-5-mini",
        "effort": "medium",
        "resize_mode": "B"
    }
    
    if os.path.exists(DEFAULT_SETTINGS_FILE):
        try:
            with open(DEFAULT_SETTINGS_FILE, "r", encoding="utf-8") as f:
                saved_settings = json.load(f)
                # ì €ì¥ëœ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ìœ íš¨í•œ ê°’ë§Œ)
                if "model" in saved_settings:
                    default_settings["model"] = saved_settings["model"]
                if "effort" in saved_settings:
                    default_settings["effort"] = saved_settings["effort"]
                if "resize_mode" in saved_settings:
                    default_settings["resize_mode"] = saved_settings["resize_mode"]
        except Exception as e:
            print(f"[WARN] ê¸°ë³¸ ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
    
    return default_settings

def save_default_settings(model, effort, resize_mode):
    """ê¸°ë³¸ ì„¤ì •ê°’ ì €ì¥"""
    settings = {
        "model": model,
        "effort": effort,
        "resize_mode": resize_mode
    }
    try:
        with open(DEFAULT_SETTINGS_FILE, "w", encoding="utf-8") as f:
            json.dump(settings, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"[ERROR] ê¸°ë³¸ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# ========================================================
# íˆ´íŒ í´ë˜ìŠ¤ (ìµœìƒë‹¨ ì •ì˜)
# ========================================================
class ToolTip:
    def __init__(self, widget, text, wraplength=400):
        self.widget = widget
        self.text = text
        self.wraplength = wraplength
        self.tipwindow = None
        # ë§ˆìš°ìŠ¤ ì´ë²¤íŠ¸ë¥¼ ê°€ë¡œì±„ì§€ ì•Šë„ë¡ ì§€ì—° ì‹œê°„ ì¶”ê°€
        self.widget.bind("<Enter>", self._on_enter)
        self.widget.bind("<Leave>", self.hide_tip)
        self.widget.bind("<Button-1>", self.hide_tip)  # í´ë¦­ ì‹œ íˆ´íŒ ìˆ¨ê¹€
        self._after_id = None

    def _on_enter(self, event=None):
        """ë§ˆìš°ìŠ¤ ì§„ì… ì‹œ ì§€ì—° í›„ íˆ´íŒ í‘œì‹œ"""
        # ê¸°ì¡´ íƒ€ì´ë¨¸ ì·¨ì†Œ
        if self._after_id:
            self.widget.after_cancel(self._after_id)
        # 500ms í›„ íˆ´íŒ í‘œì‹œ (í´ë¦­ ì´ë²¤íŠ¸ë¥¼ ë°©í•´í•˜ì§€ ì•Šë„ë¡)
        self._after_id = self.widget.after(500, self.show_tip)

    def show_tip(self, event=None):
        if self.tipwindow or not self.text: return
        x = self.widget.winfo_rootx() + 20
        y = self.widget.winfo_rooty() + 20
        self.tipwindow = tw = tk.Toplevel(self.widget)
        tw.wm_overrideredirect(True)
        # íˆ´íŒì´ í´ë¦­ ì´ë²¤íŠ¸ë¥¼ ë°›ì§€ ì•Šë„ë¡ ì„¤ì •
        tw.attributes("-topmost", True)
        tw.wm_geometry(f"+{x}+{y}")
        label = tk.Label(tw, text=self.text, justify='left', background="#ffffe0", relief='solid', borderwidth=1, font=("ë§‘ì€ ê³ ë”•", 9), wraplength=self.wraplength)
        label.pack(ipadx=4, ipady=2)
        # íˆ´íŒ í´ë¦­ ì‹œ ì¦‰ì‹œ ìˆ¨ê¹€
        tw.bind("<Button-1>", lambda e: self.hide_tip())

    def hide_tip(self, event=None):
        if self.tipwindow:
            self.tipwindow.destroy()
            self.tipwindow = None
        if self._after_id:
            self.widget.after_cancel(self._after_id)
            self._after_id = None

# ========================================================
# ë°°ì¹˜ ì¡ ê´€ë¦¬ (JSON DB)
# ========================================================
def load_batch_jobs():
    if not os.path.exists(BATCH_JOBS_FILE): return []
    try:
        with open(BATCH_JOBS_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return []

def save_batch_jobs(jobs):
    try:
        with open(BATCH_JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
    except Exception as e: print(f"[Error] ì¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def upsert_batch_job(batch_id, **kwargs):
    jobs = load_batch_jobs()
    found = False
    now_str = datetime.now().isoformat()
    for j in jobs:
        if j["batch_id"] == batch_id:
            if kwargs.get("status") == "completed" and j.get("status") != "completed":
                if "completed_at" not in kwargs: j["completed_at"] = now_str
            j.update(kwargs)
            j["updated_at"] = now_str
            found = True
            break
    if not found:
        new_job = {
            "batch_id": batch_id, "created_at": now_str, "updated_at": now_str,
            "completed_at": "", "archived": False, **kwargs
        }
        jobs.insert(0, new_job)
    save_batch_jobs(jobs)

def archive_batch_job(batch_ids, archive=True):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    for j in jobs:
        if j["batch_id"] in batch_ids: j["archived"] = archive
    save_batch_jobs(jobs)

def hard_delete_batch_job(batch_ids):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    jobs = [j for j in jobs if j["batch_id"] not in batch_ids]
    save_batch_jobs(jobs)

# ========================================================
# Payload Builder (Stage 2 ì „ìš©)
# ========================================================
def build_stage2_batch_payload(row_index, row, model, effort, use_thumb, allow_url):
    """
    Stage 2 Coreì˜ ë¡œì§ì„ í™œìš©í•˜ì—¬ Batch Payload ìƒì„±
    - use_thumb: ì¸ë„¤ì¼(ì´ë¯¸ì§€ëŒ€) í¬í•¨ ì—¬ë¶€
    - allow_url: URL ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í—ˆìš© ì—¬ë¶€ (Core ë¡œì§ì— ë”°ë¼ ë™ì‘)
    """
    try:
        # Core í•¨ìˆ˜ í˜¸ì¶œ (Coreê°€ ì˜µì…˜ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ë¡œì§ ìˆ˜í–‰)
        # ì—¬ê¸°ì„œëŠ” Coreê°€ row ì „ì²´ë¥¼ ë°›ì•„ íŒë‹¨í•œë‹¤ê³  ê°€ì •í•˜ê³ , ì˜µì…˜ì— ë”°ë¼ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•„í„°ë§í•  ìˆ˜ë„ ìˆìŒ
        # í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ Core êµ¬í˜„ìƒ rowì— ìˆëŠ” ì •ë³´ë¥¼ ë‹¤ ê°€ì ¸ì˜¤ë¯€ë¡œ, 
        # Core ìˆ˜ì • ì—†ì´ ì—¬ê¸°ì„œ request ê°ì²´ì˜ image_pathsë¥¼ ì¡°ì‘í•˜ëŠ” ê²ƒì´ ì•ˆì „í•¨.
        req = build_stage2_request_from_row(row)
    except Exception:
        return None

    if not req: return None

    # 1. ì´ë¯¸ì§€ í•„í„°ë§ ë¡œì§ (ì˜µì…˜ ì ìš©)
    final_messages = []
    
    # Prompt ë©”ì‹œì§€ (System/User text)
    # Coreê°€ ë§Œë“  messages ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì´ë¯¸ì§€ ë¶€ë¶„ë§Œ í•„í„°ë§
    for msg in req.messages:
        if not isinstance(msg.get('content'), list):
            final_messages.append(msg) # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            continue
            
        new_content = []
        for item in msg['content']:
            if item['type'] == 'text':
                new_content.append(item)
            elif item['type'] == 'image_url':
                url = item['image_url']['url']
                
                # ì¸ë„¤ì¼ ì œì™¸ ë¡œì§ (ì´ë¯¸ì§€ëŒ€ ì»¬ëŸ¼ê°’ê³¼ ë¹„êµ)
                thumb_val = safe_str(row.get("ì´ë¯¸ì§€ëŒ€", ""))
                if not use_thumb and thumb_val and url == thumb_val:
                    continue # ì¸ë„¤ì¼ì´ë©´ ê±´ë„ˆëœ€
                
                # URL í—ˆìš© ì—¬ë¶€ ë¡œì§
                if not allow_url and (url.startswith("http://") or url.startswith("https://")):
                    continue # URL ë¹„í—ˆìš©ì´ë©´ ê±´ë„ˆëœ€
                
                new_content.append(item)
        
        if new_content:
            final_messages.append({"role": msg['role'], "content": new_content})

    # 2. Body êµ¬ì„±
    body = {
        "model": model,
        "messages": final_messages,
        "response_format": {"type": "json_object"} # JSON ì¶œë ¥ ê°•ì œ
    }
    
    # 3. ì¶”ë¡  ëª¨ë¸ íŒŒë¼ë¯¸í„°
    is_reasoning = any(x in model for x in ["gpt-5", "o1", "o3"])
    if is_reasoning and effort in ["low", "medium", "high"]:
        body["reasoning_effort"] = effort
    elif not is_reasoning:
        body["temperature"] = 0.0 # ì •ë°€ë„ ìš°ì„ 

    # 4. Batch Request ê°ì²´
    request_obj = {
        "custom_id": f"row_{row_index}",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": body
    }
    return request_obj

# ========================================================
# GUI Class
# ========================================================
class Stage2BatchGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Stage 2: Batch API Manager (Multimodal & Analytics) - ğŸš€ ìºì‹± ìµœì í™” ë²„ì „")
        self.geometry("1500x950")  # ê°€ë¡œ ê¸¸ì´ ì¦ê°€ (1280 â†’ 1500)
        
        self.api_key_var = tk.StringVar()
        self.src_file_var = tk.StringVar()
        
        # ê¸°ë³¸ê°’ (ì €ì¥ëœ ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°)
        default_settings = load_default_settings()
        self.model_var = tk.StringVar(value=default_settings.get("model", "gpt-5-mini"))
        self.effort_var = tk.StringVar(value=default_settings.get("effort", "medium"))
        self.skip_exist_var = tk.BooleanVar(value=True)
        
        # Stage 2 ì˜µì…˜
        self.use_thumbnail_var = tk.BooleanVar(value=False)  # ê¸°ë³¸ê°’: ì¸ë„¤ì¼ ì œì™¸ (ì„±ëŠ¥ ìµœì í™”)
        self.allow_url_var = tk.BooleanVar(value=False)
        self.max_detail_images_var = tk.IntVar(value=10)  # ê¸°ë³¸ê°’: ìƒì„¸ì´ë¯¸ì§€ 10ê°œê¹Œì§€ë§Œ ì‚¬ìš©
        self.resize_mode_var = tk.StringVar(value=default_settings.get("resize_mode", "B"))  # ê¸°ë³¸ê°’: B(512px)

        self.batch_id_var = tk.StringVar()
        
        # ìƒì„¸ì´ë¯¸ì§€ í†µê³„ ì •ë³´ ì €ì¥
        self.detail_image_stats = {
            "max_columns": 0,  # ì—‘ì…€ì— ìˆëŠ” ìµœëŒ€ ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼ ê°œìˆ˜
            "row_counts": {},  # {ì»¬ëŸ¼ê°œìˆ˜: í–‰ê°œìˆ˜} ì˜ˆ: {15: 5, 20: 3}
        }
        
        self._configure_styles()
        self._init_ui()
        self._load_key()

    def _configure_styles(self):
        style = ttk.Style()
        try: style.theme_use('clam')
        except: pass
        
        self.configure(background=COLOR_BG)
        
        style.configure("TFrame", background=COLOR_BG)
        style.configure("TLabel", background=COLOR_BG, foreground=COLOR_TEXT, font=("ë§‘ì€ ê³ ë”•", 10))
        style.configure("Header.TLabel", font=("ë§‘ì€ ê³ ë”•", 11, "bold"), foreground="#444")
        
        style.configure("TLabelframe", background=COLOR_BG, bordercolor="#D0D7DE")
        style.configure("TLabelframe.Label", background=COLOR_BG, foreground="#0056b3", font=("ë§‘ì€ ê³ ë”•", 10, "bold"))

        style.configure("TNotebook", background=COLOR_BG, borderwidth=0)
        style.configure("TNotebook.Tab", background="#E1E4E8", padding=[12, 5], font=("ë§‘ì€ ê³ ë”•", 10))
        style.map("TNotebook.Tab", background=[("selected", COLOR_WHITE)], foreground=[("selected", COLOR_PRIMARY)])
        
        style.configure("Treeview", background=COLOR_WHITE, fieldbackground=COLOR_WHITE, font=("ë§‘ì€ ê³ ë”•", 9), rowheight=28)
        style.configure("Treeview.Heading", background=COLOR_HEADER, foreground="#333", font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        style.map("Treeview", background=[('selected', '#CCE5FF')], foreground=[('selected', 'black')])

        style.configure("TButton", font=("ë§‘ì€ ê³ ë”•", 9), padding=5, borderwidth=1)
        style.configure("Primary.TButton", background=COLOR_PRIMARY, foreground="white", bordercolor=COLOR_PRIMARY)
        style.map("Primary.TButton", background=[("active", COLOR_PRIMARY_HOVER)])
        style.configure("Success.TButton", background=COLOR_SUCCESS, foreground="white", bordercolor=COLOR_SUCCESS)
        style.map("Success.TButton", background=[("active", COLOR_SUCCESS_HOVER)])
        style.configure("Danger.TButton", background=COLOR_DANGER, foreground="white", bordercolor=COLOR_DANGER)
        style.map("Danger.TButton", background=[("active", COLOR_DANGER_HOVER)])

    def _init_ui(self):
        main_container = ttk.Frame(self, padding=15)
        main_container.pack(fill='both', expand=True)

        # 1. ìƒë‹¨ API
        f_top = ttk.LabelFrame(main_container, text="ğŸ”‘ API ì„¤ì •", padding=10)
        f_top.pack(fill='x', pady=(0, 10))
        ttk.Label(f_top, text="Batch API Key:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(side='left')
        entry_key = ttk.Entry(f_top, textvariable=self.api_key_var, show="*", width=50, font=("Consolas", 10))
        entry_key.pack(side='left', padx=10)
        btn_save = ttk.Button(f_top, text="ì €ì¥", command=self._save_key, style="Primary.TButton")
        btn_save.pack(side='left')
        ToolTip(btn_save, "ì…ë ¥í•œ API Keyë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.")
        
        btn_save_defaults = ttk.Button(f_top, text="ğŸ’¾ ê¸°ë³¸ê°’ ì €ì¥", command=self._save_defaults, style="Success.TButton")
        btn_save_defaults.pack(side='left', padx=(5, 0))
        ToolTip(btn_save_defaults, "í˜„ì¬ ì„¤ì •ëœ ëª¨ë¸, effort, ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\në‹¤ìŒ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")

        btn_help = ttk.Button(f_top, text="â“ ì‚¬ìš©ë²• / ì›Œí¬í”Œë¡œìš°", command=self._show_help_dialog)
        btn_help.pack(side='right')

        # 2. ë©”ì¸ íƒ­
        self.main_tabs = ttk.Notebook(main_container)
        self.main_tabs.pack(fill='both', expand=True, pady=5)
        
        self.tab_create = ttk.Frame(self.main_tabs)
        self.tab_manage = ttk.Frame(self.main_tabs) 
        self.tab_merge = ttk.Frame(self.main_tabs)
        
        self.main_tabs.add(self.tab_create, text=" 1. ë°°ì¹˜ ìƒì„± & ì—…ë¡œë“œ ")
        self.main_tabs.add(self.tab_manage, text=" 2. ë°°ì¹˜ ê´€ë¦¬ (ëª©ë¡/ë³‘í•©/ë¦¬í¬íŠ¸) ")
        self.main_tabs.add(self.tab_merge, text=" 3. ê°œë³„ ë³‘í•© (ìˆ˜ë™) ")
        
        self._init_tab_create()
        self._init_tab_manage()
        self._init_tab_merge()
        
        # 3. ë¡œê·¸
        f_log = ttk.LabelFrame(main_container, text="ğŸ“‹ ì‹œìŠ¤í…œ ë¡œê·¸", padding=10)
        f_log.pack(fill='both', expand=True, pady=(10, 0))
        self.log_widget = ScrolledText(f_log, height=12, state='disabled', font=("Consolas", 9), bg="#F1F3F5")
        self.log_widget.pack(fill='both', expand=True)

    def _load_key(self):
        loaded = load_api_key_from_file(API_KEY_FILE)
        if loaded: self.api_key_var.set(loaded)

    def _save_key(self):
        k = self.api_key_var.get().strip()
        if k:
            save_api_key_to_file(k, API_KEY_FILE)
            messagebox.showinfo("ì €ì¥", "API Key ì €ì¥ ì™„ë£Œ")
    
    def _save_defaults(self):
        """í˜„ì¬ ì„¤ì •ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì €ì¥"""
        model = self.model_var.get().strip()
        effort = self.effort_var.get().strip()
        resize_mode = self.resize_mode_var.get().strip()
        
        if save_default_settings(model, effort, resize_mode):
            messagebox.showinfo(
                "ì €ì¥ ì™„ë£Œ", 
                f"ê¸°ë³¸ê°’ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n\n"
                f"â€¢ ëª¨ë¸: {model}\n"
                f"â€¢ Effort: {effort}\n"
                f"â€¢ ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ: {resize_mode}\n\n"
                f"ë‹¤ìŒ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
            )
        else:
            messagebox.showerror("ì €ì¥ ì‹¤íŒ¨", "ê¸°ë³¸ê°’ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    def append_log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        self.log_widget.config(state='normal')
        self.log_widget.insert('end', f"[{ts}] {msg}\n")
        self.log_widget.see('end')
        self.log_widget.config(state='disabled')

    def _show_help_dialog(self):
        msg = (
            "[Stage 2 Batch API ì›Œí¬í”Œë¡œìš°]\n\n"
            "1. [ìƒì„± íƒ­]: ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ê³  'Start Batch'ë¥¼ í´ë¦­.\n"
            "   - ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ê²½ìš° Vision API ìš”ì²­ì„ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.\n"
            "   - ë¹„ìš©ì€ ì‹¤ì‹œê°„ API ëŒ€ë¹„ 50% ì ˆê°ë©ë‹ˆë‹¤.\n\n"
            "2. [ê´€ë¦¬ íƒ­]: ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ê³  'ì™„ë£Œ(completed)' ì‹œ ë³‘í•©í•©ë‹ˆë‹¤.\n"
            "   - [ì„ íƒ ê°±ì‹ ]: OpenAI ì„œë²„ì—ì„œ ìµœì‹  ìƒíƒœë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n"
            "   - [ì„ íƒ ë³‘í•©]: ê²°ê³¼ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì›ë³¸ ì—‘ì…€(ST2_JSON)ì— ì €ì¥í•©ë‹ˆë‹¤.\n"
            "   - [ë¶„ì„ ë¦¬í¬íŠ¸]: JSON íŒŒì‹± ì„±ê³µë¥ , í‚¤ì›Œë“œ ìˆ˜ ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n\n"
            "â€» ì´ë¯¸ì§€ê°€ ì—†ëŠ” í–‰ì€ í…ìŠ¤íŠ¸ë§Œ ë¶„ì„í•˜ê±°ë‚˜, ì„¤ì •ì— ë”°ë¼ ê±´ë„ˆëœë‹ˆë‹¤."
        )
        messagebox.showinfo("ì‚¬ìš©ë²•", msg)

    # ----------------------------------------------------
    # Tab 1: Create
    # ----------------------------------------------------
    def _init_tab_create(self):
        container = ttk.Frame(self.tab_create, padding=20)
        container.pack(fill='both', expand=True)
        
        # Step 1: íŒŒì¼
        f_file = ttk.LabelFrame(container, text="1. ì‘ì—… ëŒ€ìƒ íŒŒì¼ ì„ íƒ", padding=15)
        f_file.pack(fill='x', pady=(0, 15))
        ttk.Entry(f_file, textvariable=self.src_file_var, font=("ë§‘ì€ ê³ ë”•", 10)).pack(side='left', fill='x', expand=True)
        btn_file = ttk.Button(f_file, text="ğŸ“‚ íŒŒì¼ ì°¾ê¸°", command=self._select_src_file)
        btn_file.pack(side='right', padx=5)
        ToolTip(btn_file, "Stage 2ë¥¼ ìˆ˜í–‰í•  ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.\nì´ë¯¸ì§€ ì»¬ëŸ¼ì´ ì—†ì–´ë„ í…ìŠ¤íŠ¸ ë¶„ì„ì´ ê°€ëŠ¥í•˜ë©´ ì§„í–‰ë©ë‹ˆë‹¤.")
        
        # Step 2: ì˜µì…˜
        f_opt = ttk.LabelFrame(container, text="2. ë°°ì¹˜ ì˜µì…˜ ì„¤ì •", padding=15)
        f_opt.pack(fill='x', pady=5)
        
        # ëª¨ë¸/Effort
        fr1 = ttk.Frame(f_opt)
        fr1.pack(fill='x', pady=5)
        ttk.Label(fr1, text="ëª¨ë¸ (Model):", width=12).pack(side='left')
        models = list(MODEL_PRICING_USD_PER_MTOK.keys())
        cb_model = ttk.Combobox(fr1, textvariable=self.model_var, values=models, state="readonly", width=20)
        cb_model.pack(side='left', padx=5)
        ToolTip(cb_model, "ì´ë¯¸ì§€ ë¶„ì„(Vision) ì„±ëŠ¥ì´ ë›°ì–´ë‚œ GPT-5 ê³„ì—´ ëª¨ë¸ ê¶Œì¥.")
        
        ttk.Label(fr1, text="ì¶”ë¡  ê°•ë„:", width=10).pack(side='left', padx=(20, 5))
        cb_effort = ttk.Combobox(fr1, textvariable=self.effort_var, values=["low", "medium", "high"], state="readonly", width=12)
        cb_effort.pack(side='left', padx=5)
        ToolTip(cb_effort, "ì´ë¯¸ì§€ ë¶„ì„ì˜ ê¹Šì´ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\nMedium ê¶Œì¥.")
        
        # Stage 2 ì „ìš© ì˜µì…˜
        fr2 = ttk.Frame(f_opt)
        fr2.pack(fill='x', pady=5)
        chk_skip = ttk.Checkbutton(fr2, text=" ì´ë¯¸ ST2_JSONì´ ìˆëŠ” í–‰ì€ ê±´ë„ˆë›°ê¸° (Skip)", variable=self.skip_exist_var)
        chk_skip.pack(side='left', padx=5)
        ToolTip(chk_skip, "ì¤‘ë³µ ê³¼ê¸ˆ ë°©ì§€ë¥¼ ìœ„í•´ ì´ë¯¸ ê²°ê³¼ê°€ ìˆëŠ” í–‰ì€ ì œì™¸í•©ë‹ˆë‹¤.")

        chk_thumb = ttk.Checkbutton(fr2, text=" ì¸ë„¤ì¼(ì´ë¯¸ì§€ëŒ€) í¬í•¨", variable=self.use_thumbnail_var)
        chk_thumb.pack(side='left', padx=20)
        ToolTip(chk_thumb, "ì²´í¬ ì‹œ: ëŒ€í‘œ ì´ë¯¸ì§€(ì´ë¯¸ì§€ëŒ€)ë„ AIì—ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.\ní•´ì œ ì‹œ: ìƒì„¸ í˜ì´ì§€ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")

        chk_url = ttk.Checkbutton(fr2, text=" URL ì´ë¯¸ì§€ í—ˆìš©", variable=self.allow_url_var)
        chk_url.pack(side='left', padx=20)
        ToolTip(chk_url, "ì²´í¬ ì‹œ: ì›¹ ë§í¬(http) ì´ë¯¸ì§€ë„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.\ní•´ì œ ì‹œ: ë¡œì»¬ íŒŒì¼ ê²½ë¡œë§Œ ì¸ì‹í•©ë‹ˆë‹¤.")
        
        # ìƒì„¸ì´ë¯¸ì§€ ê°œìˆ˜ ì œí•œ ì˜µì…˜
        fr3 = ttk.Frame(f_opt)
        fr3.pack(fill='x', pady=5)
        ttk.Label(fr3, text="ìƒì„¸ì´ë¯¸ì§€ ê°œìˆ˜ ì œí•œ:", width=18).pack(side='left')
        spin_max_detail = ttk.Spinbox(fr3, from_=1, to=100, textvariable=self.max_detail_images_var, width=10)
        spin_max_detail.pack(side='left', padx=5)
        ToolTip(spin_max_detail, "ìƒì„¸ì´ë¯¸ì§€_1ë¶€í„° ì§€ì •í•œ ê°œìˆ˜ê¹Œì§€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\nì˜ˆ: 10ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ìƒì„¸ì´ë¯¸ì§€_1~ìƒì„¸ì´ë¯¸ì§€_10ê¹Œì§€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\në°°ì¹˜ ìš©ëŸ‰ê³¼ í† í° ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•œ ì˜µì…˜ì…ë‹ˆë‹¤.")
        ttk.Label(fr3, text="ê°œ (ê¸°ë³¸ê°’: 10)", font=("ë§‘ì€ ê³ ë”•", 9)).pack(side='left', padx=5)
        
        # ìƒì„¸ì´ë¯¸ì§€ í†µê³„ í‘œì‹œ
        fr3_stats = ttk.Frame(f_opt)
        fr3_stats.pack(fill='x', pady=(5, 5))
        
        # í†µê³„ í‘œì‹œ ì˜ì—­ (ë” ëˆˆì— ë„ê²Œ)
        self.detail_stats_frame = tk.Frame(fr3_stats, bg="#E3F2FD", relief="solid", bd=1, padx=10, pady=8)
        self.detail_stats_frame.pack(fill='x', padx=5)
        
        self.detail_stats_label = tk.Label(self.detail_stats_frame, 
                                           text="ğŸ“Š ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ë©´ ìƒì„¸ì´ë¯¸ì§€ í†µê³„ê°€ í‘œì‹œë©ë‹ˆë‹¤.", 
                                           font=("ë§‘ì€ ê³ ë”•", 10, "bold"), 
                                           bg="#E3F2FD",
                                           fg="#1976D2",
                                           anchor="w",
                                           justify="left")
        self.detail_stats_label.pack(side='left', fill='x', expand=True)
        
        # í†µê³„ ìƒì„¸ í™•ì¸ ë²„íŠ¼
        btn_stats = ttk.Button(self.detail_stats_frame, text="ğŸ“Š í†µê³„ ìƒì„¸ í™•ì¸", command=self._show_detail_stats_popup, width=18)
        btn_stats.pack(side='right', padx=(10, 0))
        ToolTip(btn_stats, "ìƒì„¸ì´ë¯¸ì§€ í†µê³„ë¥¼ íŒì—… ì°½ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.")
        
        # Spinbox ê°’ ë³€ê²½ ì‹œ í†µê³„ ì—…ë°ì´íŠ¸
        self.max_detail_images_var.trace('w', lambda *args: self._update_detail_stats_display())
        
        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ ì˜µì…˜
        fr_resize = ttk.Frame(f_opt)
        fr_resize.pack(fill='x', pady=5)
        ttk.Label(fr_resize, text="ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ:", width=18).pack(side='left')
        resize_mode_combo = ttk.Combobox(
            fr_resize, 
            textvariable=self.resize_mode_var, 
            values=["A", "B", "C"], 
            state="readonly", 
            width=8
        )
        resize_mode_combo.pack(side='left', padx=5)
        ttk.Label(
            fr_resize, 
            text="(A: ê¸°ë³¸/ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨, B: ê°€ë¡œ 512px, C: ê°€ë¡œ 448px)", 
            font=("ë§‘ì€ ê³ ë”•", 9)
        ).pack(side='left', padx=5)
        ToolTip(
            resize_mode_combo, 
            "ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ ì„ íƒ:\n"
            "â€¢ A (ê¸°ë³¸): ë¦¬ì‚¬ì´ì¦ˆ í•˜ì§€ ì•ŠìŒ (í˜„í–‰ ìœ ì§€)\n"
            "â€¢ B (512px): ê°€ë¡œ 512pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€, í¬ë¡­/íŒ¨ë”© ê¸ˆì§€\n"
            "â€¢ C (448px): ê°€ë¡œ 448pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€, í¬ë¡­/íŒ¨ë”© ê¸ˆì§€\n\n"
            "ëª¨ë“  ëª¨ë“œëŠ” ê³ í’ˆì§ˆ ë¦¬ìƒ˜í”Œë§(Lanczos) ì‚¬ìš©"
        )

        # Step 3: ì‹¤í–‰
        f_step3 = ttk.LabelFrame(container, text="3. ì‹¤í–‰", padding=15)
        f_step3.pack(fill='x', pady=15)
        
        # í†µí•© ë²„íŠ¼ (ê¸°ì¡´)
        btn_run = ttk.Button(f_step3, text="ğŸš€ JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (Start Batch)", command=self._start_create_batch, style="Success.TButton")
        btn_run.pack(fill='x', ipady=8, pady=(0, 10))
        ToolTip(btn_run, "1. ì—‘ì…€ ì½ê¸° (ì´ë¯¸ì§€ í¬í•¨)\n2. JSONL ìƒì„±\n3. ë°°ì¹˜ ì‹œì‘ ìš”ì²­ (24ì‹œê°„ ë‚´ ì™„ë£Œ)")
        
        # ë¶„ë¦¬ëœ ë²„íŠ¼ë“¤ (í…ŒìŠ¤íŠ¸ìš©)
        f_step3_separated = ttk.Frame(f_step3)
        f_step3_separated.pack(fill='x', pady=(5, 0))
        
        btn_jsonl_only = ttk.Button(f_step3_separated, text="ğŸ“ JSONL ìƒì„±ë§Œ (í…ŒìŠ¤íŠ¸ìš©)", command=self._start_create_jsonl_only, style="Primary.TButton")
        btn_jsonl_only.pack(side='left', fill='x', expand=True, padx=(0, 5), ipady=6)
        ToolTip(btn_jsonl_only, "ì—‘ì…€ íŒŒì¼ì„ ì½ì–´ì„œ JSONL íŒŒì¼ë§Œ ìƒì„±í•©ë‹ˆë‹¤.\nìƒì„±ëœ JSONL íŒŒì¼ì„ í™•ì¸í•œ í›„, 'ë°°ì¹˜ ì—…ë¡œë“œë§Œ' ë²„íŠ¼ìœ¼ë¡œ ë°°ì¹˜ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        btn_upload_only = ttk.Button(f_step3_separated, text="ğŸ“¤ ë°°ì¹˜ ì—…ë¡œë“œë§Œ (JSONL íŒŒì¼ ì„ íƒ)", command=self._start_upload_batch_only, style="Primary.TButton")
        btn_upload_only.pack(side='left', fill='x', expand=True, padx=(5, 0), ipady=6)
        ToolTip(btn_upload_only, "ì´ë¯¸ ìƒì„±ëœ JSONL íŒŒì¼ì„ ì„ íƒí•˜ì—¬ ë°°ì¹˜ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n'JSONL ìƒì„±ë§Œ' ë²„íŠ¼ìœ¼ë¡œ ë¨¼ì € JSONLì„ ìƒì„±í•œ í›„ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        ttk.Label(container, text="â€» ë°°ì¹˜ APIëŠ” ê²°ê³¼ ìˆ˜ì‹ ê¹Œì§€ ìµœëŒ€ 24ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. (ë¹„ìš© 50% ì ˆê°)", foreground="#666").pack()

    def _select_src_file(self):
        p = filedialog.askopenfilename(
            title="Stage2 ì—‘ì…€ ì„ íƒ (T1 ë²„ì „ë§Œ ê°€ëŠ¥)",
            filetypes=[("Excel", "*.xlsx;*.xls")]
        )
        if p:
            # T1 í¬í•¨ ì—¬ë¶€ ê²€ì¦
            base_name = os.path.splitext(os.path.basename(p))[0]
            if not re.search(r"_T1_[Ii]\d+", base_name, re.IGNORECASE):
                messagebox.showerror(
                    "ì˜¤ë¥˜", 
                    f"ì´ ë„êµ¬ëŠ” T1 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                    f"ì„ íƒí•œ íŒŒì¼: {os.path.basename(p)}\n"
                    f"íŒŒì¼ëª…ì— '_T1_I*' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
                )
                return
            
            # ì—‘ì…€ íŒŒì¼ì„ ì½ì–´ì„œ ìƒì„¸ì´ë¯¸ì§€ í†µê³„ ê³„ì‚°
            self._analyze_detail_images(p)
            
            self.src_file_var.set(p)
    
    def _analyze_detail_images(self, excel_path):
        """ì—‘ì…€ íŒŒì¼ì˜ ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            df = pd.read_excel(excel_path)
            
            # ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼ ì°¾ê¸°
            detail_cols = [c for c in df.columns if str(c).startswith("ìƒì„¸ì´ë¯¸ì§€_")]
            if detail_cols:
                def sort_key(c):
                    try:
                        return int(str(c).split("_")[1])
                    except Exception:
                        return 9999
                detail_cols.sort(key=sort_key)
                max_col_num = max([int(str(c).split("_")[1]) for c in detail_cols if str(c).split("_")[1].isdigit()], default=0)
            else:
                max_col_num = 0
            
            # ê° í–‰ì—ì„œ ì‹¤ì œë¡œ ê°’ì´ ìˆëŠ” ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼ ê°œìˆ˜ ê³„ì‚°
            row_counts = {}  # {ì»¬ëŸ¼ê°œìˆ˜: í–‰ê°œìˆ˜}
            for idx, row in df.iterrows():
                count = 0
                for col in detail_cols:
                    val = row.get(col, "")
                    if pd.notna(val) and str(val).strip():
                        count += 1
                if count > 0:
                    row_counts[count] = row_counts.get(count, 0) + 1
            
            # í†µê³„ ì €ì¥
            self.detail_image_stats = {
                "max_columns": max_col_num,
                "row_counts": row_counts,
            }
            
            # í†µê³„ í‘œì‹œ ì—…ë°ì´íŠ¸
            self._update_detail_stats_display()
            
            # íŒŒì¼ ì„ íƒ ì‹œ í†µê³„ë¥¼ ë©”ì‹œì§€ë°•ìŠ¤ë¡œë„ í‘œì‹œ (ì„ íƒì )
            if max_col_num > 0:
                rows_exceeding = sum(count for col_count, count in row_counts.items() if col_count > self.max_detail_images_var.get())
                if rows_exceeding > 0:
                    # ì„¤ì •ê°’ ì´ˆê³¼ í–‰ì´ ìˆìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€
                    messagebox.showinfo(
                        "ìƒì„¸ì´ë¯¸ì§€ í†µê³„",
                        f"ğŸ“Š ì—‘ì…€ íŒŒì¼ ë¶„ì„ ì™„ë£Œ\n\n"
                        f"ìµœëŒ€ ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼: {max_col_num}ê°œ\n"
                        f"í˜„ì¬ ì„¤ì •ê°’: {self.max_detail_images_var.get()}ê°œ\n"
                        f"âš ï¸ ì„¤ì •ê°’ ì´ˆê³¼ í–‰: {rows_exceeding}ê°œ\n\n"
                        f"ìƒì„¸ í†µê³„ëŠ” ì˜µì…˜ ì„¤ì • ì˜ì—­ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
                else:
                    # ëª¨ë“  í–‰ì´ ì„¤ì •ê°’ ì´í•˜ë©´ ì •ë³´ ë©”ì‹œì§€
                    messagebox.showinfo(
                        "ìƒì„¸ì´ë¯¸ì§€ í†µê³„",
                        f"ğŸ“Š ì—‘ì…€ íŒŒì¼ ë¶„ì„ ì™„ë£Œ\n\n"
                        f"ìµœëŒ€ ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼: {max_col_num}ê°œ\n"
                        f"í˜„ì¬ ì„¤ì •ê°’: {self.max_detail_images_var.get()}ê°œ\n"
                        f"âœ… ëª¨ë“  í–‰ì´ ì„¤ì •ê°’ ì´í•˜ì…ë‹ˆë‹¤.\n\n"
                        f"ìƒì„¸ í†µê³„ëŠ” ì˜µì…˜ ì„¤ì • ì˜ì—­ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
            
        except Exception as e:
            self.append_log(f"[WARN] ìƒì„¸ì´ë¯¸ì§€ í†µê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.detail_image_stats = {"max_columns": 0, "row_counts": {}}
            self._update_detail_stats_display()
    
    def _update_detail_stats_display(self):
        """ìƒì„¸ì´ë¯¸ì§€ í†µê³„ë¥¼ í™”ë©´ì— í‘œì‹œí•©ë‹ˆë‹¤."""
        stats = self.detail_image_stats
        max_cols = stats.get("max_columns", 0)
        row_counts = stats.get("row_counts", {})
        max_limit = self.max_detail_images_var.get()
        
        if max_cols == 0:
            self.detail_stats_label.config(
                text="ğŸ“Š ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ë©´ ìƒì„¸ì´ë¯¸ì§€ í†µê³„ê°€ í‘œì‹œë©ë‹ˆë‹¤.",
                fg="#666",
                bg="#E3F2FD"
            )
            return
        
        # ì„¤ì •í•œ ê°œìˆ˜ë³´ë‹¤ ë§ì€ ìƒì„¸ì´ë¯¸ì§€ë¥¼ ê°€ì§„ í–‰ ê°œìˆ˜ ê³„ì‚°
        rows_exceeding_limit = 0
        for col_count, row_count in row_counts.items():
            if col_count > max_limit:
                rows_exceeding_limit += row_count
        
        # í†µê³„ ë©”ì‹œì§€ ìƒì„±
        if rows_exceeding_limit > 0:
            stats_text = (
                f"ğŸ“Š ìµœëŒ€ ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼: {max_cols}ê°œ | "
                f"âš ï¸ {max_limit}ê°œ ì´ˆê³¼ í–‰: {rows_exceeding_limit}ê°œ"
            )
            color = "#d32f2f"  # ë¹¨ê°„ìƒ‰ (ê²½ê³ )
            bg_color = "#FFEBEE"  # ì—°í•œ ë¹¨ê°„ìƒ‰ ë°°ê²½
        else:
            stats_text = f"ğŸ“Š ìµœëŒ€ ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼: {max_cols}ê°œ | âœ… ëª¨ë“  í–‰ì´ {max_limit}ê°œ ì´í•˜"
            color = "#388e3c"  # ì´ˆë¡ìƒ‰ (ì •ìƒ)
            bg_color = "#E8F5E9"  # ì—°í•œ ì´ˆë¡ìƒ‰ ë°°ê²½
        
        # ìƒì„¸ í†µê³„ ì¶”ê°€ (ì˜µì…˜)
        if row_counts:
            # ê°€ì¥ ë§ì€ ìƒì„¸ì´ë¯¸ì§€ë¥¼ ê°€ì§„ í–‰ì˜ ê°œìˆ˜
            max_row_count = max(row_counts.keys(), default=0)
            if max_row_count > max_limit:
                stats_text += f" (ìµœëŒ€ {max_row_count}ê°œ ìƒì„¸ì´ë¯¸ì§€ í–‰ í¬í•¨)"
        
        # ë°°ê²½ìƒ‰ë„ í•¨ê»˜ ì—…ë°ì´íŠ¸
        self.detail_stats_label.config(text=stats_text, fg=color, bg=bg_color)
        # í”„ë ˆì„ ë°°ê²½ìƒ‰ë„ ì—…ë°ì´íŠ¸
        if hasattr(self, 'detail_stats_frame'):
            self.detail_stats_frame.config(bg=bg_color)
    
    def _show_detail_stats_popup(self):
        """ìƒì„¸ì´ë¯¸ì§€ í†µê³„ë¥¼ íŒì—… ì°½ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""
        stats = self.detail_image_stats
        max_cols = stats.get("max_columns", 0)
        row_counts = stats.get("row_counts", {})
        max_limit = self.max_detail_images_var.get()
        
        if max_cols == 0:
            messagebox.showinfo("ìƒì„¸ì´ë¯¸ì§€ í†µê³„", "ì—‘ì…€ íŒŒì¼ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # í†µê³„ ë©”ì‹œì§€ ìƒì„±
        msg_lines = [
            "ğŸ“Š ìƒì„¸ì´ë¯¸ì§€ í†µê³„",
            "",
            f"ìµœëŒ€ ìƒì„¸ì´ë¯¸ì§€ ì»¬ëŸ¼ ê°œìˆ˜: {max_cols}ê°œ",
            f"í˜„ì¬ ì„¤ì •ê°’: {max_limit}ê°œ",
            "",
        ]
        
        # ì„¤ì •ê°’ ì´ˆê³¼ í–‰ ê°œìˆ˜
        rows_exceeding = sum(count for col_count, count in row_counts.items() if col_count > max_limit)
        if rows_exceeding > 0:
            msg_lines.append(f"âš ï¸ {max_limit}ê°œ ì´ˆê³¼ í–‰: {rows_exceeding}ê°œ")
            msg_lines.append("")
        
        # ìƒì„¸ í†µê³„ (ì»¬ëŸ¼ ê°œìˆ˜ë³„ í–‰ ê°œìˆ˜)
        if row_counts:
            msg_lines.append("ìƒì„¸ì´ë¯¸ì§€ ê°œìˆ˜ë³„ í–‰ ë¶„í¬:")
            sorted_counts = sorted(row_counts.items(), key=lambda x: x[0], reverse=True)
            for col_count, row_count in sorted_counts[:20]:  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
                marker = "âš ï¸" if col_count > max_limit else "  "
                msg_lines.append(f"  {marker} {col_count}ê°œ ìƒì„¸ì´ë¯¸ì§€: {row_count}ê°œ í–‰")
            if len(sorted_counts) > 20:
                msg_lines.append(f"  ... (ì´ {len(sorted_counts)}ê°œ ê·¸ë£¹)")
        
        messagebox.showinfo("ìƒì„¸ì´ë¯¸ì§€ í†µê³„", "\n".join(msg_lines))

    def _start_create_batch(self):
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Key í•„ìš”")
            return
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ ì„ íƒ í•„ìš”")
            return
        
        # T1 í¬í•¨ ì—¬ë¶€ ê²€ì¦
        src = self.src_file_var.get().strip()
        base_name = os.path.splitext(os.path.basename(src))[0]
        if not re.search(r"_T1_[Ii]\d+", base_name, re.IGNORECASE):
            messagebox.showerror(
                "ì˜¤ë¥˜", 
                f"ì´ ë„êµ¬ëŠ” T1 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                f"ì„ íƒí•œ íŒŒì¼: {os.path.basename(src)}\n"
                f"íŒŒì¼ëª…ì— '_T1_I*' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
            )
            return
        
        t = threading.Thread(target=self._run_create_batch)
        t.daemon = True
        t.start()

    def _start_create_jsonl_only(self):
        """JSONL ìƒì„±ë§Œ ìˆ˜í–‰ (í…ŒìŠ¤íŠ¸ìš©)"""
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ ì„ íƒ í•„ìš”")
            return
        
        # T1 í¬í•¨ ì—¬ë¶€ ê²€ì¦
        src = self.src_file_var.get().strip()
        base_name = os.path.splitext(os.path.basename(src))[0]
        if not re.search(r"_T1_[Ii]\d+", base_name, re.IGNORECASE):
            messagebox.showerror(
                "ì˜¤ë¥˜", 
                f"ì´ ë„êµ¬ëŠ” T1 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                f"ì„ íƒí•œ íŒŒì¼: {os.path.basename(src)}\n"
                f"íŒŒì¼ëª…ì— '_T1_I*' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
            )
            return
        
        t = threading.Thread(target=self._run_create_jsonl_only)
        t.daemon = True
        t.start()

    def _start_upload_batch_only(self):
        """ë°°ì¹˜ ì—…ë¡œë“œë§Œ ìˆ˜í–‰ (JSONL íŒŒì¼ ì„ íƒ)"""
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Key í•„ìš”")
            return
        
        # JSONL íŒŒì¼ ì„ íƒ
        jsonl_path = filedialog.askopenfilename(
            title="ë°°ì¹˜ ì—…ë¡œë“œí•  JSONL íŒŒì¼ ì„ íƒ",
            filetypes=[("JSONL íŒŒì¼", "*.jsonl"), ("ëª¨ë“  íŒŒì¼", "*.*")]
        )
        
        if not jsonl_path:
            return
        
        # ì›ë³¸ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ ì¶”ë¡  (JSONL íŒŒì¼ëª…ì—ì„œ)
        base_name = os.path.splitext(os.path.basename(jsonl_path))[0]
        # ì˜ˆ: "íŒŒì¼ëª…_stage2_batch_input.jsonl" -> "íŒŒì¼ëª….xlsx"
        excel_path = jsonl_path.replace("_stage2_batch_input.jsonl", ".xlsx")
        if not os.path.exists(excel_path):
            # ì—‘ì…€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë´„
            excel_path = filedialog.askopenfilename(
                title="ì›ë³¸ ì—‘ì…€ íŒŒì¼ ì„ íƒ (ë°°ì¹˜ ì´ë ¥ ê¸°ë¡ìš©)",
                filetypes=[("Excel íŒŒì¼", "*.xlsx;*.xls"), ("ëª¨ë“  íŒŒì¼", "*.*")]
            )
            if not excel_path:
                return
        
        t = threading.Thread(target=self._run_upload_batch_only, args=(jsonl_path, excel_path))
        t.daemon = True
        t.start()

    def _run_create_jsonl_only(self):
        """JSONL íŒŒì¼ë§Œ ìƒì„± (ë°°ì¹˜ ì—…ë¡œë“œëŠ” í•˜ì§€ ì•ŠìŒ)"""
        src = self.src_file_var.get().strip()
        model = self.model_var.get().strip() or "gpt-5-mini"
        effort = self.effort_var.get().strip() or "medium"

        use_thumb = self.use_thumbnail_var.get()
        allow_url = self.allow_url_var.get()
        max_detail_images = self.max_detail_images_var.get()
        resize_mode = self.resize_mode_var.get().strip() or "A"

        try:
            if create_stage2_batch_input_jsonl is None:
                raise RuntimeError("create_stage2_batch_input_jsonl í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            self.append_log(f"[RUN] Stage2: JSONL ìƒì„±ë§Œ ìˆ˜í–‰ â†’ {os.path.basename(src)}")
            self.append_log(f"[INFO] ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ë²„ì „ (Cachever) ì‚¬ìš© ì¤‘")
            if CACHE_MODE_CORE:
                self.append_log(f"[INFO] âœ… stage2_core_Cache.py ë¡œë“œ ì™„ë£Œ (í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ìµœì í™”)")
            else:
                self.append_log(f"[WARN] âš ï¸ stage2_core_Cache.py ë¡œë“œ ì‹¤íŒ¨ - ìºì‹±ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            if CACHE_MODE_BATCH:
                self.append_log(f"[INFO] âœ… stage2_batch_api_ê¸°ì¡´gpt.py ìºì‹± ëª¨ë“œ í™œì„±í™” (prompt_cache_key ì‚¬ìš©)")
                self.append_log(f"[INFO] ğŸ’° í† í° ë¹„ìš© ìµœëŒ€ 90% ì ˆê° ê°€ëŠ¥ (ìºì‹œ íˆíŠ¸ìœ¨ì— ë”°ë¼ ë‹¤ë¦„)")
            else:
                self.append_log(f"[WARN] âš ï¸ stage2_batch_api_ê¸°ì¡´gpt.py ìºì‹± ëª¨ë“œ ë¹„í™œì„±í™” - prompt_cache_keyê°€ ì¶”ê°€ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            self.append_log(f"[INFO] ìƒì„¸ì´ë¯¸ì§€ ê°œìˆ˜ ì œí•œ: {max_detail_images}ê°œ (ìƒì„¸ì´ë¯¸ì§€_1 ~ ìƒì„¸ì´ë¯¸ì§€_{max_detail_images})")
            
            # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ ì„¤ëª…
            resize_mode_desc = {
                "A": "ê¸°ë³¸ ëª¨ë“œ (ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨)",
                "B": "ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ 512px (ê°€ë¡œ 512pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€)",
                "C": "ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ 448px (ê°€ë¡œ 448pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€)",
            }.get(resize_mode, f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {resize_mode}")
            self.append_log(f"[INFO] ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ: {resize_mode} ({resize_mode_desc})")

            # ì—‘ì…€ â†’ JSONL ìƒì„±
            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_stage2_batch_input.jsonl"

            # í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ í™•ì¸ í›„ íŒŒë¼ë¯¸í„° ì „ë‹¬
            import inspect
            sig = inspect.signature(create_stage2_batch_input_jsonl)
            params = list(sig.parameters.keys())
            
            call_kwargs = {
                "excel_path": src,
                "jsonl_path": jsonl_path,
                "model_name": model,
                "effort": effort,
                "skip_filled": self.skip_exist_var.get(),
                "use_thumbnail": use_thumb,
                "allow_url": allow_url,
                "log_func": self.append_log,
            }
            
            if "max_detail_images" in params:
                call_kwargs["max_detail_images"] = max_detail_images
            
            if "resize_mode" in params:
                call_kwargs["resize_mode"] = resize_mode
            
            info = create_stage2_batch_input_jsonl(**call_kwargs)

            self.append_log(
                f"[DONE] ìš”ì²­ JSONL ìƒì„±: total_rows={info['total_rows']}, "
                f"target_rows={info['target_rows']}, num_requests={info['num_requests']}"
            )

            # JSONL íŒŒì¼ í¬ê¸° í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ì²˜ë¦¬ ê±´ìˆ˜(í–‰): {info['num_requests']}ê°œ")
            self.append_log(f"[INFO] JSONL íŒŒì¼ ê²½ë¡œ: {jsonl_path}")
            self.append_log(f"[INFO] âœ… JSONL ìƒì„± ì™„ë£Œ! ì´ì œ 'ë°°ì¹˜ ì—…ë¡œë“œë§Œ' ë²„íŠ¼ìœ¼ë¡œ ë°°ì¹˜ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # íŒŒì¼ ì—´ê¸° ì˜µì…˜ ì œê³µ
            self.after(0, lambda: messagebox.showinfo(
                "JSONL ìƒì„± ì™„ë£Œ",
                f"JSONL íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                f"íŒŒì¼: {os.path.basename(jsonl_path)}\n"
                f"í¬ê¸°: {jsonl_size_mb:.2f} MB\n"
                f"ìš”ì²­ ìˆ˜: {info['num_requests']}ê°œ\n\n"
                f"íŒŒì¼ì„ í™•ì¸í•œ í›„, 'ë°°ì¹˜ ì—…ë¡œë“œë§Œ' ë²„íŠ¼ìœ¼ë¡œ ë°°ì¹˜ë¥¼ ìƒì„±í•˜ì„¸ìš”."
            ))

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            self.after(0, lambda: messagebox.showerror("ì—ëŸ¬", str(e)))

    def _run_upload_batch_only(self, jsonl_path: str, excel_path: str):
        """JSONL íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ë§Œ ì—…ë¡œë“œ"""
        key = self.api_key_var.get().strip()
        model = self.model_var.get().strip() or "gpt-5-mini"
        effort = self.effort_var.get().strip() or "medium"

        try:
            if create_batch_from_jsonl is None:
                raise RuntimeError("create_batch_from_jsonl í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            self.append_log(f"[RUN] ë°°ì¹˜ ì—…ë¡œë“œ ì‹œì‘ â†’ {os.path.basename(jsonl_path)}")
            
            # JSONL íŒŒì¼ í¬ê¸° í™•ì¸
            if not os.path.exists(jsonl_path):
                raise FileNotFoundError(f"JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")
            
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB")
            
            # ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ : 180MB ì´ìƒì´ë©´ ë¶„í•  ì²˜ë¦¬
            MAX_FILE_SIZE_MB = 180
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸°ê°€ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤: {jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB")
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)
                client_with_timeout = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch_ids = self._create_batch_chunks(
                    client=client_with_timeout,
                    jsonl_path=jsonl_path,
                    excel_path=excel_path,
                    model_name=model,
                    effort=effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=999999,
                    resize_mode=None,  # ì´ë¯¸ ìƒì„±ëœ JSONL ì—…ë¡œë“œ ì‹œì—ëŠ” None (JSONL íŒŒì¼ì—ì„œ ì¶”ë¡  ê°€ëŠ¥í•˜ì§€ë§Œ ì¼ë‹¨ None)
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                self.after(0, lambda: messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}"))
            else:
                # ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)
                client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch = create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=excel_path,
                    model_name=model,
                    log_func=self.append_log,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # ì‘ì—… ì´ë ¥ ê¸°ë¡ (resize_modeëŠ” JSONL íŒŒì¼ì—ì„œ ì¶”ë¡  ë¶ˆê°€í•˜ë¯€ë¡œ Noneìœ¼ë¡œ ì„¤ì •)
                # resize_modeëŠ” JSONL ìƒì„± ì‹œì—ë§Œ ì €ì¥ë˜ë©°, ì´ë¯¸ ìƒì„±ëœ JSONL ì—…ë¡œë“œ ì‹œì—ëŠ” custom_idì—ì„œ ì¶”ì¶œ ê°€ëŠ¥
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=excel_path,
                    jsonl_path=jsonl_path,
                    model=model,
                    effort=effort,
                    status=batch.status,
                    output_file_id=None,
                    resize_mode=None,  # ì´ë¯¸ ìƒì„±ëœ JSONL ì—…ë¡œë“œ ì‹œì—ëŠ” None
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
                try:
                    root_name = get_root_filename(excel_path)
                    JobManager.update_status(root_name, text_msg="T2 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T2 (ì§„í–‰ì¤‘)")
                except Exception:
                    pass
                
                self.after(0, lambda: messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {batch_id}"))
            
            self._load_jobs_all()
            self._load_archive_list()

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            self.after(0, lambda: messagebox.showerror("ì—ëŸ¬", str(e)))

    def _run_create_batch(self):
        """
        1ë‹¨ê³„: ì—‘ì…€ â†’ /v1/responsesìš© JSONL ìƒì„± (ê¸°ì¡´ ì•ˆì • ë²„ì „ ì½”ì–´ ì‚¬ìš©)
        2ë‹¨ê³„: JSONL ì—…ë¡œë“œ â†’ Batch ìƒì„±
        3ë‹¨ê³„: í˜„ì¬ GUIì˜ ì‘ì—… ì´ë ¥ì— ê¸°ë¡
        """
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        model = self.model_var.get().strip() or "gpt-5-mini"
        effort = self.effort_var.get().strip() or "medium"

        use_thumb = self.use_thumbnail_var.get()
        allow_url = self.allow_url_var.get()
        max_detail_images = self.max_detail_images_var.get()
        resize_mode = self.resize_mode_var.get().strip() or "A"

        try:
            if create_stage2_batch_input_jsonl is None or create_batch_from_jsonl is None:
                raise RuntimeError("stage2_batch_api_ê¸°ì¡´gpt ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ Batch ì½”ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            self.append_log(f"[RUN] Stage2: JSONL ìƒì„± + Batch ìƒì„± ì‹œì‘ â†’ {os.path.basename(src)}")
            self.append_log(f"[INFO] ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ë²„ì „ (Cachever) ì‚¬ìš© ì¤‘")
            if CACHE_MODE_CORE:
                self.append_log(f"[INFO] âœ… stage2_core_Cache.py ë¡œë“œ ì™„ë£Œ (í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ìµœì í™”)")
            else:
                self.append_log(f"[WARN] âš ï¸ stage2_core_Cache.py ë¡œë“œ ì‹¤íŒ¨ - ìºì‹±ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            if CACHE_MODE_BATCH:
                self.append_log(f"[INFO] âœ… stage2_batch_api_ê¸°ì¡´gpt.py ìºì‹± ëª¨ë“œ í™œì„±í™” (prompt_cache_key ì‚¬ìš©)")
                self.append_log(f"[INFO] ğŸ’° í† í° ë¹„ìš© ìµœëŒ€ 90% ì ˆê° ê°€ëŠ¥ (ìºì‹œ íˆíŠ¸ìœ¨ì— ë”°ë¼ ë‹¤ë¦„)")
            else:
                self.append_log(f"[WARN] âš ï¸ stage2_batch_api_ê¸°ì¡´gpt.py ìºì‹± ëª¨ë“œ ë¹„í™œì„±í™” - prompt_cache_keyê°€ ì¶”ê°€ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            self.append_log(f"[INFO] ìƒì„¸ì´ë¯¸ì§€ ê°œìˆ˜ ì œí•œ: {max_detail_images}ê°œ (ìƒì„¸ì´ë¯¸ì§€_1 ~ ìƒì„¸ì´ë¯¸ì§€_{max_detail_images})")
            
            # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ ì„¤ëª…
            resize_mode_desc = {
                "A": "ê¸°ë³¸ ëª¨ë“œ (ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨)",
                "B": "ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ 512px (ê°€ë¡œ 512pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€)",
                "C": "ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ 448px (ê°€ë¡œ 448pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€)",
            }.get(resize_mode, f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {resize_mode}")
            self.append_log(f"[INFO] ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ: {resize_mode} ({resize_mode_desc})")

            # 1) ì—‘ì…€ â†’ JSONL ìƒì„±
            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_stage2_batch_input.jsonl"

            # í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ í™•ì¸ í›„ íŒŒë¼ë¯¸í„° ì „ë‹¬ (ìºì‹œ ë¬¸ì œ ëŒ€ì‘)
            import inspect
            sig = inspect.signature(create_stage2_batch_input_jsonl)
            params = list(sig.parameters.keys())
            
            # max_detail_images íŒŒë¼ë¯¸í„° ì „ë‹¬
            call_kwargs = {
                "excel_path": src,
                "jsonl_path": jsonl_path,
                "model_name": model,
                "effort": effort,
                "skip_filled": self.skip_exist_var.get(),
                "use_thumbnail": use_thumb,
                "allow_url": allow_url,
                "log_func": self.append_log,
            }
            
            if "max_detail_images" in params:
                call_kwargs["max_detail_images"] = max_detail_images
            
            if "resize_mode" in params:
                call_kwargs["resize_mode"] = resize_mode
            
            info = create_stage2_batch_input_jsonl(**call_kwargs)

            self.append_log(
                f"[DONE] ìš”ì²­ JSONL ìƒì„±: total_rows={info['total_rows']}, "
                f"target_rows={info['target_rows']}, num_requests={info['num_requests']}"
            )

            # 2) ë°°ì¹˜ íŒŒì¼ í¬ê¸° í™•ì¸ ë° ë¶„í•  ì²˜ë¦¬
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ì²˜ë¦¬ ê±´ìˆ˜(í–‰): {info['num_requests']}ê°œ")
            
            # ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ : 180MB ì´ìƒì´ë©´ ë¶„í•  ì²˜ë¦¬ (OpenAI Batch API ì œí•œ: 200MB)
            # ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨ (500ê°œ ì œí•œ ì œê±°)
            MAX_FILE_SIZE_MB = 180
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸°ê°€ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤: {jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client_with_timeout = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch_ids = self._create_batch_chunks(
                    client=client_with_timeout,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model,
                    effort=effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=999999,  # ìš”ì²­ ìˆ˜ ì œí•œ ê±°ì˜ ì œê±° (ìš©ëŸ‰ì´ ìš°ì„ )
                    resize_mode=resize_mode,
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}")
            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch = create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model,
                    log_func=self.append_log,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # 3) ì‘ì—… ì´ë ¥ ê¸°ë¡ (resize_mode í¬í•¨)
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src,
                    jsonl_path=jsonl_path,
                    model=model,
                    effort=effort,
                    status=batch.status,
                    output_file_id=None,
                    resize_mode=resize_mode,
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage2(Text) ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡: T2 (ì§„í–‰ì¤‘) (img ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                try:
                    root_name = get_root_filename(src)
                    JobManager.update_status(root_name, text_msg="T2 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T2 (ì§„í–‰ì¤‘)")
                except Exception:
                    # ëŸ°ì²˜ë‚˜ job_history.json ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ìš©íˆ ë¬´ì‹œ
                    pass
                messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {batch_id}")
            
            self._load_jobs_all()
            self._load_archive_list()

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            messagebox.showerror("ì—ëŸ¬", str(e))
    
    def _create_batch_chunks(self, client, jsonl_path, excel_path, model_name, effort, max_size_mb=180, max_requests=999999, resize_mode=None):
        """
        í° JSONL íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ batch_group_idë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        """
        import json
        import uuid
        
        # ë°°ì¹˜ ê·¸ë£¹ ID ìƒì„± (ê°™ì€ ì—‘ì…€ì—ì„œ ë¶„í• ëœ ë°°ì¹˜ë“¤ì„ ë¬¶ìŒ)
        batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
        
        # JSONL íŒŒì¼ ì½ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ê³ ë ¤, í•˜ì§€ë§Œ í˜„ì¬ëŠ” ì „ì²´ ë¡œë“œ)
        requests = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    requests.append(json.loads(line))
        
        total_requests = len(requests)
        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (ìš©ëŸ‰ ê¸°ì¤€ë§Œ ì‚¬ìš©, ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
        original_file_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
        estimated_total_chunks = max(1, int(original_file_size_mb / max_size_mb) + 1)
        
        self.append_log(f"[INFO] ì´ {total_requests}ê±´(í–‰)ì„ ë¶„í• í•©ë‹ˆë‹¤... (ì˜ˆìƒ: ì•½ {estimated_total_chunks}ê°œ ì²­í¬, ê·¸ë£¹ ID: {batch_group_id})")
        
        # base ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜ (ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹œ ì‚¬ìš©)
        base, ext = os.path.splitext(jsonl_path)
        
        batch_ids = []
        chunk_num = 0
        chunk_files_created = []  # ìƒì„±ëœ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì •ë¦¬ìš©)
        failed_chunk_files = []  # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì¬ì‹œë„ìš©)
        
        i = 0
        while i < total_requests:
            chunk_num += 1
            chunk_requests = []
            chunk_size_bytes = 0  # ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì •í™•íˆ ê³„ì‚°
            
            # ì²­í¬ ìƒì„± (ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ , ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
            # ì‹¤ì œ íŒŒì¼ í¬ê¸°ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ JSON ì§ë ¬í™” + ì¤„ë°”ê¿ˆ ë¬¸ì ê³ ë ¤
            while i < total_requests:
                req_json = json.dumps(requests[i], ensure_ascii=False)
                req_size_bytes = len(req_json.encode('utf-8')) + 1  # +1 for newline
                
                # ìš©ëŸ‰ì´ ìš°ì„ : ë‹¤ìŒ ìš”ì²­ì„ ì¶”ê°€í•˜ë©´ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ ë§ˆì§„ 5% í¬í•¨)
                if chunk_size_bytes + req_size_bytes > max_size_mb * 1024 * 1024 * 0.95:
                    break
                
                chunk_requests.append(requests[i])
                chunk_size_bytes += req_size_bytes
                i += 1
            
            if not chunk_requests:
                break
            
            # ì²­í¬ JSONL íŒŒì¼ ìƒì„±
            chunk_jsonl_path = f"{base}_chunk{chunk_num:03d}{ext}"
            chunk_files_created.append(chunk_jsonl_path)
            
            with open(chunk_jsonl_path, "w", encoding="utf-8") as f:
                for req in chunk_requests:
                    f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            chunk_size_mb = os.path.getsize(chunk_jsonl_path) / (1024 * 1024)
            # ì‹¤ì œ ìƒì„±ëœ ì²­í¬ ìˆ˜ë¡œ í‘œì‹œ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë  ìˆ˜ ìˆìŒ)
            self.append_log(f"[INFO] ì²­í¬ {chunk_num}: {len(chunk_requests)}ê±´(í–‰) í¬í•¨, {chunk_size_mb:.2f} MB")
            
            # ë°°ì¹˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 3
            retry_count = 0
            batch_created = False
            
            while retry_count < max_retries and not batch_created:
                try:
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹œë„ ì¤‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                    batch = create_batch_from_jsonl(
                        client=client,
                        jsonl_path=chunk_jsonl_path,
                        excel_path=excel_path,
                        model_name=model_name,
                        log_func=self.append_log,
                    )
                    
                    batch_id = batch.id
                    batch_ids.append(batch_id)
                    self.append_log(f"âœ… ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_id}")
                    batch_created = True
                    
                    # ì‘ì—… ì´ë ¥ ê¸°ë¡ (ê·¸ë£¹ ì •ë³´ í¬í•¨)
                    # total_chunksëŠ” ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë˜ë¯€ë¡œ ì¼ë‹¨ chunk_numìœ¼ë¡œ ì„¤ì •
                    upsert_batch_job(
                        batch_id=batch_id,
                        src_excel=excel_path,
                        jsonl_path=chunk_jsonl_path,
                        model=model_name,
                        effort=effort,
                        status=batch.status,
                        output_file_id=None,
                        batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                        chunk_index=chunk_num,  # ì²­í¬ ë²ˆí˜¸
                        total_chunks=chunk_num,  # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì²­í¬ ìˆ˜ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨)
                        resize_mode=resize_mode,  # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ
                    )
                except Exception as e:
                    error_str = str(e).lower()
                    is_token_limit_error = "enqueued token limit" in error_str or "token limit reached" in error_str
                    
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                        self.append_log(f"âš ï¸ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                        if is_token_limit_error:
                            self.append_log(f"[WARN] í† í° ì œí•œ ì˜¤ë¥˜ ê°ì§€. ë” ê¸´ ëŒ€ê¸° ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            wait_time = max(wait_time, 30)  # í† í° ì œí•œ ì˜¤ë¥˜ëŠ” ìµœì†Œ 30ì´ˆ ëŒ€ê¸°
                        self.append_log(f"[INFO] {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        import time
                        time.sleep(wait_time)
                    else:
                        self.append_log(f"âŒ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}")
                        if is_token_limit_error:
                            self.append_log(f"[INFO] í† í° ì œí•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì¼ë¶€ ë°°ì¹˜ê°€ ì™„ë£Œëœ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                            self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼: {chunk_jsonl_path}")
                            self.append_log(f"[INFO] ë‚˜ì¤‘ì— '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì´ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        import traceback
                        self.append_log(traceback.format_exc())
                        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìœ ì§€ (ìˆ˜ë™ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                        failed_chunk_files.append({
                            "chunk_num": chunk_num,
                            "chunk_file": chunk_jsonl_path,
                            "error": str(e),
                            "is_token_limit": is_token_limit_error,
                            "excel_path": excel_path,
                            "model_name": model_name,
                            "effort": effort,
                            "batch_group_id": batch_group_id,
                        })
                        # í•˜ì§€ë§Œ ë°°ì¹˜ IDëŠ” ì¶”ê°€ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ batch_idsì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ
        
        # ëª¨ë“  ì²­í¬ ìƒì„± ì™„ë£Œ í›„, total_chunksë¥¼ ì‹¤ì œ ìƒì„±ëœ ë°°ì¹˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        actual_total_chunks = len(batch_ids)
        if actual_total_chunks > 0:
            self.append_log(f"[INFO] ì´ {actual_total_chunks}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ. ì‘ì—… ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
            # ê²½ìŸ ì¡°ê±´ ë°©ì§€ë¥¼ ìœ„í•´ ì›ìì  ì—…ë°ì´íŠ¸
            jobs = load_batch_jobs()
            updated_count = 0
            for j in jobs:
                if j.get("batch_group_id") == batch_group_id:
                    j["total_chunks"] = actual_total_chunks
                    updated_count += 1
            if updated_count > 0:
                save_batch_jobs(jobs)
                self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {actual_total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.append_log(f"âš ï¸ ìƒì„±ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì„±ê³µí•œ ë°°ì¹˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²­í¬ íŒŒì¼ ì •ë¦¬ ì˜µì…˜ ì œê³µ (í˜„ì¬ëŠ” ìœ ì§€)
        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìˆ˜ë™ ì¬ì‹œë„ë¥¼ ìœ„í•´ ìœ ì§€
        if actual_total_chunks < chunk_num:
            failed_chunks = chunk_num - actual_total_chunks
            self.append_log(f"âš ï¸ {failed_chunks}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²­í¬ íŒŒì¼ì€ ìœ ì§€ë©ë‹ˆë‹¤.")
            
            # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ì„ ëª…í™•íˆ í‘œì‹œ
            if failed_chunk_files:
                self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡:")
                for failed_info in failed_chunk_files:
                    self.append_log(f"  - ì²­í¬ {failed_info['chunk_num']}: {os.path.basename(failed_info['chunk_file'])}")
                    if failed_info['is_token_limit']:
                        self.append_log(f"    â†’ í† í° ì œí•œ ì˜¤ë¥˜. ì¼ë¶€ ë°°ì¹˜ ì™„ë£Œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                
                # ì‹¤íŒ¨ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                failed_info_path = f"{base}_failed_chunks.json"
                try:
                    with open(failed_info_path, "w", encoding="utf-8") as f:
                        json.dump(failed_chunk_files, f, ensure_ascii=False, indent=2)
                    self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ ì €ì¥: {os.path.basename(failed_info_path)}")
                    self.append_log(f"[INFO] ë‚˜ì¤‘ì— ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤íŒ¨í•œ ì²­í¬ë§Œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    # GUIì— ìë™ìœ¼ë¡œ ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ì•Œë¦¼
                    self.after(0, lambda: self._handle_failed_chunks(failed_info_path, failed_chunk_files))
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
        try:
            root_name = get_root_filename(excel_path)
            JobManager.update_status(root_name, text_msg="T2 (ì§„í–‰ì¤‘)")
            self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> T2 (ì§„í–‰ì¤‘)")
        except Exception:
            pass
        
        self._load_jobs_all()
        self._load_archive_list()
        
        return batch_ids

    # ----------------------------------------------------
    # Tab 2: Manage
    # ----------------------------------------------------
    def _init_tab_manage(self):
        container = ttk.Frame(self.tab_manage, padding=10)
        container.pack(fill='both', expand=True)
        sub_tabs = ttk.Notebook(container)
        sub_tabs.pack(fill='both', expand=True)
        
        self.sub_active = ttk.Frame(sub_tabs, padding=10)
        self.sub_archive = ttk.Frame(sub_tabs, padding=10)
        sub_tabs.add(self.sub_active, text=" â–¶ ì§„í–‰ì¤‘ / ì™„ë£Œ (Active) ")
        sub_tabs.add(self.sub_archive, text=" ğŸ—‘ íœ´ì§€í†µ (Archive) ")
        
        # Active UI
        f_ctrl = ttk.Frame(self.sub_active)
        f_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_ctrl, text="ğŸ”„ ì„ íƒ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ“¥ ì„ íƒ ì¼ê´„ ë³‘í•©", command=self._merge_selected, style="Primary.TButton").pack(side='left', padx=10)
        ttk.Button(f_ctrl, text="ğŸ“Š ì„ íƒ ì¼ê´„ ë¶„ì„ ë¦¬í¬íŠ¸", command=self._report_selected_unified, style="Success.TButton").pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ—‘ íœ´ì§€í†µ ì´ë™", command=self._archive_selected, style="Danger.TButton").pack(side='right', padx=2)
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ë²„íŠ¼ ì¶”ê°€
        f_group_ctrl = ttk.Frame(self.sub_active)
        f_group_ctrl.pack(fill='x', pady=(0, 5))
        ttk.Label(f_group_ctrl, text="ğŸ’¡ ê·¸ë£¹ í—¤ë”ë¥¼ ë”ë¸”í´ë¦­í•˜ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°ê°€ ë©ë‹ˆë‹¤.", 
                 font=("ë§‘ì€ ê³ ë”•", 8), foreground="#666").pack(side='left', padx=5)
        ttk.Button(f_group_ctrl, text="ğŸ“‚ ëª¨ë“  ê·¸ë£¹ í¼ì¹˜ê¸°", command=lambda: self._expand_all_groups(self.tree_active)).pack(side='right', padx=2)
        ttk.Button(f_group_ctrl, text="ğŸ“ ëª¨ë“  ê·¸ë£¹ ì ‘ê¸°", command=lambda: self._collapse_all_groups(self.tree_active)).pack(side='right', padx=2)
        
        cols = ("batch_id", "excel_name", "memo", "status", "created", "completed", "model", "effort", "counts", "group")
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš© (íŠ¸ë¦¬ ì•„ì´ì½˜ + ì»¬ëŸ¼ í—¤ë”)
        self.tree_active = ttk.Treeview(self.sub_active, columns=cols, show='tree headings', height=15, selectmode='extended')
        self.tree_active.tag_configure('odd', background=COLOR_WHITE)
        self.tree_active.tag_configure('even', background='#F2F7FF')
        self.tree_active.tag_configure('group', background='#E8F5E9')  # ê·¸ë£¹ ë°°ì¹˜ ê°•ì¡°
        self.tree_active.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))  # ê·¸ë£¹ í—¤ë” ê°•ì¡°
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_active.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_active.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_active.heading("memo", text="ë©”ëª¨")
        self.tree_active.heading("status", text="ìƒíƒœ")
        self.tree_active.heading("created", text="ìƒì„±ì¼")
        self.tree_active.heading("completed", text="ì™„ë£Œì¼")
        self.tree_active.heading("model", text="ëª¨ë¸")
        self.tree_active.heading("effort", text="Effort")
        self.tree_active.heading("counts", text="ê±´ìˆ˜")
        self.tree_active.heading("group", text="ê·¸ë£¹")
        # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •: ê·¸ë£¹ëª…ì´ ê¸¸ì–´ì„œ íŠ¸ë¦¬ ì»¬ëŸ¼ í™•ëŒ€, ì¼ë¶€ ì»¬ëŸ¼ì€ ì¶•ì†Œ
        self.tree_active.column("#0", width=350, anchor="w")  # íŠ¸ë¦¬ ì»¬ëŸ¼ (ê·¸ë£¹ëª… í‘œì‹œ) - 250 â†’ 350
        self.tree_active.column("batch_id", width=200, anchor="w")
        self.tree_active.column("excel_name", width=200, anchor="w")  # ì—‘ì…€ íŒŒì¼ëª…
        self.tree_active.column("memo", width=150, anchor="w")  # ë©”ëª¨
        self.tree_active.column("status", width=100, anchor="center")
        self.tree_active.column("created", width=120, anchor="center")  # ì¤„ì„
        self.tree_active.column("completed", width=120, anchor="center")  # ì¤„ì„
        self.tree_active.column("model", width=100, anchor="center")  # ì¤„ì„
        self.tree_active.column("effort", width=70, anchor="center")  # 80 â†’ 70
        self.tree_active.column("counts", width=80, anchor="center")  # ì¤„ì„
        self.tree_active.column("group", width=100, anchor="center")  # 120 â†’ 100
        self.tree_active.pack(fill='both', expand=True, padx=5, pady=5)
        
        self.menu_active = Menu(self, tearoff=0)
        self.menu_active.add_command(label="ìƒíƒœ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active))
        self.menu_active.add_command(label="ê²°ê³¼ ë³‘í•©", command=self._merge_selected)
        self.menu_active.add_command(label="ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±", command=self._report_selected_unified)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ë¡œ ì´ë™ (ì¬ì‹œë„)", command=self._move_failed_to_retry)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="íœ´ì§€í†µìœ¼ë¡œ ì´ë™", command=self._archive_selected)
        self.tree_active.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_active, self.menu_active))
        self.tree_active.bind("<Double-1>", self._on_tree_double_click)

        # Archive UI
        f_arch_ctrl = ttk.Frame(self.sub_archive)
        f_arch_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_arch_ctrl, text="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected, style="Primary.TButton").pack(side='left')
        ttk.Button(f_arch_ctrl, text="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected, style="Danger.TButton").pack(side='right')
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ë²„íŠ¼ ì¶”ê°€
        f_arch_group_ctrl = ttk.Frame(self.sub_archive)
        f_arch_group_ctrl.pack(fill='x', pady=(0, 5))
        ttk.Label(f_arch_group_ctrl, text="ğŸ’¡ ê·¸ë£¹ í—¤ë”ë¥¼ ë”ë¸”í´ë¦­í•˜ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°ê°€ ë©ë‹ˆë‹¤.", 
                 font=("ë§‘ì€ ê³ ë”•", 8), foreground="#666").pack(side='left', padx=5)
        ttk.Button(f_arch_group_ctrl, text="ğŸ“‚ ëª¨ë“  ê·¸ë£¹ í¼ì¹˜ê¸°", command=lambda: self._expand_all_groups(self.tree_arch)).pack(side='right', padx=2)
        ttk.Button(f_arch_group_ctrl, text="ğŸ“ ëª¨ë“  ê·¸ë£¹ ì ‘ê¸°", command=lambda: self._collapse_all_groups(self.tree_arch)).pack(side='right', padx=2)
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš©
        self.tree_arch = ttk.Treeview(self.sub_archive, columns=cols, show='tree headings', height=15, selectmode='extended')
        self.tree_arch.tag_configure('odd', background=COLOR_WHITE)
        self.tree_arch.tag_configure('even', background='#FFF2F2')
        self.tree_arch.tag_configure('group', background='#FFE8E8')  # ê·¸ë£¹ ë°°ì¹˜ ê°•ì¡°
        self.tree_arch.tag_configure('group_header', background='#FFCDD2', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))  # ê·¸ë£¹ í—¤ë” ê°•ì¡°
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_arch.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_arch.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_arch.heading("memo", text="ë©”ëª¨")
        self.tree_arch.heading("status", text="ìƒíƒœ")
        self.tree_arch.heading("created", text="ìƒì„±ì¼")
        self.tree_arch.heading("completed", text="ì™„ë£Œì¼")
        self.tree_arch.heading("model", text="ëª¨ë¸")
        self.tree_arch.heading("effort", text="Effort")
        self.tree_arch.heading("counts", text="ê±´ìˆ˜")
        self.tree_arch.heading("group", text="ê·¸ë£¹")
        # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •: ê·¸ë£¹ëª…ì´ ê¸¸ì–´ì„œ íŠ¸ë¦¬ ì»¬ëŸ¼ í™•ëŒ€, ì¼ë¶€ ì»¬ëŸ¼ì€ ì¶•ì†Œ
        self.tree_arch.column("#0", width=350, anchor="w")  # íŠ¸ë¦¬ ì»¬ëŸ¼ (ê·¸ë£¹ëª… í‘œì‹œ) - 250 â†’ 350
        self.tree_arch.column("batch_id", width=200, anchor="w")
        self.tree_arch.column("excel_name", width=200, anchor="w")  # ì—‘ì…€ íŒŒì¼ëª…
        self.tree_arch.column("memo", width=150, anchor="w")  # ë©”ëª¨
        self.tree_arch.column("status", width=100, anchor="center")
        self.tree_arch.column("created", width=120, anchor="center")  # ì¤„ì„
        self.tree_arch.column("completed", width=120, anchor="center")  # ì¤„ì„
        self.tree_arch.column("model", width=100, anchor="center")  # ì¤„ì„
        self.tree_arch.column("effort", width=70, anchor="center")  # 80 â†’ 70
        self.tree_arch.column("counts", width=80, anchor="center")  # ì¤„ì„
        self.tree_arch.column("group", width=100, anchor="center")  # 120 â†’ 100
        self.tree_arch.pack(fill='both', expand=True)
        
        self.menu_arch = Menu(self, tearoff=0)
        self.menu_arch.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_arch))
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected)
        self.menu_arch.add_command(label="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected)
        self.tree_arch.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_arch, self.menu_arch))
        
        self._load_jobs_all()
        self._load_archive_list()

    def _expand_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ í¼ì¹©ë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=True)
    
    def _collapse_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ ì ‘ìŠµë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=False)
    
    def _show_context_menu(self, event, tree, menu):
        item = tree.identify_row(event.y)
        if item:
            if item not in tree.selection(): tree.selection_set(item)
            menu.post(event.x_root, event.y_root)

    def _get_selected_ids(self, tree):
        """
        ì„ íƒëœ í•­ëª©ì—ì„œ ë°°ì¹˜ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê·¸ë£¹ í—¤ë”ë¥¼ ì„ íƒí•˜ë©´ ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        selection = tree.selection()
        ids = []
        
        for item in selection:
            vals = tree.item(item)['values']
            batch_id = vals[0] if vals else ""
            
            # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
            if not batch_id:
                # ê·¸ë£¹ í—¤ë”ì˜ ìì‹ ë…¸ë“œë“¤(ë°°ì¹˜ë“¤)ì„ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
                children = tree.get_children(item)
                for child in children:
                    child_vals = tree.item(child)['values']
                    if child_vals and child_vals[0]:
                        ids.append(child_vals[0])
            else:
                # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°
                ids.append(batch_id)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(ids))
    
    def _edit_memo(self, tree):
        """ì„ íƒëœ ë°°ì¹˜ì˜ ë©”ëª¨ë¥¼ í¸ì§‘í•©ë‹ˆë‹¤."""
        selection = tree.selection()
        if not selection:
            messagebox.showwarning("ê²½ê³ ", "ë©”ëª¨ë¥¼ í¸ì§‘í•  ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # ì²« ë²ˆì§¸ ì„ íƒëœ í•­ëª©ì˜ ë°°ì¹˜ ID ê°€ì ¸ì˜¤ê¸°
        item = selection[0]
        vals = tree.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° ì²˜ë¦¬
        if not batch_id:
            messagebox.showinfo("ì•ˆë‚´", "ê·¸ë£¹ í—¤ë”ê°€ ì•„ë‹Œ ê°œë³„ ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # í˜„ì¬ ë©”ëª¨ ê°€ì ¸ì˜¤ê¸°
        jobs = load_batch_jobs()
        current_memo = ""
        for j in jobs:
            if j["batch_id"] == batch_id:
                current_memo = j.get("memo", "") or ""
                break
        
        # ë©”ëª¨ í¸ì§‘ ë‹¤ì´ì–¼ë¡œê·¸
        dialog = tk.Toplevel(self)
        dialog.title("ë©”ëª¨ í¸ì§‘")
        dialog.geometry("500x200")
        dialog.transient(self)
        dialog.grab_set()
        
        # ë°°ì¹˜ ID í‘œì‹œ
        tk.Label(dialog, text=f"ë°°ì¹˜ ID: {batch_id[:30]}...", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(10, 5))
        
        # ë©”ëª¨ ì…ë ¥ í•„ë“œ
        tk.Label(dialog, text="ë©”ëª¨:", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(5, 0))
        memo_entry = tk.Text(dialog, height=5, width=60, font=("ë§‘ì€ ê³ ë”•", 9))
        memo_entry.pack(fill="both", expand=True, padx=10, pady=5)
        memo_entry.insert("1.0", current_memo)
        memo_entry.focus()
        
        # ë²„íŠ¼
        btn_frame = tk.Frame(dialog)
        btn_frame.pack(fill="x", padx=10, pady=10)
        
        def save_memo():
            new_memo = memo_entry.get("1.0", "end-1c").strip()
            upsert_batch_job(batch_id, memo=new_memo)
            self.append_log(f"[INFO] ë°°ì¹˜ {batch_id[:20]}... ë©”ëª¨ ì—…ë°ì´íŠ¸: {new_memo[:50]}...")
            self._load_jobs_all()
            self._load_archive_list()
            dialog.destroy()
            messagebox.showinfo("ì™„ë£Œ", "ë©”ëª¨ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        tk.Button(btn_frame, text="ì €ì¥", command=save_memo, bg="#4CAF50", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)
        tk.Button(btn_frame, text="ì·¨ì†Œ", command=dialog.destroy, bg="#f44336", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)

    def _load_jobs_all(self):
        if not hasattr(self, 'tree_active'): return
        for i in self.tree_active.get_children(): self.tree_active.delete(i)
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ì„ ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ê±´: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0]
            created_at = first_job.get("created_at", "")
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = dt.strftime("%m-%d %H:%M")
                except:
                    date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
            else:
                date_str = "-"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
            group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
            excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
            memo = first_job.get("memo", "") or "-"
            group_node = self.tree_active.insert("", "end", 
                text=group_header_text,
                values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                memo = j.get("memo", "") or "-"
                tag = 'group'
                self.tree_active.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even'
            self.tree_active.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))

    def _load_archive_list(self):
        if not hasattr(self, 'tree_arch'): return
        for i in self.tree_arch.get_children(): self.tree_arch.delete(i)
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if not j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs)) if group_jobs else len(group_jobs)
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ê±´: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0]
            created_at = first_job.get("created_at", "")
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = dt.strftime("%m-%d %H:%M")
                except:
                    date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
            else:
                date_str = "-"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
            group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
            excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
            memo = first_job.get("memo", "") or "-"
            group_node = self.tree_arch.insert("", "end", 
                text=group_header_text,
                values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                memo = j.get("memo", "") or "-"
                tag = 'group'
                self.tree_arch.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even'
            self.tree_arch.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))

    # --- Actions ---
    def _refresh_selected(self, tree):
        ids = self._get_selected_ids(tree)
        if not ids: return
        jobs = load_batch_jobs()
        completed = [bid for bid in ids if next((x for x in jobs if x["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        if completed:
            if messagebox.askyesno("í™•ì¸", f"{len(completed)}ê±´ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\në¯¸ì™„ë£Œ ê±´ë§Œ ê°±ì‹ í• ê¹Œìš”?"):
                ids = [i for i in ids if i not in completed]
        if not ids: return
        t = threading.Thread(target=self._run_refresh_ids, args=(ids,))
        t.daemon = True
        t.start()

    def _run_refresh_ids(self, ids):
        key = self.api_key_var.get().strip()
        client = OpenAI(api_key=key)
        self.append_log(f"ì„ íƒëœ {len(ids)}ê±´ ê°±ì‹  ì¤‘...")
        for bid in ids:
            try:
                remote = client.batches.retrieve(bid)
                rc = None
                if remote.request_counts:
                    rc = {"total": remote.request_counts.total, "completed": remote.request_counts.completed, "failed": remote.request_counts.failed}
                
                # expired ìƒíƒœë„ ê°±ì‹  ê°€ëŠ¥ (output_file_id í™•ì¸ì„ ìœ„í•´)
                # output_file_id ì¶”ì¶œ: ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
                output_file_id = getattr(remote, "output_file_id", None)
                if not output_file_id:
                    # output_file ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                    output_file = getattr(remote, "output_file", None)
                    if output_file:
                        if isinstance(output_file, str):
                            output_file_id = output_file
                        else:
                            output_file_id = getattr(output_file, "id", None) or getattr(output_file, "file_id", None)
                
                # model_dump()ë¥¼ í†µí•œ ì¶”ê°€ í™•ì¸ (ê°±ì‹  ì‹œì—ë„ ì ìš©)
                if not output_file_id and remote.status == "completed":
                    try:
                        if hasattr(remote, "model_dump"):
                            dump = remote.model_dump()
                            if "output_file_id" in dump and dump["output_file_id"]:
                                output_file_id = dump["output_file_id"]
                            elif "output_file" in dump:
                                of = dump["output_file"]
                                if isinstance(of, str) and of:
                                    output_file_id = of
                                elif isinstance(of, dict) and "id" in of:
                                    output_file_id = of["id"]
                    except Exception:
                        pass
                
                upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id, request_counts=rc)
                
                if remote.status == "expired" and output_file_id:
                    self.append_log(f"â„¹ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ì§€ë§Œ output_file_idê°€ ìˆìŠµë‹ˆë‹¤. (ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")
                elif remote.status == "completed":
                    if output_file_id:
                        self.append_log(f"âœ… {bid}: {remote.status} (output_file_id: {output_file_id})")
                    else:
                        self.append_log(f"âš ï¸ {bid}: {remote.status} (output_file_id ì—†ìŒ - ë””ë²„ê¹… í•„ìš”)")
                else:
                    self.append_log(f"âœ… {bid}: {remote.status}")
            except Exception as e:
                self.append_log(f"{bid} ê°±ì‹  ì‹¤íŒ¨: {e}")
        self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])
        self.append_log("ê°±ì‹  ì™„ë£Œ")

    def _merge_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        
        # ì„ íƒëœ ë°°ì¹˜ë“¤ì˜ ê·¸ë£¹ ì •ë³´ í™•ì¸
        selected_jobs = [j for j in jobs if j["batch_id"] in ids]
        group_ids = set()
        for j in selected_jobs:
            group_id = j.get("batch_group_id")
            if group_id:
                group_ids.add(group_id)
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        all_target_ids = set(ids)
        if group_ids:
            for group_id in group_ids:
                # completed ë˜ëŠ” expired ìƒíƒœì¸ ë°°ì¹˜ í¬í•¨ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
                group_batches = [j for j in jobs if j.get("batch_group_id") == group_id and j.get("status") in ["completed", "expired"]]
                for j in group_batches:
                    all_target_ids.add(j["batch_id"])
            
            if len(all_target_ids) > len(ids):
                group_info = f"\n\nê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ {len(all_target_ids) - len(ids)}ê°œê°€ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤."
            else:
                group_info = ""
        else:
            group_info = ""
        
        # completed ë˜ëŠ” expired ìƒíƒœì¸ ë°°ì¹˜ í¬í•¨ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
        targets = [bid for bid in all_target_ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") in ["completed", "expired"]]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ë³‘í•©í•  ìˆ˜ ìˆëŠ” 'completed' ë˜ëŠ” 'expired' ìƒíƒœì˜ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if messagebox.askyesno("ë³‘í•©", f"ì´ {len(targets)}ê±´ì„ ë³‘í•©í•˜ì‹œê² ìŠµë‹ˆê¹Œ?{group_info}"):
            t = threading.Thread(target=self._run_merge_multi, args=(list(targets),))
            t.daemon = True
            t.start()

    def _run_merge_multi(self, ids):
        """
        ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ê¸°ì¡´ Stage2 Batch ì½”ì–´(download_batch_output_if_ready + merge_batch_output_to_excel)ë¥¼
        ì´ìš©í•´ ì„ íƒëœ Batch ë“¤ì— ëŒ€í•´ ê²°ê³¼ JSONL ë‹¤ìš´ë¡œë“œ + ì—‘ì…€ ë³‘í•©ì„ ìˆ˜í–‰.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ í•˜ë‚˜ì˜ ì—‘ì…€ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.
        """
        key = self.api_key_var.get().strip()
        client = OpenAI(api_key=key)
        success_cnt = 0
        total_cost = 0.0
        
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ë°°ì¹˜ ë¶„ë¥˜
        groups_to_merge = {}  # {group_id: [batch_ids]}
        ungrouped_batches = []  # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤
        
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not job:
                continue
            
            group_id = job.get("batch_group_id")
            if group_id:
                if group_id not in groups_to_merge:
                    groups_to_merge[group_id] = []
                groups_to_merge[group_id].append(bid)
            else:
                ungrouped_batches.append(bid)
        
        # ê·¸ë£¹ë³„ ë³‘í•© ì²˜ë¦¬
        for group_id, batch_ids in groups_to_merge.items():
            self.append_log(f"--- ê·¸ë£¹ ë³‘í•© ì‹œì‘: {group_id} ({len(batch_ids)}ê°œ ë°°ì¹˜) ---")
            try:
                # ê·¸ë£¹ ë‚´ ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì›ë³¸ ì—‘ì…€ ê²½ë¡œ ì‚¬ìš©
                first_job = next((j for j in jobs if j["batch_id"] == batch_ids[0]), None)
                if not first_job:
                    continue
                
                src_path = first_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ì„ì‹œ JSONLì— ìˆ˜ì§‘
                all_output_lines = []
                model_name = first_job.get("model", "gpt-5-mini")
                total_group_cost = 0.0
                
                # ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ (chunk_indexê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ)
                def get_chunk_index(bid):
                    job = next((j for j in jobs if j["batch_id"] == bid), None)
                    if job:
                        idx = job.get("chunk_index")
                        return idx if idx is not None else 999999  # chunk_indexê°€ ì—†ìœ¼ë©´ ë§¨ ë’¤ë¡œ
                    return 999999
                
                batch_ids_sorted = sorted(batch_ids, key=get_chunk_index)
                
                for bid in batch_ids_sorted:
                    self.append_log(f"  [ê·¸ë£¹] ë°°ì¹˜ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    try:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if not local_job:
                            continue
                        
                        # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                        if local_job.get("status") == "merged":
                            self.append_log(f"  â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        base_dir = os.path.dirname(src_path)
                        base_name, _ = os.path.splitext(os.path.basename(src_path))
                        out_jsonl = os.path.join(base_dir, f"{base_name}_stage2_batch_output_{bid}.jsonl")
                        
                        if download_batch_output_if_ready is None:
                            raise RuntimeError("stage2_batch_api_ê¸°ì¡´gpt ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ Batch ë³‘í•© ì½”ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # ë°°ì¹˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                        ok, status = download_batch_output_if_ready(
                            client=client,
                            batch_id=bid,
                            output_jsonl_path=out_jsonl,
                            log_func=self.append_log,
                        )
                        
                        upsert_batch_job(
                            batch_id=bid,
                            status=status,
                            output_jsonl=out_jsonl if ok else local_job.get("output_jsonl", ""),
                        )
                        
                        # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ ë‹¤ìš´ë¡œë“œ ì„±ê³µí•œ ê²½ìš°ë§Œ ìˆ˜ì§‘
                        if not ok or status not in ["completed", "expired"]:
                            self.append_log(f"  âš ï¸ {bid}: ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤. (status={status}, ok={ok})")
                            continue
                        
                        # JSONL íŒŒì¼ ì½ì–´ì„œ ìˆ˜ì§‘ ë° ìºì‹± í†µê³„ ìˆ˜ì§‘
                        batch_cached_tok = 0
                        batch_total_requests = 0
                        batch_cache_hits = 0
                        if os.path.exists(out_jsonl):
                            with open(out_jsonl, "r", encoding="utf-8") as f:
                                for line in f:
                                    line = line.strip()
                                    if line:
                                        all_output_lines.append(line)
                                        # ìºì‹± í†µê³„ ìˆ˜ì§‘
                                        try:
                                            data = json.loads(line)
                                            response_body = data.get("response", {}).get("body", {})
                                            usage = response_body.get("usage", {})
                                            input_tokens_details = usage.get("input_tokens_details", {})
                                            cached_tokens = input_tokens_details.get("cached_tokens", 0)
                                            batch_cached_tok += cached_tokens
                                            batch_total_requests += 1
                                            if cached_tokens > 0:
                                                batch_cache_hits += 1
                                        except:
                                            pass
                        
                        # ë°°ì¹˜ë³„ ìºì‹± í†µê³„ ì¶œë ¥
                        if batch_total_requests > 0:
                            cache_hit_rate = (batch_cache_hits / batch_total_requests * 100)
                            self.append_log(f"  [ìºì‹±] {bid}: ìš”ì²­ {batch_total_requests}ê±´, íˆíŠ¸ {batch_cache_hits}ê±´ ({cache_hit_rate:.1f}%), ìºì‹œ í† í° {batch_cached_tok:,}")
                        
                    except Exception as e:
                        self.append_log(f"  âŒ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if not all_output_lines:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ë³‘í•©í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ì˜ ì „ì²´ ì²­í¬ ìˆ˜ í™•ì¸ ë° ê²€ì¦
                expected_total_chunks = first_job.get("total_chunks")
                if expected_total_chunks:
                    # ì‹¤ì œë¡œ ê²°ê³¼ë¥¼ ë‹¤ìš´ë¡œë“œí•œ ë°°ì¹˜ ìˆ˜ ê³„ì‚° (completed ë˜ëŠ” expired ìƒíƒœ í¬í•¨)
                    downloaded_batch_ids = []
                    for bid in batch_ids_sorted:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if local_job and local_job.get("status") in ["completed", "expired"]:
                            out_jsonl = local_job.get("output_jsonl") or os.path.join(
                                os.path.dirname(src_path),
                                f"{os.path.splitext(os.path.basename(src_path))[0]}_stage2_batch_output_{bid}.jsonl"
                            )
                            if os.path.exists(out_jsonl):
                                downloaded_batch_ids.append(bid)
                    
                    if len(downloaded_batch_ids) < expected_total_chunks:
                        missing = expected_total_chunks - len(downloaded_batch_ids)
                        self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì˜ˆìƒ {expected_total_chunks}ê°œ ì¤‘ {len(downloaded_batch_ids)}ê°œë§Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({missing}ê°œ ëˆ„ë½ ê°€ëŠ¥)")
                
                # ì„ì‹œ í†µí•© JSONL íŒŒì¼ ìƒì„± (ì¤‘ë³µ ì œê±°: custom_id ê¸°ì¤€)
                base_dir = os.path.dirname(src_path)
                base_name, _ = os.path.splitext(os.path.basename(src_path))
                merged_jsonl = os.path.join(base_dir, f"{base_name}_stage2_batch_output_merged_{group_id}.jsonl")
                
                # custom_id ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ê°™ì€ custom_idê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¤ë©´ ë§ˆì§€ë§‰ ê²ƒë§Œ ì‚¬ìš©)
                result_map = {}  # {custom_id: json_line}
                duplicate_count = 0
                
                for line in all_output_lines:
                    try:
                        obj = json.loads(line)
                        custom_id = obj.get("custom_id", "")
                        if custom_id:
                            if custom_id in result_map:
                                duplicate_count += 1
                            result_map[custom_id] = line  # ë§ˆì§€ë§‰ ê²ƒë§Œ ìœ ì§€
                    except Exception as e:
                        self.append_log(f"  [WARN] JSONL ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                
                # ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥
                with open(merged_jsonl, "w", encoding="utf-8") as f:
                    for line in result_map.values():
                        f.write(line + "\n")
                
                if duplicate_count > 0:
                    self.append_log(f"  [ê·¸ë£¹] í†µí•© JSONL ìƒì„±: {len(all_output_lines)}ê°œ ê²°ê³¼ ì¤‘ {duplicate_count}ê°œ ì¤‘ë³µ ì œê±° â†’ {len(result_map)}ê°œ ê³ ìœ  ê²°ê³¼")
                else:
                    self.append_log(f"  [ê·¸ë£¹] í†µí•© JSONL ìƒì„±: {len(result_map)}ê°œ ê³ ìœ  ê²°ê³¼ (ì¤‘ë³µ ì—†ìŒ)")
                
                # í†µí•© JSONLì—ì„œ ìºì‹± í†µê³„ ìˆ˜ì§‘
                total_group_cached = 0
                total_group_requests = 0
                total_group_cache_hits = 0
                with open(merged_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        try:
                            data = json.loads(line)
                            response_body = data.get("response", {}).get("body", {})
                            usage = response_body.get("usage", {})
                            input_tokens_details = usage.get("input_tokens_details", {})
                            cached_tokens = input_tokens_details.get("cached_tokens", 0)
                            total_group_cached += cached_tokens
                            total_group_requests += 1
                            if cached_tokens > 0:
                                total_group_cache_hits += 1
                        except:
                            pass
                
                # ê·¸ë£¹ ì „ì²´ ìºì‹± í†µê³„ ì¶œë ¥
                group_cache_hit_rate = (total_group_cache_hits / total_group_requests * 100) if total_group_requests > 0 else 0
                self.append_log(f"  [ê·¸ë£¹ ìºì‹± í†µê³„] ìš”ì²­ {total_group_requests:,}ê±´, íˆíŠ¸ {total_group_cache_hits:,}ê±´ ({group_cache_hit_rate:.1f}%), ìºì‹œ í† í° {total_group_cached:,}")
                
                # í†µí•© JSONLì„ ì—‘ì…€ì— ë³‘í•©
                if merge_batch_output_to_excel is None:
                    raise RuntimeError("stage2_batch_api_ê¸°ì¡´gpt ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ Batch ë³‘í•© ì½”ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                info = merge_batch_output_to_excel(
                    excel_path=src_path,
                    output_jsonl_path=merged_jsonl,
                    model_name=model_name,
                    skip_filled=self.skip_exist_var.get(),
                    log_func=self.append_log,
                )
                
                # ìºì‹œë¡œ ì ˆê°ëœ ë¹„ìš© ê³„ì‚°
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                group_cache_savings = (total_group_cached / 1_000_000) * pricing["input"] * 0.5
                if group_cache_savings > 0:
                    self.append_log(f"  [ê·¸ë£¹ ë¹„ìš©ì ˆê°] ìºì‹±ìœ¼ë¡œ ì´ ${group_cache_savings:.4f} ì ˆê°")
                
                total_group_cost += info.get("total_cost_usd") or 0.0
                total_cost += total_group_cost
                
                # Stage2 ìµœì¢… íŒŒì¼ëª…: *_T2_... í˜•ì‹ìœ¼ë¡œ ë²„ì „ ì—…
                core_out_path = info["out_excel_path"]
                final_out_path = None
                try:
                    df_done = pd.read_excel(core_out_path)
                    
                    # ST2_JSONì´ ìˆëŠ” í–‰ê³¼ ì—†ëŠ” í–‰ ë¶„ë¦¬
                    if "ST2_JSON" in df_done.columns:
                        # ST2_JSONì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì¸ í–‰ ì°¾ê¸°
                        df_with_st2 = df_done[df_done["ST2_JSON"].notna() & (df_done["ST2_JSON"] != '')].copy()
                        df_no_st2 = df_done[df_done["ST2_JSON"].isna() | (df_done["ST2_JSON"] == '')].copy()
                    else:
                        # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  í–‰ì´ ST2_JSON ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
                        df_with_st2 = pd.DataFrame()
                        df_no_st2 = df_done.copy()
                    
                    # ST2_JSONì´ ì—†ëŠ” í–‰ë“¤ì„ T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³„ë„ íŒŒì¼ ì €ì¥
                    no_st2_path = None
                    if len(df_no_st2) > 0:
                        base_dir = os.path.dirname(src_path)
                        base_name, ext = os.path.splitext(os.path.basename(src_path))
                        
                        # í˜„ì¬ íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: _T1_I0)
                        # T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³€ê²½
                        name_only_clean = re.sub(r"\([^)]*\)", "", base_name)  # ê¸°ì¡´ ê´„í˜¸ ì œê±°
                        all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
                        
                        if all_matches:
                            # ë§ˆì§€ë§‰ ë²„ì „ íŒ¨í„´ ì‚¬ìš©
                            match = all_matches[-1]
                            original_name = name_only_clean[: match.start()].rstrip("_")
                            current_i = int(match.group(4))
                            # T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ìƒì„±
                            new_filename = f"{original_name}_T2-2(ì‹¤íŒ¨)_I{current_i}{ext}"
                        else:
                            # ë²„ì „ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ T2-2(ì‹¤íŒ¨)_I0ë¡œ ìƒì„±
                            new_filename = f"{base_name}_T2-2(ì‹¤íŒ¨)_I0{ext}"
                        
                        no_st2_path = os.path.join(base_dir, new_filename)
                        df_no_st2.to_excel(no_st2_path, index=False)
                        
                        self.append_log(f"  [ê·¸ë£¹] T2-2(ì‹¤íŒ¨) ë¶„ë¦¬ íŒŒì¼: {os.path.basename(no_st2_path)} ({len(df_no_st2)}ê°œ í–‰)")
                        self.append_log(f"         â€» ì´ íŒŒì¼ì€ T2-1 ë‹¨ê³„ê¹Œì§€ë§Œ ì‘ì—… ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        
                        # ë¶„ë¦¬ëœ íŒŒì¼ì˜ ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                        try:
                            no_st2_root_name = get_root_filename(no_st2_path)
                            JobManager.update_status(no_st2_root_name, text_msg="T2-2(ì‹¤íŒ¨)")
                            self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸: {no_st2_root_name} -> T2-2(ì‹¤íŒ¨)")
                        except Exception as e:
                            self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                    
                    # ST2_JSONì´ ìˆëŠ” í–‰ë“¤ë§Œ ì €ì¥
                    if len(df_with_st2) > 0:
                        df_done = df_with_st2
                    else:
                        self.append_log(f"  âš ï¸ ê·¸ë£¹ {group_id}: ST2_JSONì´ ìˆëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                    
                    final_out_path = get_next_version_path(src_path, task_type="text")
                    
                    if safe_save_excel(df_done, final_out_path):
                        info["out_excel_path"] = final_out_path
                        # T2 ë²„ì „ íŒŒì¼ ì €ì¥ ì„±ê³µ ì‹œ, ì½”ì–´ê°€ ìƒì„±í•œ ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
                        if core_out_path != final_out_path and os.path.exists(core_out_path):
                            try:
                                os.remove(core_out_path)
                                self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(core_out_path)}")
                            except Exception as e:
                                self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                    else:
                        final_out_path = core_out_path
                except Exception as e:
                    final_out_path = core_out_path
                    self.append_log(f"[WARN] T2 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ë¥¼ merged ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                for bid in batch_ids:
                    upsert_batch_job(
                        batch_id=bid,
                        out_excel=final_out_path,
                        status="merged",
                    )
                
                # ì‹¤í–‰ ì´ë ¥ ê¸°ë¡
                try:
                    if first_job:
                        c_at_str = first_job.get("created_at", "")
                        if c_at_str:
                            try:
                                # ISO í˜•ì‹ íŒŒì‹± (Zë¥¼ +00:00ìœ¼ë¡œ ë³€í™˜)
                                c_at = datetime.fromisoformat(c_at_str.replace('Z', '+00:00'))
                                # ì‹œê°„ëŒ€ ì •ë³´ê°€ ìˆìœ¼ë©´ naiveë¡œ ë³€í™˜ (datetime.now()ì™€ ì¼ì¹˜)
                                if c_at.tzinfo is not None:
                                    c_at = c_at.replace(tzinfo=None)
                            except:
                                c_at = datetime.now()
                        else:
                            c_at = datetime.now()
                        finish_dt = datetime.now()
                        elapsed = (finish_dt - c_at).total_seconds()
                        
                        append_run_history(
                            stage="Stage 2 Batch (Grouped)",
                            model_name=model_name,
                            reasoning_effort=first_job.get("effort", "medium"),
                            src_file=src_path,
                            out_file=final_out_path,
                            total_rows=info["total_rows"],
                            api_rows=info["merged"],
                            elapsed_seconds=elapsed,
                            total_in_tok=info["total_in_tok"],
                            total_out_tok=info["total_out_tok"],
                            total_reasoning_tok=info["total_reasoning_tok"],
                            input_cost_usd=info["input_cost_usd"],
                            output_cost_usd=info["output_cost_usd"],
                            total_cost_usd=total_group_cost,
                            start_dt=c_at,
                            finish_dt=finish_dt,
                            api_type="batch",
                            batch_id=f"{group_id} ({len(batch_ids)} batches)",
                            success_rows=info["merged"],
                            fail_rows=info["missing"],
                        )
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨: {e}")
                
                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
                try:
                    root_name = get_root_filename(src_path)
                    JobManager.update_status(root_name, text_msg="T2-2(ë¶„ì„ì™„ë£Œ)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T2-2(ë¶„ì„ì™„ë£Œ)")
                except Exception as e:
                    self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")
                
                # ê·¸ë£¹ ì „ì²´ ìºì‹± í†µê³„ ì¶œë ¥ (í†µí•© JSONLì—ì„œ ìˆ˜ì§‘)
                total_group_cached = 0
                total_group_requests = 0
                total_group_cache_hits = 0
                with open(merged_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        try:
                            data = json.loads(line)
                            response_body = data.get("response", {}).get("body", {})
                            usage = response_body.get("usage", {})
                            input_tokens_details = usage.get("input_tokens_details", {})
                            cached_tokens = input_tokens_details.get("cached_tokens", 0)
                            total_group_cached += cached_tokens
                            total_group_requests += 1
                            if cached_tokens > 0:
                                total_group_cache_hits += 1
                        except:
                            pass
                
                group_cache_hit_rate = (total_group_cache_hits / total_group_requests * 100) if total_group_requests > 0 else 0
                group_cache_savings_pct = (total_group_cached / (info.get("total_in_tok", 0) or 1)) * 100 if info.get("total_in_tok", 0) > 0 else 0
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                group_cache_savings = (total_group_cached / 1_000_000) * pricing["input"] * 0.5
                
                self.append_log(f"âœ… ê·¸ë£¹ ë³‘í•© ì™„ë£Œ: {os.path.basename(final_out_path)} ({len(batch_ids)}ê°œ ë°°ì¹˜)")
                self.append_log(f"  [ê·¸ë£¹ ìºì‹± í†µê³„] ìš”ì²­ {total_group_requests:,}ê±´, íˆíŠ¸ {total_group_cache_hits:,}ê±´ ({group_cache_hit_rate:.1f}%), ìºì‹œ í† í° {total_group_cached:,} ({group_cache_savings_pct:.1f}%)")
                if group_cache_savings > 0:
                    self.append_log(f"  [ê·¸ë£¹ ë¹„ìš©ì ˆê°] ìºì‹±ìœ¼ë¡œ ì´ ${group_cache_savings:.4f} ì ˆê°")
                success_cnt += 1
                
            except Exception as e:
                self.append_log(f"âŒ ê·¸ë£¹ {group_id} ë³‘í•© ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ ê°œë³„ ë³‘í•© (ê¸°ì¡´ ë¡œì§)
        for bid in ungrouped_batches:
            self.append_log(f"--- ë³‘í•© ì‹œì‘: {bid} ---")
            try:
                jobs = load_batch_jobs()
                local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                if not local_job:
                    self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì‘ì—… ì´ë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë³‘í•© ë°©ì§€)
                if local_job.get("status") == "merged":
                    self.append_log(f"â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                src_path = local_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ {bid}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                base_dir = os.path.dirname(src_path)
                base_name, _ = os.path.splitext(os.path.basename(src_path))
                out_jsonl = os.path.join(base_dir, f"{base_name}_stage2_batch_output.jsonl")

                if download_batch_output_if_ready is None or merge_batch_output_to_excel is None:
                    raise RuntimeError("stage2_batch_api_ê¸°ì¡´gpt ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ Batch ë³‘í•© ì½”ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                # 1) Batch ê²°ê³¼ JSONL ë‹¤ìš´ë¡œë“œ
                ok, status = download_batch_output_if_ready(
                    client=client,
                    batch_id=bid,
                    output_jsonl_path=out_jsonl,
                    log_func=self.append_log,
                )

                upsert_batch_job(
                    batch_id=bid,
                    status=status,
                    output_jsonl=out_jsonl if ok else local_job.get("output_jsonl", ""),
                )

                # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ ë‹¤ìš´ë¡œë“œ ì„±ê³µí•œ ê²½ìš°ë§Œ ë³‘í•©
                if not ok or status not in ["completed", "expired"]:
                    self.append_log(f"âš ï¸ {bid}: ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ì–´ì„œ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤. (status={status}, ok={ok})")
                    continue

                # 2) JSONL â†’ ì—‘ì…€ ë³‘í•© + ë¹„ìš©/í† í° ê³„ì‚° (ê¸°ì¡´ ì½”ì–´ ì‚¬ìš©)
                model_name = local_job.get("model", "gpt-5-mini")
                info = merge_batch_output_to_excel(
                    excel_path=src_path,
                    output_jsonl_path=out_jsonl,
                    model_name=model_name,
                    skip_filled=self.skip_exist_var.get(),
                    log_func=self.append_log,
                )

                total_cost += info.get("total_cost_usd") or 0.0

                # 3) Stage2 ìµœì¢… íŒŒì¼ëª…: *_T2_... í˜•ì‹ìœ¼ë¡œ ë²„ì „ ì—…
                core_out_path = info["out_excel_path"]
                final_out_path = None
                try:
                    # ì½”ì–´ê°€ ë§Œë“  ì™„ë£Œ íŒŒì¼ì„ ë‹¤ì‹œ ì½ì–´ì™€ì„œ T2 ë²„ì „ íŒŒì¼ë¡œ ì €ì¥
                    df_done = pd.read_excel(core_out_path)
                    
                    # ST2_JSONì´ ìˆëŠ” í–‰ê³¼ ì—†ëŠ” í–‰ ë¶„ë¦¬
                    if "ST2_JSON" in df_done.columns:
                        # ST2_JSONì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì¸ í–‰ ì°¾ê¸°
                        df_with_st2 = df_done[df_done["ST2_JSON"].notna() & (df_done["ST2_JSON"] != '')].copy()
                        df_no_st2 = df_done[df_done["ST2_JSON"].isna() | (df_done["ST2_JSON"] == '')].copy()
                    else:
                        # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  í–‰ì´ ST2_JSON ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
                        df_with_st2 = pd.DataFrame()
                        df_no_st2 = df_done.copy()
                    
                    # ST2_JSONì´ ì—†ëŠ” í–‰ë“¤ì„ T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³„ë„ íŒŒì¼ ì €ì¥
                    no_st2_path = None
                    if len(df_no_st2) > 0:
                        base_dir = os.path.dirname(src_path)
                        base_name, ext = os.path.splitext(os.path.basename(src_path))
                        
                        # í˜„ì¬ íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: _T1_I0)
                        # T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ë³€ê²½
                        name_only_clean = re.sub(r"\([^)]*\)", "", base_name)  # ê¸°ì¡´ ê´„í˜¸ ì œê±°
                        all_matches = list(re.finditer(r"_([Tt])(\d+)_([Ii])(\d+)", name_only_clean, re.IGNORECASE))
                        
                        if all_matches:
                            # ë§ˆì§€ë§‰ ë²„ì „ íŒ¨í„´ ì‚¬ìš©
                            match = all_matches[-1]
                            original_name = name_only_clean[: match.start()].rstrip("_")
                            current_i = int(match.group(4))
                            # T2-2(ì‹¤íŒ¨) ë²„ì „ìœ¼ë¡œ ìƒì„±
                            new_filename = f"{original_name}_T2-2(ì‹¤íŒ¨)_I{current_i}{ext}"
                        else:
                            # ë²„ì „ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ T2-2(ì‹¤íŒ¨)_I0ë¡œ ìƒì„±
                            new_filename = f"{base_name}_T2-2(ì‹¤íŒ¨)_I0{ext}"
                        
                        no_st2_path = os.path.join(base_dir, new_filename)
                        df_no_st2.to_excel(no_st2_path, index=False)
                        
                        self.append_log(f"  T2-2(ì‹¤íŒ¨) ë¶„ë¦¬ íŒŒì¼: {os.path.basename(no_st2_path)} ({len(df_no_st2)}ê°œ í–‰)")
                        self.append_log(f"  â€» ì´ íŒŒì¼ì€ T2-1 ë‹¨ê³„ê¹Œì§€ë§Œ ì‘ì—… ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        
                        # ë¶„ë¦¬ëœ íŒŒì¼ì˜ ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                        try:
                            no_st2_root_name = get_root_filename(no_st2_path)
                            JobManager.update_status(no_st2_root_name, text_msg="T2-2(ì‹¤íŒ¨)")
                            self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸: {no_st2_root_name} -> T2-2(ì‹¤íŒ¨)")
                        except Exception as e:
                            self.append_log(f"[Launcher] ë¶„ë¦¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                    
                    # ST2_JSONì´ ìˆëŠ” í–‰ë“¤ë§Œ ì €ì¥
                    if len(df_with_st2) > 0:
                        df_done = df_with_st2
                    else:
                        self.append_log(f"âš ï¸ {bid}: ST2_JSONì´ ìˆëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                    
                    final_out_path = get_next_version_path(src_path, task_type="text")

                    if safe_save_excel(df_done, final_out_path):
                        info["out_excel_path"] = final_out_path
                        # T2 ë²„ì „ íŒŒì¼ ì €ì¥ ì„±ê³µ ì‹œ, ì½”ì–´ê°€ ìƒì„±í•œ ì¤‘ê°„ íŒŒì¼(_stage2_batch_ì™„ë£Œ) ì‚­ì œ
                        if core_out_path != final_out_path and os.path.exists(core_out_path):
                            try:
                                os.remove(core_out_path)
                                self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(core_out_path)}")
                            except Exception as e:
                                self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                    else:
                        # ì €ì¥ ì‹¤íŒ¨ ì‹œ, ì½”ì–´ ì™„ë£Œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        final_out_path = core_out_path
                except Exception as e:
                    final_out_path = core_out_path
                    self.append_log(f"[WARN] T2 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

                upsert_batch_job(
                    batch_id=bid,
                    out_excel=final_out_path,
                    status="merged",
                )

                # ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ (naive datetime ê¸°ì¤€)
                try:
                    c_at_str = local_job.get("created_at", "")
                    if c_at_str:
                        try:
                            # ISO í˜•ì‹ íŒŒì‹± (Zë¥¼ +00:00ìœ¼ë¡œ ë³€í™˜)
                            c_at = datetime.fromisoformat(c_at_str.replace('Z', '+00:00'))
                            # ì‹œê°„ëŒ€ ì •ë³´ê°€ ìˆìœ¼ë©´ naiveë¡œ ë³€í™˜ (datetime.now()ì™€ ì¼ì¹˜)
                            if c_at.tzinfo is not None:
                                c_at = c_at.replace(tzinfo=None)
                        except:
                            c_at = datetime.now()
                    else:
                        c_at = datetime.now()
                    finish_dt = datetime.now()
                    elapsed = (finish_dt - c_at).total_seconds()

                    append_run_history(
                        stage="Stage 2 Batch",
                        model_name=model_name,
                        reasoning_effort=local_job.get("effort", "medium"),
                        src_file=src_path,
                        out_file=info["out_excel_path"],
                        total_rows=info["total_rows"],
                        api_rows=info["merged"],
                        elapsed_seconds=elapsed,
                        total_in_tok=info["total_in_tok"],
                        total_out_tok=info["total_out_tok"],
                        total_reasoning_tok=info["total_reasoning_tok"],
                        input_cost_usd=info["input_cost_usd"],
                        output_cost_usd=info["output_cost_usd"],
                        total_cost_usd=info["total_cost_usd"],
                        start_dt=c_at,
                        finish_dt=finish_dt,
                        api_type="batch",
                        batch_id=bid,
                        success_rows=info["merged"],
                        fail_rows=info["missing"],
                    )
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨: {e}")

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— Stage2(Text) ì™„ë£Œ ìƒíƒœ ê¸°ë¡: T2-2(ë¶„ì„ì™„ë£Œ) (img ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                try:
                    root_name = get_root_filename(src_path)
                    JobManager.update_status(root_name, text_msg="T2-2(ë¶„ì„ì™„ë£Œ)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> T2-2(ë¶„ì„ì™„ë£Œ)")
                except Exception as e:
                    self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")

                self.append_log(f"âœ… ë³‘í•© ì™„ë£Œ: {os.path.basename(final_out_path)}")
                success_cnt += 1
            except Exception as e:
                self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: {e}")

        self.append_log(f"=== ì¼ê´„ ë³‘í•© ë (ì„±ê³µ: {success_cnt}, ì´ ë¹„ìš© ì¶”ì •: ${total_cost:.4f}) ===")
        self._load_jobs_all()
        messagebox.showinfo("ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©(ì¶”ì •): ${total_cost:.4f}")

    def _report_selected_unified(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        jobs = load_batch_jobs()
        targets = [bid for bid in ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") == "merged"]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ìƒíƒœê°€ 'merged'ì¸ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if messagebox.askyesno("ë¦¬í¬íŠ¸", f"ì„ íƒí•œ {len(targets)}ê±´ì˜ JSON ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_report_unified, args=(targets,))
            t.daemon = True
            t.start()

    def _run_report_unified(self, ids):
        self.append_log(f"--- JSON ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ({len(ids)}ê±´) ---")
        jobs = load_batch_jobs()
        all_reps = []
        
        # ê·¸ë£¹ë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ì¤‘ë³µ ì œê±° (ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ í•˜ë‚˜ì˜ ë³‘í•©ëœ íŒŒì¼ë§Œ ì‚¬ìš©)
        processed_groups = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ ê·¸ë£¹ ID
        processed_files = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ íŒŒì¼ ê²½ë¡œ (ê·¸ë£¹ ì—†ëŠ” ê²½ìš°)
        
        for bid in ids:
            local_job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not local_job: continue
            
            # ê·¸ë£¹ì´ ìˆëŠ” ê²½ìš°: ê·¸ë£¹ë³„ë¡œ í•œ ë²ˆë§Œ ì²˜ë¦¬
            group_id = local_job.get("batch_group_id")
            if group_id:
                if group_id in processed_groups:
                    self.append_log(f"â­ï¸ {bid}: ê·¸ë£¹ {group_id}ëŠ” ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                processed_groups.add(group_id)
            
            out_path = local_job.get("out_excel")
            if not out_path or not os.path.exists(out_path):
                self.append_log(f"âŒ íŒŒì¼ ëˆ„ë½: {bid}")
                continue
            
            # ê·¸ë£¹ ì—†ëŠ” ê²½ìš°: íŒŒì¼ ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
            if not group_id:
                if out_path in processed_files:
                    self.append_log(f"â­ï¸ {bid}: íŒŒì¼ {os.path.basename(out_path)}ëŠ” ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                processed_files.add(out_path)
            
            try:
                df = pd.read_excel(out_path)
                if "ST2_JSON" not in df.columns: continue
                for idx, row in df.iterrows():
                    st2 = safe_str(row.get("ST2_JSON", ""))
                    parsed = "âŒ ì‹¤íŒ¨"
                    kw_cnt = 0
                    if st2.strip().startswith("{"):
                        try:
                            js = json.loads(st2)
                            kw_cnt = len(js.get("search_keywords", []))
                            parsed = "âœ… ì„±ê³µ"
                        except: pass
                    
                    all_reps.append({
                        "Batch_ID": bid,
                        "í–‰ë²ˆí˜¸": idx+2,
                        "ìƒí’ˆì½”ë“œ": safe_str(row.get("ìƒí’ˆì½”ë“œ", "")),
                        "JSONìƒíƒœ": parsed,
                        "í‚¤ì›Œë“œìˆ˜": kw_cnt
                    })
            except Exception as e:
                self.append_log(f"âš ï¸ {bid} ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                pass

        if not all_reps:
            messagebox.showinfo("ì•Œë¦¼", "ë°ì´í„° ì—†ìŒ")
            return

        try:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            path = os.path.join(os.path.dirname(__file__), f"Stage2_Analysis_Report_{ts}.xlsx")
            pd.DataFrame(all_reps).to_excel(path, index=False)
            self.append_log(f"ğŸ“Š ë¦¬í¬íŠ¸ ì™„ë£Œ: {os.path.basename(path)}")
            if messagebox.askyesno("ì™„ë£Œ", "íŒŒì¼ì„ ì—¬ì‹œê² ìŠµë‹ˆê¹Œ?"): os.startfile(path)
        except Exception as e: messagebox.showerror("ì˜¤ë¥˜", str(e))

    def _archive_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if ids and messagebox.askyesno("ë³´ê´€", f"{len(ids)}ê±´ íœ´ì§€í†µ ì´ë™?"):
            archive_batch_job(ids, True)
            self._load_jobs_all()
            self._load_archive_list()

    def _restore_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ë³µêµ¬", f"{len(ids)}ê±´ ë³µêµ¬?"):
            archive_batch_job(ids, False)
            self._load_jobs_all()
            self._load_archive_list()

    def _hard_delete_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ì‚­ì œ", "ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            hard_delete_batch_job(ids)
            self._load_archive_list()

    def _move_failed_to_retry(self):
        """ì„ íƒëœ failed ë°°ì¹˜ë¥¼ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ëª©ë¡ìœ¼ë¡œ ì´ë™"""
        ids = self._get_selected_ids(self.tree_active)
        if not ids:
            messagebox.showwarning("ê²½ê³ ", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        jobs = load_batch_jobs()
        failed_jobs = []
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if job and job.get("status") == "failed":
                failed_jobs.append(job)
        
        if not failed_jobs:
            messagebox.showinfo("ì•Œë¦¼", "ì„ íƒí•œ ë°°ì¹˜ ì¤‘ 'failed' ìƒíƒœì¸ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not messagebox.askyesno("í™•ì¸", f"{len(failed_jobs)}ê°œì˜ failed ë°°ì¹˜ë¥¼ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ëª©ë¡ìœ¼ë¡œ ì´ë™í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            return
        
        # ì‹¤íŒ¨í•œ ì²­í¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        failed_chunks = []
        for job in failed_jobs:
            batch_id = job["batch_id"]
            jsonl_path = job.get("jsonl_path", "")
            
            # jsonl_pathê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì—‘ì…€ ê²½ë¡œì—ì„œ ì¶”ë¡ 
            if not jsonl_path or not os.path.exists(jsonl_path):
                src_excel = job.get("src_excel", "")
                if src_excel and os.path.exists(src_excel):
                    base, _ = os.path.splitext(src_excel)
                    jsonl_path = f"{base}_stage2_batch_input.jsonl"
                    # ì²­í¬ íŒŒì¼ì¸ ê²½ìš°ë„ í™•ì¸
                    if not os.path.exists(jsonl_path):
                        # ì²­í¬ íŒŒì¼ íŒ¨í„´ ì°¾ê¸°
                        base_dir = os.path.dirname(src_excel)
                        base_name = os.path.basename(src_excel)
                        base_name_only, _ = os.path.splitext(base_name)
                        # ì²­í¬ íŒŒì¼ íŒ¨í„´: *_chunk001.jsonl
                        chunk_index = job.get("chunk_index", 1)
                        jsonl_path = os.path.join(base_dir, f"{base_name_only}_stage2_batch_input_chunk{chunk_index:03d}.jsonl")
            
            # jsonl_pathê°€ ì—¬ì „íˆ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if not jsonl_path or not os.path.exists(jsonl_path):
                self.append_log(f"âš ï¸ {batch_id}: JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. (jsonl_path: {jsonl_path})")
                continue
            
            chunk_num = job.get("chunk_index", 1)
            error_msg = job.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜") or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
            is_token_limit = "token limit" in error_msg.lower() or "enqueued token limit" in error_msg.lower()
            
            failed_chunk_info = {
                "chunk_num": chunk_num,
                "chunk_file": jsonl_path,
                "error": error_msg,
                "is_token_limit": is_token_limit,
                "excel_path": job.get("src_excel", ""),
                "model_name": job.get("model", "gpt-5-mini"),
                "effort": job.get("effort", "medium"),
                "batch_group_id": job.get("batch_group_id", ""),
            }
            failed_chunks.append(failed_chunk_info)
        
        if not failed_chunks:
            messagebox.showwarning("ê²½ê³ ", "ì´ë™í•  ìˆ˜ ìˆëŠ” failed ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.\n(JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ)")
            return
        
        # ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ ìƒì„± ë˜ëŠ” ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€
        # ì›ë³¸ ì—‘ì…€ ê²½ë¡œì—ì„œ ê¸°ë³¸ ê²½ë¡œ ì¶”ë¡ 
        if failed_chunks[0].get("excel_path"):
            base_dir = os.path.dirname(failed_chunks[0]["excel_path"])
            base_name, _ = os.path.splitext(os.path.basename(failed_chunks[0]["excel_path"]))
            failed_info_path = os.path.join(base_dir, f"{base_name}_failed_chunks.json")
        else:
            # ì—‘ì…€ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ì— ì €ì¥
            failed_info_path = os.path.join(os.path.dirname(__file__), "failed_chunks.json")
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì½ì–´ì„œ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        existing_chunks = []
        if os.path.exists(failed_info_path):
            try:
                with open(failed_info_path, "r", encoding="utf-8") as f:
                    existing_chunks = json.load(f)
            except Exception as e:
                self.append_log(f"[WARN] ê¸°ì¡´ ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        # ì¤‘ë³µ ì œê±° (chunk_file ê¸°ì¤€)
        existing_files = {chunk.get("chunk_file", "") for chunk in existing_chunks}
        new_chunks = [chunk for chunk in failed_chunks if chunk.get("chunk_file", "") not in existing_files]
        
        if new_chunks:
            # ê¸°ì¡´ í•­ëª©ê³¼ ìƒˆ í•­ëª© ë³‘í•©
            all_chunks = existing_chunks + new_chunks
            try:
                with open(failed_info_path, "w", encoding="utf-8") as f:
                    json.dump(all_chunks, f, ensure_ascii=False, indent=2)
                
                # ì¬ì‹œë„ ëª©ë¡ ì—…ë°ì´íŠ¸
                self.failed_chunks_file_var.set(failed_info_path)
                self._load_failed_chunks_from_file(failed_info_path)
                
                self.append_log(f"[INFO] {len(new_chunks)}ê°œì˜ failed ë°°ì¹˜ë¥¼ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ëª©ë¡ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                self.append_log(f"[INFO] ì‹¤íŒ¨ ì •ë³´ íŒŒì¼: {os.path.basename(failed_info_path)}")
                
                # ì¬ì‹œë„ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
                self.main_tabs.select(self.tab_merge)
                
                messagebox.showinfo("ì™„ë£Œ", f"{len(new_chunks)}ê°œì˜ failed ë°°ì¹˜ë¥¼ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ëª©ë¡ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\n\níŒŒì¼: {os.path.basename(failed_info_path)}\n\nì¬ì‹œë„ íƒ­ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
            except Exception as e:
                self.append_log(f"[ERROR] ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                messagebox.showerror("ì˜¤ë¥˜", f"ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨:\n{e}")
        else:
            messagebox.showinfo("ì•Œë¦¼", "ëª¨ë“  failed ë°°ì¹˜ê°€ ì´ë¯¸ ì¬ì‹œë„ ëª©ë¡ì— ìˆìŠµë‹ˆë‹¤.")

    def _on_tree_double_click(self, event):
        """ë”ë¸”í´ë¦­ ì‹œ: ê·¸ë£¹ í—¤ë”ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°, ë°°ì¹˜ë©´ ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™"""
        sel = self.tree_active.selection()
        if not sel: return
        
        item = sel[0]
        vals = self.tree_active.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš°: ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
        if not batch_id:
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            children = self.tree_active.get_children(item)
            if children:
                # ìì‹ì´ ìˆìœ¼ë©´ ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
                if self.tree_active.item(item, 'open'):
                    self.tree_active.item(item, open=False)
                else:
                    self.tree_active.item(item, open=True)
        else:
            # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°: ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™
            self.batch_id_var.set(batch_id)
            self.main_tabs.select(self.tab_merge)

    # ----------------------------------------------------
    # Tab 3: Manual
    # ----------------------------------------------------
    def _init_tab_merge(self):
        container = ttk.Frame(self.tab_merge, padding=20)
        container.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì„¹ì…˜
        f_retry = ttk.LabelFrame(container, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", padding=15)
        f_retry.pack(fill='x', pady=(0, 15))
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        f_list = ttk.Frame(f_retry)
        f_list.pack(fill='both', expand=True, pady=(0, 10))
        ttk.Label(f_list, text="ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(anchor='w')
        
        # Treeviewë¡œ ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        list_frame = ttk.Frame(f_list)
        list_frame.pack(fill='both', expand=True, pady=5)
        
        scrollbar = ttk.Scrollbar(list_frame)
        scrollbar.pack(side='right', fill='y')
        
        self.failed_chunks_tree = ttk.Treeview(list_frame, columns=("chunk_num", "file_name", "error_type"), 
                                                show='headings', height=4, yscrollcommand=scrollbar.set)
        scrollbar.config(command=self.failed_chunks_tree.yview)
        
        self.failed_chunks_tree.heading("chunk_num", text="ì²­í¬ ë²ˆí˜¸")
        self.failed_chunks_tree.heading("file_name", text="íŒŒì¼ëª…")
        self.failed_chunks_tree.heading("error_type", text="ì˜¤ë¥˜ ìœ í˜•")
        
        self.failed_chunks_tree.column("chunk_num", width=80, anchor="center")
        self.failed_chunks_tree.column("file_name", width=300, anchor="w")
        self.failed_chunks_tree.column("error_type", width=150, anchor="center")
        
        self.failed_chunks_tree.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ ì…ë ¥
        f_file = ttk.Frame(f_retry)
        f_file.pack(fill='x', pady=(10, 0))
        ttk.Label(f_file, text="ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼:", font=("ë§‘ì€ ê³ ë”•", 9)).pack(side='left')
        self.failed_chunks_file_var = tk.StringVar()
        ttk.Entry(f_file, textvariable=self.failed_chunks_file_var, width=50, font=("Consolas", 9)).pack(side='left', padx=5, fill='x', expand=True)
        btn_select = ttk.Button(f_file, text="ğŸ“‚ ì°¾ê¸°", command=self._select_failed_chunks_file)
        btn_select.pack(side='left', padx=5)
        btn_retry = ttk.Button(f_file, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", command=self._retry_failed_chunks, style="Success.TButton")
        btn_retry.pack(side='left', padx=5)
        ToolTip(btn_retry, "í† í° ì œí•œ ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ì‹¤íŒ¨í•œ ì²­í¬ë¥¼ ì¬ì‹œë„í•©ë‹ˆë‹¤.\nì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ì„ ì„ íƒí•˜ê³  ì¬ì‹œë„ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
        
        f_in = ttk.LabelFrame(container, text="ê°œë³„ ì‘ì—…", padding=15)
        f_in.pack(fill='x', pady=(0, 15))
        ttk.Label(f_in, text="Batch ID:").pack(side='left')
        ttk.Entry(f_in, textvariable=self.batch_id_var, width=45, font=("Consolas", 10)).pack(side='left', padx=10)
        
        f_btn = ttk.Frame(container)
        f_btn.pack(fill='x', pady=20)
        ttk.Button(f_btn, text="1. ê²°ê³¼ ë³‘í•©", command=self._start_merge, style="Primary.TButton").pack(fill='x', pady=5)
        ttk.Button(f_btn, text="2. ë‹¨ì¼ ë¦¬í¬íŠ¸", command=self._start_diff_report).pack(fill='x', pady=5)

    def _start_merge(self):
        t = threading.Thread(target=self._run_merge)
        t.daemon = True
        t.start()

    def _run_merge(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_merge_multi([bid])

    def _start_diff_report(self):
        t = threading.Thread(target=self._run_diff_report)
        t.daemon = True
        t.start()

    def _run_diff_report(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_report_unified([bid])
    
    def _handle_failed_chunks(self, failed_info_path, failed_chunk_files):
        """ì‹¤íŒ¨í•œ ì²­í¬ê°€ ìˆì„ ë•Œ GUIì— ìë™ìœ¼ë¡œ í‘œì‹œí•˜ê³  ì•Œë¦¼"""
        # ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ìë™ ì„¤ì •
        self.failed_chunks_file_var.set(failed_info_path)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ì„ Treeviewì— í‘œì‹œ
        for item in self.failed_chunks_tree.get_children():
            self.failed_chunks_tree.delete(item)
        
        for failed_info in failed_chunk_files:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
            file_name = os.path.basename(chunk_file)
            
            self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        
        # ì•Œë¦¼ ë©”ì‹œì§€ ë° íƒ­ ì „í™˜
        failed_count = len(failed_chunk_files)
        token_limit_count = sum(1 for f in failed_chunk_files if f.get("is_token_limit", False))
        
        msg = f"âš ï¸ {failed_count}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\n"
        if token_limit_count > 0:
            msg += f"â€¢ í† í° ì œí•œ ì˜¤ë¥˜: {token_limit_count}ê°œ\n"
        msg += f"â€¢ ì‹¤íŒ¨ ì •ë³´ê°€ ìë™ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        msg += f"â€¢ '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        messagebox.showwarning("ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨", msg)
        
        # ì¬ì‹œë„ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
        self.main_tabs.select(self.tab_merge)
    
    def _select_failed_chunks_file(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ"""
        path = filedialog.askopenfilename(
            title="ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ",
            filetypes=[("JSON íŒŒì¼", "*.json"), ("ëª¨ë“  íŒŒì¼", "*.*")]
        )
        if path:
            self.failed_chunks_file_var.set(path)
            # íŒŒì¼ì„ ì„ íƒí•˜ë©´ ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(path)
    
    def _load_failed_chunks_from_file(self, file_path):
        """ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ì„ ì½ì–´ì„œ ëª©ë¡ì— í‘œì‹œ"""
        if not os.path.exists(file_path):
            return
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ê¸°ì¡´ ëª©ë¡ ì‚­ì œ
            for item in self.failed_chunks_tree.get_children():
                self.failed_chunks_tree.delete(item)
            
            # ëª©ë¡ì— ì¶”ê°€
            for failed_info in failed_chunks:
                chunk_num = failed_info.get("chunk_num", 0)
                chunk_file = failed_info.get("chunk_file", "")
                error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
                file_name = os.path.basename(chunk_file)
                
                self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        except Exception as e:
            self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    def _retry_failed_chunks(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„"""
        failed_file = self.failed_chunks_file_var.get().strip()
        if not failed_file:
            messagebox.showwarning("ê²½ê³ ", "ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        if not os.path.exists(failed_file):
            messagebox.showerror("ì˜¤ë¥˜", f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{failed_file}")
            return
        
        if not self.api_key_var.get():
            messagebox.showwarning("ê²½ê³ ", "API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        try:
            with open(failed_file, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(failed_file)
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨:\n{e}")
            return
        
        if not failed_chunks:
            messagebox.showinfo("ì•Œë¦¼", "ì¬ì‹œë„í•  ì‹¤íŒ¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if messagebox.askyesno("í™•ì¸", f"{len(failed_chunks)}ê°œ ì‹¤íŒ¨í•œ ì²­í¬ë¥¼ ì¬ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_retry_failed_chunks, args=(failed_chunks,))
            t.daemon = True
            t.start()
    
    def _run_retry_failed_chunks(self, failed_chunks):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì‹¤í–‰"""
        key = self.api_key_var.get().strip()
        import httpx
        timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
        
        self.append_log(f"[RETRY] ì‹¤íŒ¨í•œ ì²­í¬ {len(failed_chunks)}ê°œ ì¬ì‹œë„ ì‹œì‘...")
        
        retry_batch_ids = []
        for failed_info in failed_chunks:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            excel_path = failed_info.get("excel_path", "")
            model_name = failed_info.get("model_name", "gpt-5-mini")
            effort = failed_info.get("effort", "medium")
            batch_group_id = failed_info.get("batch_group_id", "")
            
            if not os.path.exists(chunk_file):
                self.append_log(f"âš ï¸ ì²­í¬ {chunk_num}: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_file}")
                continue
            
            self.append_log(f"[RETRY] ì²­í¬ {chunk_num} ì¬ì‹œë„ ì¤‘... ({os.path.basename(chunk_file)})")
            
            try:
                batch = create_batch_from_jsonl(
                    client=client,
                    jsonl_path=chunk_file,
                    excel_path=excel_path,
                    model_name=model_name,
                    log_func=self.append_log,
                )
                
                batch_id = batch.id
                retry_batch_ids.append(batch_id)
                self.append_log(f"âœ… ì²­í¬ {chunk_num} ì¬ì‹œë„ ì„±ê³µ: {batch_id}")
                
                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=excel_path,
                    jsonl_path=chunk_file,
                    model=model_name,
                    effort=effort,
                    status=batch.status,
                    output_file_id=None,
                    batch_group_id=batch_group_id,
                    chunk_index=chunk_num,
                )
                
            except Exception as e:
                self.append_log(f"âŒ ì²­í¬ {chunk_num} ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        if retry_batch_ids:
            self.append_log(f"âœ… ì¬ì‹œë„ ì™„ë£Œ: {len(retry_batch_ids)}ê°œ ë°°ì¹˜ ìƒì„±ë¨")
            # ë°°ì¹˜ ëª©ë¡ ê°±ì‹ 
            self.after(0, lambda: [
                self._load_jobs_all(),
                self._load_archive_list(),
                self.main_tabs.select(self.tab_manage),  # ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
                messagebox.showinfo("ì™„ë£Œ", f"{len(retry_batch_ids)}ê°œ ì²­í¬ ì¬ì‹œë„ ì„±ê³µ:\n{', '.join(retry_batch_ids[:5])}{'...' if len(retry_batch_ids) > 5 else ''}\n\në°°ì¹˜ ê´€ë¦¬ íƒ­ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.")
            ])
        else:
            self.append_log(f"âš ï¸ ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.after(0, lambda: messagebox.showwarning("ê²½ê³ ", "ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤."))

if __name__ == "__main__":
    app = Stage2BatchGUI()
    app.mainloop()