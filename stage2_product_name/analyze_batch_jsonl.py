"""
ë°°ì¹˜ JSONL íŒŒì¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- custom_id ì¤‘ë³µ ì²´í¬
- ì²­í¬ íŒŒì¼ë³„ ìš”ì²­ ìˆ˜ ë° í¬ê¸° ë¶„ì„
- ì „ì²´ í†µê³„ ì œê³µ
"""

import json
import os
import glob
from collections import Counter

def analyze_jsonl_file(jsonl_path):
    """ë‹¨ì¼ JSONL íŒŒì¼ ë¶„ì„"""
    if not os.path.exists(jsonl_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")
        return None
    
    custom_ids = []
    total_size = os.path.getsize(jsonl_path)
    line_count = 0
    
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            line_count += 1
            try:
                obj = json.loads(line)
                custom_id = obj.get('custom_id', '')
                if custom_id:
                    custom_ids.append(custom_id)
            except Exception as e:
                print(f"âš ï¸ ë¼ì¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    # ì¤‘ë³µ ì²´í¬
    id_counter = Counter(custom_ids)
    duplicates = {k: v for k, v in id_counter.items() if v > 1}
    
    return {
        'file': os.path.basename(jsonl_path),
        'path': jsonl_path,
        'total_lines': line_count,
        'unique_custom_ids': len(set(custom_ids)),
        'total_custom_ids': len(custom_ids),
        'duplicates': duplicates,
        'duplicate_count': len(duplicates),
        'file_size_mb': total_size / (1024 * 1024),
        'avg_size_per_request_mb': (total_size / line_count) / (1024 * 1024) if line_count > 0 else 0
    }

def analyze_chunk_files(base_dir, pattern):
    """ì²­í¬ íŒŒì¼ë“¤ ë¶„ì„"""
    os.chdir(base_dir)
    files = sorted(glob.glob(pattern))
    
    if not files:
        print(f"âŒ íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
        return
    
    print(f"=" * 80)
    print(f"ğŸ“Š ë°°ì¹˜ JSONL íŒŒì¼ ë¶„ì„")
    print(f"=" * 80)
    print(f"ì´ ì²­í¬ íŒŒì¼ ìˆ˜: {len(files)}\n")
    
    all_custom_ids = []
    all_duplicates = {}
    total_requests = 0
    total_size = 0
    
    # ê° ì²­í¬ íŒŒì¼ ë¶„ì„
    chunk_results = []
    for i, file_path in enumerate(files, 1):
        result = analyze_jsonl_file(file_path)
        if result:
            chunk_results.append(result)
            all_custom_ids.extend([cid for _ in range(result['total_custom_ids'])])
            all_duplicates.update(result['duplicates'])
            total_requests += result['total_lines']
            total_size += result['file_size_mb']
            
            # ì²­í¬ë³„ ìƒì„¸ ì •ë³´
            status = "âš ï¸ ì¤‘ë³µ ìˆìŒ" if result['duplicate_count'] > 0 else "âœ… ì •ìƒ"
            print(f"ì²­í¬ {i:2d}: {result['file']}")
            print(f"  - ìš”ì²­ ìˆ˜: {result['total_lines']}ê°œ")
            print(f"  - ê³ ìœ  custom_id: {result['unique_custom_ids']}ê°œ")
            print(f"  - íŒŒì¼ í¬ê¸°: {result['file_size_mb']:.2f} MB")
            print(f"  - ìš”ì²­ë‹¹ í‰ê·  í¬ê¸°: {result['avg_size_per_request_mb']:.4f} MB")
            if result['duplicate_count'] > 0:
                print(f"  - âš ï¸ ì¤‘ë³µ custom_id: {result['duplicate_count']}ê°œ")
                for dup_id, count in list(result['duplicates'].items())[:5]:
                    print(f"      â€¢ {dup_id}: {count}íšŒ")
                if result['duplicate_count'] > 5:
                    print(f"      ... ì™¸ {result['duplicate_count'] - 5}ê°œ")
            print(f"  - ìƒíƒœ: {status}\n")
    
    # ì „ì²´ í†µê³„
    print(f"=" * 80)
    print(f"ğŸ“ˆ ì „ì²´ í†µê³„")
    print(f"=" * 80)
    print(f"ì´ ì²­í¬ íŒŒì¼ ìˆ˜: {len(files)}")
    print(f"ì´ ìš”ì²­ ìˆ˜: {total_requests}ê°œ")
    print(f"ê³ ìœ  custom_id ìˆ˜: {len(set(all_custom_ids))}ê°œ")
    print(f"ì´ íŒŒì¼ í¬ê¸°: {total_size:.2f} MB")
    print(f"í‰ê·  ì²­í¬ í¬ê¸°: {total_size / len(files):.2f} MB")
    print(f"ìš”ì²­ë‹¹ í‰ê·  í¬ê¸°: {(total_size * 1024 * 1024) / total_requests / (1024 * 1024):.4f} MB")
    
    # ì „ì²´ ì¤‘ë³µ ì²´í¬
    all_id_counter = Counter(all_custom_ids)
    all_duplicates_final = {k: v for k, v in all_id_counter.items() if v > 1}
    
    if all_duplicates_final:
        print(f"\nâš ï¸ ì „ì²´ ì¤‘ë³µ custom_id: {len(all_duplicates_final)}ê°œ")
        print(f"ì¤‘ë³µëœ ìš”ì²­ ì´ ìˆ˜: {sum(v - 1 for v in all_duplicates_final.values())}ê°œ")
        print(f"\nì¤‘ë³µëœ custom_id ëª©ë¡ (ì²˜ìŒ 10ê°œ):")
        for dup_id, count in list(all_duplicates_final.items())[:10]:
            print(f"  â€¢ {dup_id}: {count}íšŒ (ì¤‘ë³µ {count - 1}íšŒ)")
        if len(all_duplicates_final) > 10:
            print(f"  ... ì™¸ {len(all_duplicates_final) - 10}ê°œ")
        
        # ì¤‘ë³µì´ ë°œìƒí•œ ì²­í¬ íŒŒì¼ ì°¾ê¸°
        print(f"\nì¤‘ë³µì´ ë°œìƒí•œ ì²­í¬ íŒŒì¼:")
        for result in chunk_results:
            if result['duplicate_count'] > 0:
                print(f"  - {result['file']}: {result['duplicate_count']}ê°œ ì¤‘ë³µ")
    else:
        print(f"\nâœ… ì¤‘ë³µ ì—†ìŒ: ëª¨ë“  custom_idê°€ ê³ ìœ í•©ë‹ˆë‹¤.")
    
    # ì˜ˆìƒ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (ëŒ€ëµì )
    # ê° ìš”ì²­ë‹¹ í‰ê·  í¬ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •
    avg_size_per_request = (total_size * 1024 * 1024) / total_requests if total_requests > 0 else 0
    print(f"\nğŸ’¡ ì°¸ê³  ì •ë³´:")
    print(f"  - ìš”ì²­ë‹¹ í‰ê·  í¬ê¸°: {avg_size_per_request / (1024 * 1024):.4f} MB")
    print(f"  - 180MB ê¸°ì¤€ ì˜ˆìƒ ì²­í¬ ìˆ˜: {total_size / 180:.1f}ê°œ")
    print(f"  - ì‹¤ì œ ì²­í¬ ìˆ˜: {len(files)}ê°œ")
    if total_size / 180 < len(files):
        print(f"  - âš ï¸ ì˜ˆìƒë³´ë‹¤ {len(files) - int(total_size / 180)}ê°œ ë” ë§ì€ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f"\n" + "=" * 80)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  python analyze_batch_jsonl.py <JSONL_íŒŒì¼_ê²½ë¡œ_ë˜ëŠ”_ë””ë ‰í† ë¦¬>")
        print("\nì˜ˆì‹œ:")
        print("  python analyze_batch_jsonl.py ì¤‘ë³µì œê±°_ì‹ ë…„DBì˜¤ë„ˆí´ëœ_1_T1_I3_stage2_batch_input.jsonl")
        print("  python analyze_batch_jsonl.py . ì¤‘ë³µì œê±°_ì‹ ë…„DBì˜¤ë„ˆí´ëœ_1_T1_I3_stage2_batch_input_chunk*.jsonl")
        sys.exit(1)
    
    target = sys.argv[1]
    
    if os.path.isfile(target):
        # ë‹¨ì¼ íŒŒì¼ ë¶„ì„
        result = analyze_jsonl_file(target)
        if result:
            print(f"ğŸ“Š íŒŒì¼ ë¶„ì„: {result['file']}")
            print(f"ìš”ì²­ ìˆ˜: {result['total_lines']}ê°œ")
            print(f"ê³ ìœ  custom_id: {result['unique_custom_ids']}ê°œ")
            print(f"íŒŒì¼ í¬ê¸°: {result['file_size_mb']:.2f} MB")
            if result['duplicate_count'] > 0:
                print(f"âš ï¸ ì¤‘ë³µ custom_id: {result['duplicate_count']}ê°œ")
                for dup_id, count in result['duplicates'].items():
                    print(f"  â€¢ {dup_id}: {count}íšŒ")
            else:
                print(f"âœ… ì¤‘ë³µ ì—†ìŒ")
    elif os.path.isdir(target):
        # ë””ë ‰í† ë¦¬ì—ì„œ íŒ¨í„´ ê²€ìƒ‰
        if len(sys.argv) >= 3:
            pattern = sys.argv[2]
        else:
            pattern = "*_stage2_batch_input*.jsonl"
        analyze_chunk_files(target, pattern)
    else:
        print(f"âŒ íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target}")
