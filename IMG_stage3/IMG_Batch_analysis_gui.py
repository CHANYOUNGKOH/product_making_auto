"""
IMG_Batch_analysis_gui.py

Stage 3-1: ì¸ë„¤ì¼ ì´ë¯¸ì§€ ë¶„ì„ (ë°°ì¹˜/ëŒ€ëŸ‰)
- ê¸°ëŠ¥: Batch JSONL ìƒì„± -> ì—…ë¡œë“œ -> ì‹¤í–‰ -> ê²°ê³¼ ë³‘í•©
- IMG_analysis_core.pyë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬
- ì…ë ¥: I2 ë˜ëŠ” I3 íŒŒì¼ë§Œ í—ˆìš©
- ì¶œë ¥: í•­ìƒ I3ë¡œ ê³ ì •
"""

import os
import json
import re
import threading
import subprocess
import platform
from datetime import datetime
from typing import Optional

import pandas as pd
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, Menu
from tkinter.scrolledtext import ScrolledText

from openai import OpenAI

from IMG_analysis_core import (
    API_KEY_FILE,
    DEFAULT_MODEL,
    load_api_key_from_file,
    save_api_key_to_file,
    build_analysis_messages,
    MODEL_PRICING_USD_PER_MTOK,
)

# ========================================================
# ë©”ì¸ ëŸ°ì²˜ ì—°ë™ìš© JobManager & íŒŒì¼ëª… ìœ í‹¸
# ========================================================
def get_root_filename(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´(_T*_I* ë˜ëŠ” _t*_i*, T4(ì™„)_I* í¬í•¨) ë° ê¸°íƒ€ ê¼¬ë¦¬í‘œë¥¼ ë–¼ê³  ì›ë³¸ëª…(Key)ë§Œ ì¶”ì¶œ
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0.xlsx -> ë‚˜ì´í‚¤.xlsx
    ì˜ˆ: ì•„ë””ë‹¤ìŠ¤_T3_I2.xlsx -> ì•„ë””ë‹¤ìŠ¤.xlsx
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0(ì—…ì™„).xlsx -> ë‚˜ì´í‚¤.xlsx
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0_T1_I1.xlsx -> ë‚˜ì´í‚¤.xlsx (ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ ì œê±°)
    ì˜ˆ: ë‚˜ì´í‚¤_T4(ì™„)_I2.xlsx -> ë‚˜ì´í‚¤.xlsx
    """
    name = os.path.basename(filename)
    base, ext = os.path.splitext(name)
    
    # 1. ë²„ì „ íŒ¨í„´ (_Tìˆ«ì(ê´„í˜¸)?_Iìˆ«ì ë˜ëŠ” _tìˆ«ì(ê´„í˜¸)?_iìˆ«ì) ë°˜ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    # íŒ¨í„´ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì œê±°, T4(ì™„)_I* íŒ¨í„´ë„ í¬í•¨
    while True:
        new_base = re.sub(r"_[Tt]\d+\([^)]*\)_[Ii]\d+", "", base, flags=re.IGNORECASE)  # T4(ì™„)_I* íŒ¨í„´ ì œê±°
        new_base = re.sub(r"_[Tt]\d+_[Ii]\d+", "", new_base, flags=re.IGNORECASE)  # ì¼ë°˜ T*_I* íŒ¨í„´ ì œê±°
        if new_base == base:
            break
        base = new_base
    
    # 2. ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„), (ì™„ë£Œ) ë“±) - ë²„ì „ íŒ¨í„´ì˜ ê´„í˜¸ëŠ” ì´ë¯¸ ì œê±°ë¨
    base = re.sub(r"\([^)]*\)", "", base)
    
    # 3. ê¸°íƒ€ êµ¬í˜• ê¼¬ë¦¬í‘œ ì œê±° (í˜¸í™˜ì„± ìœ ì§€)
    suffixes = ["_img_analysis_done", "_img_analysis_batch_done", "_stage1_mapping", "_stage1_img_mapping", "_stage2_analysis", "_stage3_done", "_stage4_2_done", "_with_images"]
    for s in suffixes:
        base = base.replace(s, "")
    
    # 4. ëì— ë‚¨ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    base = base.rstrip("_")
        
    return base + ext


def get_i3_output_path(input_path: str) -> str:
    """
    ì…ë ¥ íŒŒì¼ëª…ì„ ë¶„ì„í•´ì„œ I3ë¡œ ê³ ì •ëœ ì¶œë ¥ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì…ë ¥: I2 ë˜ëŠ” I3 íŒŒì¼ (ì˜ˆ: ìƒí’ˆ_T3_I2.xlsx, ìƒí’ˆ_T3_I3.xlsx, ìƒí’ˆ_T4(ì™„)_I2.xlsx)
    ì¶œë ¥: í•­ìƒ I3 (ì˜ˆ: ìƒí’ˆ_T3_I3.xlsx, ìƒí’ˆ_T4(ì™„)_I3.xlsx)
    """
    dir_name = os.path.dirname(input_path)
    base_name = os.path.basename(input_path)
    name_only, ext = os.path.splitext(base_name)

    # T4(ì™„)_I* ë˜ëŠ” ì¼ë°˜ _T*_I* íŒ¨í„´ ë§¤ì¹­
    pattern = r"_T(\d+)(\([^)]+\))?_I(\d+)$"
    match = re.search(pattern, name_only, re.IGNORECASE)

    if match:
        current_t = int(match.group(1))
        t_suffix = match.group(2) or ""  # (ì™„) ë¶€ë¶„ì´ ìˆìœ¼ë©´ ìœ ì§€
        original_name = name_only[: match.start()]
    else:
        # ë²„ì „ ì •ë³´ê°€ ì—†ìœ¼ë©´ T ë²„ì „ ì¶”ì¶œ ì‹œë„ (ê´„í˜¸ í¬í•¨ ê°€ëŠ¥)
        t_match = re.search(r"_T(\d+)(\([^)]+\))?", name_only, re.IGNORECASE)
        if t_match:
            current_t = int(t_match.group(1))
            t_suffix = t_match.group(2) or ""
            original_name = name_only[: t_match.start()]
        else:
            current_t = 0
            t_suffix = ""
            original_name = name_only

    # í•­ìƒ I3ë¡œ ê³ ì •, T ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì˜ˆ: T4(ì™„) ë˜ëŠ” T4)
    new_filename = f"{original_name}_T{current_t}{t_suffix}_I3{ext}"
    return os.path.join(dir_name, new_filename)


class JobManager:
    DB_FILE = None

    @classmethod
    def find_db_path(cls):
        if cls.DB_FILE and os.path.exists(cls.DB_FILE):
            return cls.DB_FILE

        current_dir = os.path.dirname(os.path.abspath(__file__))
        search_dirs = [
            current_dir,
            os.path.abspath(os.path.join(current_dir, "..")),
            os.path.abspath(os.path.join(current_dir, "..", "..")),
        ]

        for d in search_dirs:
            target = os.path.join(d, "job_history.json")
            if os.path.exists(target):
                cls.DB_FILE = target
                return target

        default_path = os.path.abspath(os.path.join(current_dir, "..", "job_history.json"))
        cls.DB_FILE = default_path
        return default_path

    @classmethod
    def load_jobs(cls):
        db_path = cls.find_db_path()
        if not os.path.exists(db_path):
            return {}
        try:
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    @classmethod
    def update_status(cls, filename, text_msg=None, img_msg=None, img_s3_1_msg=None, img_s3_2_msg=None):
        """
        ì‘ì—… ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            filename: íŒŒì¼ëª… (root filename)
            text_msg: í…ìŠ¤íŠ¸ ìƒíƒœ ë©”ì‹œì§€
            img_msg: ì´ë¯¸ì§€ ì „ì²´ ìƒíƒœ ë©”ì‹œì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
            img_s3_1_msg: Stage 3-1 (ì¸ë„¤ì¼ ë¶„ì„) ìƒíƒœ ë©”ì‹œì§€
            img_s3_2_msg: Stage 3-2 (ì „ì²˜ë¦¬) ìƒíƒœ ë©”ì‹œì§€
        """
        db_path = cls.find_db_path()
        data = cls.load_jobs()
        now = datetime.now().strftime("%m-%d %H:%M")

        if filename not in data:
            data[filename] = {
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "text_status": "ëŒ€ê¸°",
                "text_time": "-",
                "image_status": "ëŒ€ê¸°",
                "image_time": "-",
                "image_s3_1_status": "-",  # Stage 3-1: ì¸ë„¤ì¼ ë¶„ì„
                "image_s3_1_time": "-",
                "image_s3_2_status": "-",  # Stage 3-2: ì „ì²˜ë¦¬
                "image_s3_2_time": "-",
                "memo": "",
            }

        if text_msg:
            data[filename]["text_status"] = text_msg
            data[filename]["text_time"] = now
        
        if img_msg:
            # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ image_statusë„ ì—…ë°ì´íŠ¸
            data[filename]["image_status"] = img_msg
            data[filename]["image_time"] = now
        
        if img_s3_1_msg:
            data[filename]["image_s3_1_status"] = img_s3_1_msg
            data[filename]["image_s3_1_time"] = now
            # image_status í†µí•© ì—…ë°ì´íŠ¸ (S3-1, S3-2 ì ‘ë‘ì‚¬ ì œê±°)
            parts = []
            if data[filename].get("image_s3_1_status", "-") != "-":
                parts.append(data[filename]['image_s3_1_status'])  # "I3-1 (ì§„í–‰ì¤‘)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if data[filename].get("image_s3_2_status", "-") != "-":
                parts.append(data[filename]['image_s3_2_status'])  # "I3-2 (ì™„ë£Œ)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if parts:
                data[filename]["image_status"] = " / ".join(parts)
                data[filename]["image_time"] = now
        
        if img_s3_2_msg:
            data[filename]["image_s3_2_status"] = img_s3_2_msg
            data[filename]["image_s3_2_time"] = now
            # image_status í†µí•© ì—…ë°ì´íŠ¸ (S3-1, S3-2 ì ‘ë‘ì‚¬ ì œê±°)
            parts = []
            if data[filename].get("image_s3_1_status", "-") != "-":
                parts.append(data[filename]['image_s3_1_status'])  # "I3-1 (ì§„í–‰ì¤‘)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if data[filename].get("image_s3_2_status", "-") != "-":
                parts.append(data[filename]['image_s3_2_status'])  # "I3-2 (ì™„ë£Œ)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if parts:
                data[filename]["image_status"] = " / ".join(parts)
                data[filename]["image_time"] = now

        data[filename]["last_update"] = now

        try:
            with open(db_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"[JobManager Error] {e}")


def safe_save_excel(df: pd.DataFrame, path: str) -> bool:
    """ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆì–´ ì €ì¥ì´ ì•ˆ ë  ë•Œ ì¬ì‹œë„ë¥¼ ìœ ë„í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            df.to_excel(path, index=False)
            return True
        except PermissionError:
            if not messagebox.askretrycancel(
                "ì €ì¥ ì‹¤íŒ¨",
                f"ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤!\n[{os.path.basename(path)}]\n\n"
                "íŒŒì¼ì„ ë‹«ê³  'ë‹¤ì‹œ ì‹œë„'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
            ):
                return False
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False


# === ê¸°ë³¸ ì„¤ì • ===
BATCH_JOBS_FILE = os.path.join(os.path.dirname(__file__), "img_analysis_batch_jobs.json")

# --- UI ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ---
COLOR_BG = "#F8F9FA"
COLOR_WHITE = "#FFFFFF"
COLOR_PRIMARY = "#4A90E2"
COLOR_PRIMARY_HOVER = "#357ABD"
COLOR_SUCCESS = "#28A745"
COLOR_SUCCESS_HOVER = "#218838"
COLOR_DANGER = "#DC3545"
COLOR_DANGER_HOVER = "#C82333"
COLOR_TEXT = "#333333"
COLOR_HEADER = "#E9ECEF"

def get_seoul_now():
    try:
        from pytz import timezone
        return datetime.now(timezone("Asia/Seoul"))
    except:
        return datetime.now()

# ========================================================
# ë°°ì¹˜ ì¡ ê´€ë¦¬ (JSON DB)
# ========================================================
def load_batch_jobs():
    if not os.path.exists(BATCH_JOBS_FILE):
        return []
    try:
        with open(BATCH_JOBS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return []

def save_batch_jobs(jobs):
    try:
        with open(BATCH_JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"[Error] ì¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_excel_name_from_path(path: str) -> str:
    """ì „ì²´ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ"""
    if not path:
        return "-"
    return os.path.basename(path)

def upsert_batch_job(batch_id, **kwargs):
    jobs = load_batch_jobs()
    found = False
    now_str = datetime.now().isoformat()
    
    for j in jobs:
        if j["batch_id"] == batch_id:
            if kwargs.get("status") == "completed" and j.get("status") != "completed":
                if "completed_at" not in kwargs:
                    j["completed_at"] = now_str
            j.update(kwargs)
            j["updated_at"] = now_str
            found = True
            break
            
    if not found:
        new_job = {
            "batch_id": batch_id,
            "created_at": now_str,
            "updated_at": now_str,
            "completed_at": "",
            "archived": False,
            **kwargs
        }
        jobs.insert(0, new_job)
    save_batch_jobs(jobs)

def archive_batch_job(batch_ids, archive=True):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    for j in jobs:
        if j["batch_id"] in batch_ids:
            j["archived"] = archive
    save_batch_jobs(jobs)

def hard_delete_batch_job(batch_ids):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    jobs = [j for j in jobs if j["batch_id"] not in batch_ids]
    save_batch_jobs(jobs)

# ========================================================
# GUI Class
# ========================================================
class ImageAnalysisBatchGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Stage 3-1: ì¸ë„¤ì¼ ì´ë¯¸ì§€ ë¶„ì„ (ë°°ì¹˜/ëŒ€ëŸ‰)")
        self.geometry("1250x950")
        
        self.api_key_var = tk.StringVar()
        
        # íŒŒì¼ ë³€ìˆ˜
        self.src_file_var = tk.StringVar()
        self.skip_exist_var = tk.BooleanVar(value=True)
        self.skip_bad_label_var = tk.BooleanVar(value=True)  # 'bad' ë¼ë²¨ í–‰ ì œì™¸ (ê¸°ë³¸ê°’: True)
        
        # ëª¨ë¸ ì„¤ì • ë³€ìˆ˜
        self.model_var = tk.StringVar(value="gpt-5-mini")
        self.effort_var = tk.StringVar(value="low")
        
        # íƒ­ 3 ë³€ìˆ˜
        self.batch_id_var = tk.StringVar()
        self.failed_chunks_file_var = tk.StringVar()
        
        self._configure_styles()
        self._init_ui()
        self._load_key()

    def _configure_styles(self):
        style = ttk.Style()
        try: style.theme_use('clam')
        except: pass
        
        self.configure(background=COLOR_BG)
        
        style.configure("TFrame", background=COLOR_BG)
        style.configure("TLabel", background=COLOR_BG, foreground=COLOR_TEXT, font=("ë§‘ì€ ê³ ë”•", 10))
        style.configure("Header.TLabel", font=("ë§‘ì€ ê³ ë”•", 11, "bold"), foreground="#444")
        
        style.configure("TLabelframe", background=COLOR_BG, bordercolor="#D0D7DE")
        style.configure("TLabelframe.Label", background=COLOR_BG, foreground="#0056b3", font=("ë§‘ì€ ê³ ë”•", 10, "bold"))

        style.configure("TNotebook", background=COLOR_BG, borderwidth=0)
        style.configure("TNotebook.Tab", background="#E1E4E8", padding=[12, 5], font=("ë§‘ì€ ê³ ë”•", 10))
        style.map("TNotebook.Tab", background=[("selected", COLOR_WHITE)], foreground=[("selected", COLOR_PRIMARY)])
        
        style.configure("Treeview", background=COLOR_WHITE, fieldbackground=COLOR_WHITE, font=("ë§‘ì€ ê³ ë”•", 9), rowheight=28)
        style.configure("Treeview.Heading", background=COLOR_HEADER, foreground="#333", font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        style.map("Treeview", background=[('selected', '#CCE5FF')], foreground=[('selected', 'black')])

        style.configure("TButton", font=("ë§‘ì€ ê³ ë”•", 9), padding=5, borderwidth=1)
        style.configure("Primary.TButton", background=COLOR_PRIMARY, foreground="white", bordercolor=COLOR_PRIMARY)
        style.map("Primary.TButton", background=[("active", COLOR_PRIMARY_HOVER)])
        style.configure("Success.TButton", background=COLOR_SUCCESS, foreground="white", bordercolor=COLOR_SUCCESS)
        style.map("Success.TButton", background=[("active", COLOR_SUCCESS_HOVER)])
        style.configure("Danger.TButton", background=COLOR_DANGER, foreground="white", bordercolor=COLOR_DANGER)
        style.map("Danger.TButton", background=[("active", COLOR_DANGER_HOVER)])

    def _init_ui(self):
        main_container = ttk.Frame(self, padding=15)
        main_container.pack(fill='both', expand=True)

        # 1. ìƒë‹¨ API Key
        f_top = ttk.LabelFrame(main_container, text="ğŸ”‘ API ì„¤ì •", padding=10)
        f_top.pack(fill='x', pady=(0, 10))
        ttk.Label(f_top, text="Batch API Key:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(side='left')
        entry_key = ttk.Entry(f_top, textvariable=self.api_key_var, show="*", width=50, font=("Consolas", 10))
        entry_key.pack(side='left', padx=10)
        ttk.Button(f_top, text="ì €ì¥", command=self._save_key, style="Primary.TButton").pack(side='left')

        # 2. ë©”ì¸ íƒ­
        self.main_tabs = ttk.Notebook(main_container)
        self.main_tabs.pack(fill='both', expand=True, pady=5)
        
        self.tab_create = ttk.Frame(self.main_tabs)
        self.tab_manage = ttk.Frame(self.main_tabs) 
        self.tab_merge = ttk.Frame(self.main_tabs)
        
        self.main_tabs.add(self.tab_create, text=" 1. ë°°ì¹˜ ìƒì„± & ì—…ë¡œë“œ ")
        self.main_tabs.add(self.tab_manage, text=" 2. ë°°ì¹˜ ê´€ë¦¬ (ëª©ë¡/ë³‘í•©) ")
        self.main_tabs.add(self.tab_merge, text=" 3. ê°œë³„ ë³‘í•© (ìˆ˜ë™) ")
        
        self._init_tab_create()
        self._init_tab_manage()
        self._init_tab_merge()
        
        # 3. ë¡œê·¸
        f_log = ttk.LabelFrame(main_container, text="ğŸ“‹ ì‹œìŠ¤í…œ ë¡œê·¸", padding=10)
        f_log.pack(fill='both', expand=True, pady=(10, 0))
        self.log_widget = ScrolledText(f_log, height=22, state='disabled', font=("Consolas", 9), bg="#F1F3F5")
        self.log_widget.pack(fill='both', expand=True)

    def _load_key(self):
        loaded = load_api_key_from_file(API_KEY_FILE)
        if loaded: self.api_key_var.set(loaded)

    def _save_key(self):
        k = self.api_key_var.get().strip()
        if k:
            save_api_key_to_file(k, API_KEY_FILE)
            messagebox.showinfo("ì €ì¥", "API Key ì €ì¥ ì™„ë£Œ")

    def append_log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        self.log_widget.config(state='normal')
        self.log_widget.insert('end', f"[{ts}] {msg}\n")
        self.log_widget.see('end')
        self.log_widget.config(state='disabled')

    # ----------------------------------------------------
    # Tab 1: Create (ìƒì„±)
    # ----------------------------------------------------
    def _init_tab_create(self):
        container = ttk.Frame(self.tab_create, padding=20)
        container.pack(fill='both', expand=True)
        
        # Step 1: íŒŒì¼
        f_file = ttk.LabelFrame(container, text="1. ì‘ì—… ëŒ€ìƒ íŒŒì¼ (IMG_S1_ëˆ„ë¼ í¬í•¨, I2 ë˜ëŠ” I3)", padding=15)
        f_file.pack(fill='x', pady=(0, 15))
        ttk.Entry(f_file, textvariable=self.src_file_var, font=("ë§‘ì€ ê³ ë”•", 10)).pack(side='left', fill='x', expand=True)
        ttk.Button(f_file, text="ğŸ“‚ íŒŒì¼ ì°¾ê¸°", command=self._select_src_file).pack(side='right', padx=5)
        
        # Step 2: ëª¨ë¸ ì„¤ì •
        f_opt = ttk.LabelFrame(container, text="2. ëª¨ë¸ ì„¤ì •", padding=15)
        f_opt.pack(fill='x', pady=5)

        # ëª¨ë¸ & Effort
        fr1 = ttk.Frame(f_opt)
        fr1.pack(fill='x', pady=5)
        ttk.Label(fr1, text="ëª¨ë¸ (Model):", width=12).pack(side='left')
        models = list(MODEL_PRICING_USD_PER_MTOK.keys()) if MODEL_PRICING_USD_PER_MTOK else ["gpt-5-mini", "gpt-5", "gpt-5-nano"]
        cb_model = ttk.Combobox(fr1, textvariable=self.model_var, values=models, state="readonly", width=20)
        cb_model.pack(side='left', padx=5)
        
        ttk.Label(fr1, text="ì¶”ë¡  ê°•ë„:", width=10).pack(side='left', padx=(20, 5))
        ttk.Combobox(fr1, textvariable=self.effort_var, values=["none", "low", "medium", "high"], state="readonly", width=12).pack(side='left', padx=5)
        
        # ì²´í¬ë°•ìŠ¤
        f_row_chk = ttk.Frame(f_opt)
        f_row_chk.pack(fill='x', pady=10)
        ttk.Checkbutton(f_row_chk, text=" ì´ë¯¸ view_point ë“±ì´ ìˆëŠ” í–‰ ê±´ë„ˆë›°ê¸°", variable=self.skip_exist_var).pack(side='left')
        
        f_row_chk2 = ttk.Frame(f_opt)
        f_row_chk2.pack(fill='x', pady=5)
        ttk.Checkbutton(f_row_chk2, text=" 'bad' ë¼ë²¨ì´ ìˆëŠ” í–‰ ì œì™¸ (IMG_S1_íœ´ë¨¼ë¼ë²¨ ë˜ëŠ” IMG_S1_AIë¼ë²¨ì´ 'bad'ì¸ ê²½ìš°)", variable=self.skip_bad_label_var).pack(side='left')
        
        # Step 3: ì‹¤í–‰
        f_step3 = ttk.LabelFrame(container, text="3. ì‹¤í–‰", padding=15)
        f_step3.pack(fill='x', pady=15)

        btn = ttk.Button(f_step3, text="ğŸš€ JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (Start Batch)", command=self._start_create_batch, style="Success.TButton")
        btn.pack(fill='x', ipady=8)
        ttk.Label(container, text="â€» ë°°ì¹˜ APIëŠ” ê²°ê³¼ ìˆ˜ì‹ ê¹Œì§€ ìµœëŒ€ 24ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. (ë¹„ìš© 50% ì ˆê°)", foreground="#666").pack()

    def _select_src_file(self):
        p = filedialog.askopenfilename(
            title="ì¸ë„¤ì¼ ë¶„ì„ ì—‘ì…€ ì„ íƒ (I2 ë²„ì „ë§Œ ê°€ëŠ¥)",
            filetypes=[("Excel", "*.xlsx;*.xls")]
        )
        if p:
            base_name = os.path.basename(p)
            # I2 í¬í•¨ ì—¬ë¶€ ê²€ì¦
            if not re.search(r"_I2", base_name, re.IGNORECASE):
                messagebox.showerror(
                    "ì˜¤ë¥˜", 
                    f"ì´ ë„êµ¬ëŠ” I2 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                    f"ì„ íƒí•œ íŒŒì¼: {base_name}\n"
                    f"íŒŒì¼ëª…ì— '_I2' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
                )
                return
            
            self.src_file_var.set(p)
            self.append_log(f"íŒŒì¼ ì„ íƒë¨: {base_name} (I2)")

    def _start_create_batch(self):
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        t = threading.Thread(target=self._run_create_batch)
        t.daemon = True
        t.start()

    def _run_create_batch(self):
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        
        model_name = self.model_var.get().strip() or "gpt-5-mini"
        reasoning_effort = self.effort_var.get().strip() or "low"
        
        try:
            client = OpenAI(api_key=key)
            self.append_log(f"ì—‘ì…€ ë¡œë“œ ì¤‘... {os.path.basename(src)}")
            df = pd.read_excel(src)
            
            if "IMG_S1_ëˆ„ë¼" not in df.columns:
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼(IMG_S1_ëˆ„ë¼)ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            self.append_log(f"ì„¤ì •: ëª¨ë¸={model_name}, effort={reasoning_effort}")

            jsonl_lines = []
            skipped_cnt = 0
            
            result_cols = [
                "view_point", "subject_position", "subject_size", "lighting_condition",
                "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
                "bg_layout_hint_en"
            ]
            
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get():
                    has_result = False
                    val = str(row.get("view_point", "")).strip()
                    if val and val != "nan" and val:
                        has_result = True
                    if has_result:
                        skipped_cnt += 1
                        continue
                
                # 'bad' ë¼ë²¨ ì²´í¬ (ê¸°ë³¸ê°’: True)
                if self.skip_bad_label_var.get():
                    human_label = str(row.get("IMG_S1_íœ´ë¨¼ë¼ë²¨", "")).strip().lower()
                    ai_label = str(row.get("IMG_S1_AIë¼ë²¨", "")).strip().lower()
                    
                    if human_label == "bad" or ai_label == "bad":
                        self.append_log(f"[Row {idx+1}] 'bad' ë¼ë²¨ì´ ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤. (íœ´ë¨¼ë¼ë²¨: {row.get('IMG_S1_íœ´ë¨¼ë¼ë²¨', '')}, AIë¼ë²¨: {row.get('IMG_S1_AIë¼ë²¨', '')})")
                        skipped_cnt += 1
                        continue
                
                # ëˆ„ë¼ ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
                thumbnail_path = str(row.get("IMG_S1_ëˆ„ë¼", "")).strip()
                if not thumbnail_path or thumbnail_path == "nan":
                    skipped_cnt += 1
                    continue
                
                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not os.path.exists(thumbnail_path):
                    self.append_log(f"[Row {idx+1}] ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {thumbnail_path}")
                    skipped_cnt += 1
                    continue
                
                try:
                    # build_analysis_messagesë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ìƒì„±
                    messages = build_analysis_messages(thumbnail_path)
                    
                    body = {
                        "model": model_name,
                        "messages": messages,
                    }
                    
                    # gpt-5 ê³„ì—´ì€ reasoning_effort ì‚¬ìš©
                    is_reasoning = any(x in model_name for x in ["gpt-5", "o1", "o3"])
                    if is_reasoning and reasoning_effort != "none":
                        body["reasoning_effort"] = reasoning_effort

                    request_obj = {
                        "custom_id": f"row_{idx}",
                        "method": "POST",
                        "url": "/v1/chat/completions",
                        "body": body
                    }
                    
                    jsonl_lines.append(json.dumps(request_obj, ensure_ascii=False))
                except Exception as e:
                    self.append_log(f"[Row {idx+1}] ìŠ¤í‚µ: {e}")
                    skipped_cnt += 1
                    continue
            
            if not jsonl_lines:
                self.append_log("ìƒì„±í•  ìš”ì²­ ì—†ìŒ.")
                return

            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_img_analysis_batch_input.jsonl"
            with open(jsonl_path, "w", encoding="utf-8") as f:
                f.write("\n".join(jsonl_lines))
            
            self.append_log(f"JSONL ìƒì„± ì™„ë£Œ: {len(jsonl_lines)}ê±´ (ìŠ¤í‚µ {skipped_cnt}ê±´)")
            
            # íŒŒì¼ í¬ê¸° ë° ìš”ì²­ ìˆ˜ í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            info = {
                'num_requests': len(jsonl_lines),
                'file_size_mb': jsonl_size_mb
            }
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ìš”ì²­ ìˆ˜: {info['num_requests']}ê°œ")
            
            # ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ : 180MB ì´ìƒì´ë©´ ë¶„í•  ì²˜ë¦¬ (OpenAI Batch API ì œí•œ: 200MB)
            # ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨ (500ê°œ ì œí•œ ì œê±°)
            MAX_FILE_SIZE_MB = 180
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸°ê°€ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤: {jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client_with_timeout = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch_ids = self._create_batch_chunks(
                    client=client_with_timeout,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    effort=reasoning_effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=999999,  # ìš”ì²­ ìˆ˜ ì œí•œ ê±°ì˜ ì œê±° (ìš©ëŸ‰ì´ ìš°ì„ )
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}")
            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    reasoning_effort=reasoning_effort,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src,
                    jsonl_path=jsonl_path,
                    model=model_name,
                    effort=reasoning_effort,
                    status=batch.status,
                    output_file_id=None,
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— I3-1 ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡ - img ìƒíƒœë§Œ ì—…ë°ì´íŠ¸ (text ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                try:
                    root_name = get_root_filename(src)
                    JobManager.update_status(root_name, img_s3_1_msg="I3-1 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> I3-1 (ì§„í–‰ì¤‘)")
                except Exception:
                    pass
                messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {batch_id}")
            
            self._load_jobs_all()
            self._load_archive_list()

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            messagebox.showerror("ì—ëŸ¬", str(e))
    
    def _create_batch_from_jsonl(self, client, jsonl_path, excel_path, model_name, reasoning_effort):
        """JSONL íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë°°ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        if not os.path.exists(jsonl_path):
            raise FileNotFoundError(f"ì…ë ¥ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")

        with open(jsonl_path, "rb") as f:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
            up_file = client.files.create(file=f, purpose="batch", timeout=600)

        batch = client.batches.create(
            input_file_id=up_file.id,
            endpoint="/v1/chat/completions",
            completion_window="24h"
        )
        return batch
    
    def _create_batch_chunks(self, client, jsonl_path, excel_path, model_name, effort, max_size_mb=180, max_requests=999999):
        """
        í° JSONL íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ batch_group_idë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        """
        import uuid
        
        # ë°°ì¹˜ ê·¸ë£¹ ID ìƒì„± (ê°™ì€ ì—‘ì…€ì—ì„œ ë¶„í• ëœ ë°°ì¹˜ë“¤ì„ ë¬¶ìŒ)
        batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
        
        # JSONL íŒŒì¼ ì½ê¸°
        requests = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    requests.append(json.loads(line))
        
        total_requests = len(requests)
        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (ìš©ëŸ‰ ê¸°ì¤€ë§Œ ì‚¬ìš©, ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
        original_file_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
        estimated_total_chunks = max(1, int(original_file_size_mb / max_size_mb) + 1)
        
        self.append_log(f"[INFO] ì´ {total_requests}ê°œ ìš”ì²­ì„ ë¶„í• í•©ë‹ˆë‹¤... (ì˜ˆìƒ: ì•½ {estimated_total_chunks}ê°œ ì²­í¬, ê·¸ë£¹ ID: {batch_group_id})")
        
        # base ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜ (ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹œ ì‚¬ìš©)
        base, ext = os.path.splitext(jsonl_path)
        
        batch_ids = []
        chunk_num = 0
        chunk_files_created = []  # ìƒì„±ëœ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì •ë¦¬ìš©)
        failed_chunk_files = []  # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì¬ì‹œë„ìš©)
        
        i = 0
        while i < total_requests:
            chunk_num += 1
            chunk_requests = []
            chunk_size_bytes = 0  # ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì •í™•íˆ ê³„ì‚°
            
            # ì²­í¬ ìƒì„± (ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ , ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
            while i < total_requests:
                req_json = json.dumps(requests[i], ensure_ascii=False)
                req_size_bytes = len(req_json.encode('utf-8')) + 1  # +1 for newline
                
                # ìš©ëŸ‰ì´ ìš°ì„ : ë‹¤ìŒ ìš”ì²­ì„ ì¶”ê°€í•˜ë©´ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ ë§ˆì§„ 5% í¬í•¨)
                if chunk_size_bytes + req_size_bytes > max_size_mb * 1024 * 1024 * 0.95:
                    break
                
                chunk_requests.append(requests[i])
                chunk_size_bytes += req_size_bytes
                i += 1
            
            if not chunk_requests:
                break
            
            # ì²­í¬ JSONL íŒŒì¼ ìƒì„±
            chunk_jsonl_path = f"{base}_chunk{chunk_num:03d}{ext}"
            chunk_files_created.append(chunk_jsonl_path)
            
            with open(chunk_jsonl_path, "w", encoding="utf-8") as f:
                for req in chunk_requests:
                    f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            chunk_size_mb = os.path.getsize(chunk_jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] ì²­í¬ {chunk_num}: {len(chunk_requests)}ê°œ ìš”ì²­, {chunk_size_mb:.2f} MB")
            
            # ë°°ì¹˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 3
            retry_count = 0
            batch_created = False
            
            while retry_count < max_retries and not batch_created:
                try:
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹œë„ ì¤‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                    batch = self._create_batch_from_jsonl(
                        client=client,
                        jsonl_path=chunk_jsonl_path,
                        excel_path=excel_path,
                        model_name=model_name,
                        reasoning_effort=effort,
                    )
                    
                    batch_id = batch.id
                    batch_ids.append(batch_id)
                    self.append_log(f"âœ… ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_id}")
                    
                    batch_created = True
                    
                    # ì‘ì—… ì´ë ¥ ê¸°ë¡ (ê·¸ë£¹ ì •ë³´ í¬í•¨)
                    upsert_batch_job(
                        batch_id=batch_id,
                        src_excel=excel_path,
                        jsonl_path=chunk_jsonl_path,
                        model=model_name,
                        effort=effort,
                        status=batch.status,
                        output_file_id=None,
                        batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                        chunk_index=chunk_num,  # ì²­í¬ ë²ˆí˜¸
                        total_chunks=chunk_num,  # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì²­í¬ ìˆ˜ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨)
                    )
                except Exception as e:
                    error_str = str(e).lower()
                    is_token_limit_error = "enqueued token limit" in error_str or "token limit reached" in error_str
                    
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                        self.append_log(f"âš ï¸ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                        if is_token_limit_error:
                            self.append_log(f"[WARN] í† í° ì œí•œ ì˜¤ë¥˜ ê°ì§€. ë” ê¸´ ëŒ€ê¸° ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            wait_time = max(wait_time, 30)  # í† í° ì œí•œ ì˜¤ë¥˜ëŠ” ìµœì†Œ 30ì´ˆ ëŒ€ê¸°
                        self.append_log(f"[INFO] {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        import time
                        time.sleep(wait_time)
                    else:
                        self.append_log(f"âŒ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}")
                        if is_token_limit_error:
                            self.append_log(f"[INFO] í† í° ì œí•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì¼ë¶€ ë°°ì¹˜ê°€ ì™„ë£Œëœ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                            self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼: {chunk_jsonl_path}")
                            self.append_log(f"[INFO] ë‚˜ì¤‘ì— '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì´ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        import traceback
                        self.append_log(traceback.format_exc())
                        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìœ ì§€ (ìˆ˜ë™ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                        failed_chunk_files.append({
                            "chunk_num": chunk_num,
                            "chunk_file": chunk_jsonl_path,
                            "error": str(e),
                            "is_token_limit": is_token_limit_error,
                            "excel_path": excel_path,
                            "model_name": model_name,
                            "effort": effort,
                            "batch_group_id": batch_group_id,
                        })
                        # í•˜ì§€ë§Œ ë°°ì¹˜ IDëŠ” ì¶”ê°€ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ batch_idsì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ
        
        # ëª¨ë“  ì²­í¬ ìƒì„± ì™„ë£Œ í›„, total_chunksë¥¼ ì‹¤ì œ ìƒì„±ëœ ë°°ì¹˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        actual_total_chunks = len(batch_ids)
        if actual_total_chunks > 0:
            self.append_log(f"[INFO] ì´ {actual_total_chunks}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ. ì‘ì—… ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
            jobs = load_batch_jobs()
            updated_count = 0
            for j in jobs:
                if j.get("batch_group_id") == batch_group_id:
                    j["total_chunks"] = actual_total_chunks
                    updated_count += 1
            if updated_count > 0:
                save_batch_jobs(jobs)
                self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {actual_total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.append_log(f"âš ï¸ ìƒì„±ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì„±ê³µí•œ ë°°ì¹˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²­í¬ íŒŒì¼ ì •ë¦¬ ì˜µì…˜ ì œê³µ (í˜„ì¬ëŠ” ìœ ì§€)
        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìˆ˜ë™ ì¬ì‹œë„ë¥¼ ìœ„í•´ ìœ ì§€
        if actual_total_chunks < chunk_num:
            failed_chunks = chunk_num - actual_total_chunks
            self.append_log(f"âš ï¸ {failed_chunks}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²­í¬ íŒŒì¼ì€ ìœ ì§€ë©ë‹ˆë‹¤.")
            
            # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ì„ ëª…í™•íˆ í‘œì‹œ
            if failed_chunk_files:
                self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡:")
                for failed_info in failed_chunk_files:
                    self.append_log(f"  - ì²­í¬ {failed_info['chunk_num']}: {os.path.basename(failed_info['chunk_file'])}")
                    if failed_info['is_token_limit']:
                        self.append_log(f"    â†’ í† í° ì œí•œ ì˜¤ë¥˜. ì¼ë¶€ ë°°ì¹˜ ì™„ë£Œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                
                # ì‹¤íŒ¨ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                failed_info_path = f"{base}_failed_chunks.json"
                try:
                    with open(failed_info_path, "w", encoding="utf-8") as f:
                        json.dump(failed_chunk_files, f, ensure_ascii=False, indent=2)
                    self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ ì €ì¥: {os.path.basename(failed_info_path)}")
                    self.append_log(f"[INFO] ë‚˜ì¤‘ì— ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤íŒ¨í•œ ì²­í¬ë§Œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    # GUIì— ìë™ìœ¼ë¡œ ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ì•Œë¦¼
                    self.after(0, lambda: self._handle_failed_chunks(failed_info_path, failed_chunk_files))
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
        try:
            root_name = get_root_filename(excel_path)
            JobManager.update_status(root_name, img_s3_1_msg="I3-1 (ì§„í–‰ì¤‘)")
            self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> I3-1 (ì§„í–‰ì¤‘)")
        except Exception:
            pass
        
        return batch_ids

    # ----------------------------------------------------
    # Tab 2: Manage (List & Trash)
    # ----------------------------------------------------
    def _init_tab_manage(self):
        container = ttk.Frame(self.tab_manage, padding=10)
        container.pack(fill='both', expand=True)

        sub_tabs = ttk.Notebook(container)
        sub_tabs.pack(fill='both', expand=True)
        
        self.sub_active = ttk.Frame(sub_tabs, padding=10)
        self.sub_archive = ttk.Frame(sub_tabs, padding=10)
        
        sub_tabs.add(self.sub_active, text=" â–¶ ì§„í–‰ì¤‘ / ì™„ë£Œ (Active) ")
        sub_tabs.add(self.sub_archive, text=" ğŸ—‘ íœ´ì§€í†µ (Archive) ")
        
        # --- Active Tab UI ---
        f_ctrl = ttk.Frame(self.sub_active)
        f_ctrl.pack(fill='x', pady=(0, 10))
        
        ttk.Button(f_ctrl, text="ğŸ”„ ì„ íƒ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ“¥ ì„ íƒ ì¼ê´„ ë³‘í•©", command=self._merge_selected, style="Primary.TButton").pack(side='left', padx=10)
        ttk.Button(f_ctrl, text="ğŸ—‘ íœ´ì§€í†µ ì´ë™", command=self._archive_selected, style="Danger.TButton").pack(side='right', padx=2)
        
        # ì»¬ëŸ¼ ì •ì˜: batch_id | excel_name | memo | status | created | completed | model | effort | counts | group
        cols = ("batch_id", "excel_name", "memo", "status", "created", "completed", "model", "effort", "counts", "group")
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš© (íŠ¸ë¦¬ ì•„ì´ì½˜ + ì»¬ëŸ¼ í—¤ë”)
        self.tree_active = ttk.Treeview(self.sub_active, columns=cols, show='tree headings', height=12, selectmode='extended')
        
        self.tree_active.tag_configure('odd', background=COLOR_WHITE)
        self.tree_active.tag_configure('even', background='#F2F7FF')
        self.tree_active.tag_configure('group', background='#E8F4FD')
        self.tree_active.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_active.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_active.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_active.heading("memo", text="ë©”ëª¨")
        self.tree_active.heading("status", text="ìƒíƒœ")
        self.tree_active.heading("created", text="ìƒì„±ì¼ì‹œ")
        self.tree_active.heading("completed", text="ì™„ë£Œì¼ì‹œ")
        self.tree_active.heading("model", text="ëª¨ë¸")
        self.tree_active.heading("effort", text="Effort")
        self.tree_active.heading("counts", text="ì™„ë£Œ/ì „ì²´")
        self.tree_active.heading("group", text="ê·¸ë£¹")
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree_active.column("batch_id", width=180, anchor="w")
        self.tree_active.column("excel_name", width=200, anchor="w")
        self.tree_active.column("memo", width=150, anchor="w")
        self.tree_active.column("status", width=80, anchor="center")
        self.tree_active.column("created", width=120, anchor="center")
        self.tree_active.column("completed", width=120, anchor="center")
        self.tree_active.column("model", width=80, anchor="center")
        self.tree_active.column("effort", width=60, anchor="center")
        self.tree_active.column("counts", width=80, anchor="center")
        self.tree_active.column("group", width=80, anchor="center")
        
        self.tree_active.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ë²„íŠ¼
        f_group_ctrl = ttk.Frame(self.sub_active)
        f_group_ctrl.pack(fill='x', padx=5, pady=(0, 5))
        ttk.Button(f_group_ctrl, text="ğŸ“‚ ëª¨ë“  ê·¸ë£¹ í¼ì¹˜ê¸°", command=lambda: self._expand_all_groups(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_group_ctrl, text="ğŸ“ ëª¨ë“  ê·¸ë£¹ ì ‘ê¸°", command=lambda: self._collapse_all_groups(self.tree_active)).pack(side='left', padx=2)
        
        # ìš°í´ë¦­ ë©”ë‰´
        self.menu_active = Menu(self, tearoff=0)
        self.menu_active.add_command(label="ìƒíƒœ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ê²°ê³¼ ë³‘í•©", command=self._merge_selected)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="íœ´ì§€í†µìœ¼ë¡œ ì´ë™", command=self._archive_selected)
        
        self.tree_active.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_active, self.menu_active))
        self.tree_active.bind("<Double-1>", self._on_tree_double_click)

        # --- Archive Tab UI ---
        f_arch_ctrl = ttk.Frame(self.sub_archive)
        f_arch_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_arch_ctrl, text="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected, style="Primary.TButton").pack(side='left')
        ttk.Button(f_arch_ctrl, text="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected, style="Danger.TButton").pack(side='right')
        
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš©
        self.tree_arch = ttk.Treeview(self.sub_archive, columns=cols, show='tree headings', height=12, selectmode='extended')
        self.tree_arch.tag_configure('odd', background=COLOR_WHITE)
        self.tree_arch.tag_configure('even', background='#FFF2F2')
        self.tree_arch.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_arch.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_arch.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_arch.heading("memo", text="ë©”ëª¨")
        self.tree_arch.heading("status", text="ìƒíƒœ")
        self.tree_arch.heading("created", text="ìƒì„±ì¼ì‹œ")
        self.tree_arch.heading("completed", text="ì™„ë£Œì¼ì‹œ")
        self.tree_arch.heading("model", text="ëª¨ë¸")
        self.tree_arch.heading("effort", text="Effort")
        self.tree_arch.heading("counts", text="ì™„ë£Œ/ì „ì²´")
        self.tree_arch.heading("group", text="ê·¸ë£¹")
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree_arch.column("batch_id", width=180, anchor="w")
        self.tree_arch.column("excel_name", width=200, anchor="w")
        self.tree_arch.column("memo", width=150, anchor="w")
        self.tree_arch.column("status", width=80, anchor="center")
        self.tree_arch.column("created", width=120, anchor="center")
        self.tree_arch.column("completed", width=120, anchor="center")
        self.tree_arch.column("model", width=80, anchor="center")
        self.tree_arch.column("effort", width=60, anchor="center")
        self.tree_arch.column("counts", width=80, anchor="center")
        self.tree_arch.column("group", width=80, anchor="center")
        
        self.tree_arch.pack(fill='both', expand=True)
        
        # Archive ìš°í´ë¦­ ë©”ë‰´
        self.menu_arch = Menu(self, tearoff=0)
        self.menu_arch.add_command(label="ë³µêµ¬", command=self._restore_selected)
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_arch))
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected)
        self.tree_arch.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_arch, self.menu_arch))
        
        self._load_jobs_all()
        self._load_archive_list()

    def _show_context_menu(self, event, tree, menu):
        item = tree.identify_row(event.y)
        if item:
            if item not in tree.selection():
                tree.selection_set(item)
            menu.post(event.x_root, event.y_root)

    def _expand_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ í¼ì¹©ë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=True)
    
    def _collapse_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ ì ‘ìŠµë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=False)
    
    def _get_selected_ids(self, tree):
        """
        ì„ íƒëœ í•­ëª©ì—ì„œ ë°°ì¹˜ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê·¸ë£¹ í—¤ë”ë¥¼ ì„ íƒí•˜ë©´ ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        selection = tree.selection()
        ids = []
        
        for item in selection:
            vals = tree.item(item)['values']
            batch_id = vals[0] if vals else ""
            
            # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
            if not batch_id:
                # ê·¸ë£¹ í—¤ë”ì˜ ìì‹ ë…¸ë“œë“¤(ë°°ì¹˜ë“¤)ì„ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
                children = tree.get_children(item)
                for child in children:
                    child_vals = tree.item(child)['values']
                    if child_vals and child_vals[0]:
                        ids.append(child_vals[0])
            else:
                # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°
                ids.append(batch_id)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(ids))
    
    def _edit_memo(self, tree):
        """ì„ íƒëœ ë°°ì¹˜ì˜ ë©”ëª¨ë¥¼ í¸ì§‘í•©ë‹ˆë‹¤."""
        selection = tree.selection()
        if not selection:
            messagebox.showinfo("ì•ˆë‚´", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        item = selection[0]
        vals = tree.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        if not batch_id:
            messagebox.showinfo("ì•ˆë‚´", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # í˜„ì¬ ë©”ëª¨ ê°€ì ¸ì˜¤ê¸°
        jobs = load_batch_jobs()
        current_memo = ""
        for j in jobs:
            if j["batch_id"] == batch_id:
                current_memo = j.get("memo", "") or ""
                break
        
        # ë©”ëª¨ í¸ì§‘ ë‹¤ì´ì–¼ë¡œê·¸
        dialog = tk.Toplevel(self)
        dialog.title("ë©”ëª¨ í¸ì§‘")
        dialog.geometry("500x200")
        dialog.transient(self)
        dialog.grab_set()
        
        # ë°°ì¹˜ ID í‘œì‹œ
        tk.Label(dialog, text=f"ë°°ì¹˜ ID: {batch_id[:30]}...", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(10, 5))
        
        # ë©”ëª¨ ì…ë ¥ í•„ë“œ
        tk.Label(dialog, text="ë©”ëª¨:", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(5, 0))
        memo_entry = tk.Text(dialog, height=5, width=60, font=("ë§‘ì€ ê³ ë”•", 9))
        memo_entry.pack(fill="both", expand=True, padx=10, pady=5)
        memo_entry.insert("1.0", current_memo)
        memo_entry.focus()
        
        # ë²„íŠ¼
        btn_frame = tk.Frame(dialog)
        btn_frame.pack(fill="x", padx=10, pady=10)
        
        def save_memo():
            new_memo = memo_entry.get("1.0", "end-1c").strip()
            upsert_batch_job(batch_id, memo=new_memo)
            self.append_log(f"[INFO] ë°°ì¹˜ {batch_id[:20]}... ë©”ëª¨ ì—…ë°ì´íŠ¸: {new_memo[:50]}...")
            self._load_jobs_all()
            self._load_archive_list()
            dialog.destroy()
            messagebox.showinfo("ì™„ë£Œ", "ë©”ëª¨ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        tk.Button(btn_frame, text="ì €ì¥", command=save_memo, bg="#4CAF50", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)
        tk.Button(btn_frame, text="ì·¨ì†Œ", command=dialog.destroy, bg="#f44336", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)

    def _load_jobs_all(self):
        if not hasattr(self, 'tree_active'): return
        for i in self.tree_active.get_children(): self.tree_active.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ì„ ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0]
            created_at = first_job.get("created_at", "")
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = dt.strftime("%m-%d %H:%M")
                except:
                    date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
            else:
                date_str = "-"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
            group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
            excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
            memo = first_job.get("memo", "") or "-"
            group_node = self.tree_active.insert("", "end", 
                text=group_header_text,
                values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                memo = j.get("memo", "") or "-"
                tag = 'group'
                self.tree_active.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, 
                        j.get("model"), j.get("effort", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
                idx += 1
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_active.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), 
                    c_at, f_at, 
                    j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))
            idx += 1

    def _load_archive_list(self):
        if not hasattr(self, 'tree_arch'): return
        for i in self.tree_arch.get_children(): self.tree_arch.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if not j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs)) if group_jobs else len(group_jobs)
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0] if group_jobs else None
            if first_job:
                created_at = first_job.get("created_at", "")
                if created_at:
                    try:
                        dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                        date_str = dt.strftime("%m-%d %H:%M")
                    except:
                        date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
                else:
                    date_str = "-"
                
                # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
                group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
                excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
                memo = first_job.get("memo", "") or "-"
                group_node = self.tree_arch.insert("", "end", 
                    text=group_header_text,
                    values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                    tags=('group_header',),
                    open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
                )
                
                # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
                for j in group_jobs:
                    cnt = "-"
                    if "request_counts" in j and j["request_counts"]:
                        rc = j["request_counts"]
                        cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                    c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                    f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                    chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                    group_display = f"ì²­í¬ {chunk_info}"
                    excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                    memo = j.get("memo", "") or "-"
                    tag = 'group'
                    self.tree_arch.insert(group_node, "end", 
                        text=f"  â””â”€ {j['batch_id'][:20]}...",
                        values=(
                            j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, 
                            j.get("model"), j.get("effort", "-"), cnt, group_display
                        ), 
                        tags=(tag,))
                    idx += 1
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_arch.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), 
                    c_at, f_at, 
                    j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))
            idx += 1

    # --- Batch Actions ---
    def _refresh_selected(self, tree):
        ids = self._get_selected_ids(tree)
        if not ids: return
        
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        jobs = load_batch_jobs()
        completed = [bid for bid in ids if next((x for x in jobs if x["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        
        if completed:
            if messagebox.askyesno("í™•ì¸", f"{len(completed)}ê±´ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\nì œì™¸í•˜ê³  ë¯¸ì™„ë£Œ ê±´ë§Œ ê°±ì‹ í• ê¹Œìš”?"):
                ids = [i for i in ids if i not in completed]
        
        if not ids:
            messagebox.showinfo("ì·¨ì†Œ", "ê°±ì‹ í•  ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        t = threading.Thread(target=self._run_refresh_ids, args=(ids,))
        t.daemon = True
        t.start()

    def _run_refresh_ids(self, ids):
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return
        
        self.append_log(f"ì„ íƒëœ {len(ids)}ê±´ ê°±ì‹  ì¤‘...")
        success_cnt = 0
        fail_cnt = 0
        
        for bid in ids:
            try:
                remote = client.batches.retrieve(bid)
                rc = None
                if remote.request_counts:
                    rc = {"total": remote.request_counts.total, "completed": remote.request_counts.completed, "failed": remote.request_counts.failed}
                
                # expired ìƒíƒœë„ ê°±ì‹  ê°€ëŠ¥ (output_file_id í™•ì¸ì„ ìœ„í•´)
                output_file_id = getattr(remote, "output_file_id", None)
                upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id, request_counts=rc)
                
                if remote.status == "expired" and output_file_id:
                    self.append_log(f"â„¹ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ì§€ë§Œ output_file_idê°€ ìˆìŠµë‹ˆë‹¤. (ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")
                else:
                    self.append_log(f"âœ… {bid}: {remote.status}")
                success_cnt += 1
            except Exception as e:
                self.append_log(f"âŒ {bid} ê°±ì‹  ì‹¤íŒ¨: {e}")
                fail_cnt += 1
        
        self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])
        if fail_cnt > 0:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}, ì‹¤íŒ¨: {fail_cnt})")
        else:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}ê±´)")

    def _merge_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        jobs = load_batch_jobs()
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        group_ids = set()
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if job:
                group_id = job.get("batch_group_id")
                if group_id:
                    group_ids.add(group_id)
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        all_target_ids = set(ids)
        for group_id in group_ids:
            if group_id:
                # completed ë˜ëŠ” expired ìƒíƒœì¸ ë°°ì¹˜ í¬í•¨ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
                group_batches = [j for j in jobs if j.get("batch_group_id") == group_id and j.get("status") in ["completed", "expired"]]
                for j in group_batches:
                    all_target_ids.add(j["batch_id"])
        
        if len(all_target_ids) > len(ids):
            group_info = f"\n\nê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ {len(all_target_ids) - len(ids)}ê°œê°€ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤."
        else:
            group_info = ""
        
        # completed, expired ë˜ëŠ” merged ìƒíƒœì¸ ë°°ì¹˜ ëª¨ë‘ ì„ íƒ ê°€ëŠ¥ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
        targets = [bid for bid in all_target_ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") in ["completed", "expired", "merged"]]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ë³‘í•©í•  ìˆ˜ ìˆëŠ” 'completed', 'expired' ë˜ëŠ” 'merged' ìƒíƒœì˜ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        msg = f"ì„ íƒí•œ {len(targets)}ê±´ì„ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ?{group_info}"
        if messagebox.askyesno("ë³‘í•©", msg):
            t = threading.Thread(target=self._run_merge_multi, args=(targets,))
            t.daemon = True
            t.start()

    def _run_merge_multi(self, ids):
        """
        ì„ íƒëœ Batch ë“¤ì— ëŒ€í•´ ê²°ê³¼ JSONL ë‹¤ìš´ë¡œë“œ + ì—‘ì…€ ë³‘í•©ì„ ìˆ˜í–‰.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ í•˜ë‚˜ì˜ ì—‘ì…€ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.
        """
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return
        
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ë°°ì¹˜ ë¶„ë¥˜
        groups_to_merge = {}  # {group_id: [batch_ids]}
        ungrouped_batches = []  # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤
        
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not job:
                continue
            
            group_id = job.get("batch_group_id")
            if group_id:
                if group_id not in groups_to_merge:
                    groups_to_merge[group_id] = []
                groups_to_merge[group_id].append(bid)
            else:
                ungrouped_batches.append(bid)
        
        success_cnt = 0
        total_cost = 0.0
        success_folders = set()  # ì„±ê³µí•œ íŒŒì¼ë“¤ì´ ì €ì¥ëœ í´ë”ë“¤ ì¶”ì 
        
        # ê·¸ë£¹ë³„ ë³‘í•© ì²˜ë¦¬
        for group_id, batch_ids in groups_to_merge.items():
            self.append_log(f"--- ê·¸ë£¹ ë³‘í•© ì‹œì‘: {group_id} ({len(batch_ids)}ê°œ ë°°ì¹˜) ---")
            try:
                # ê·¸ë£¹ ë‚´ ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì›ë³¸ ì—‘ì…€ ê²½ë¡œ ì‚¬ìš©
                first_job = next((j for j in jobs if j["batch_id"] == batch_ids[0]), None)
                if not first_job:
                    continue
                
                src_path = first_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
                all_results_map = {}  # {custom_id: result_data}
                total_group_in = 0
                total_group_out = 0
                total_group_cost = 0.0
                model_name = first_job.get("model", "gpt-5-mini")
                
                # ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ (chunk_indexê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ)
                def get_chunk_index(bid):
                    job = next((j for j in jobs if j["batch_id"] == bid), None)
                    if job:
                        idx = job.get("chunk_index")
                        return idx if idx is not None else 999999
                    return 999999
                
                batch_ids_sorted = sorted(batch_ids, key=get_chunk_index)
                
                for bid in batch_ids_sorted:
                    self.append_log(f"  [ê·¸ë£¹] ë°°ì¹˜ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    try:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if not local_job:
                            continue
                        
                        # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                        if local_job.get("status") == "merged":
                            self.append_log(f"  â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        # Batch ìƒíƒœ ë° ê²°ê³¼ íŒŒì¼ ID ì¡°íšŒ
                        remote = client.batches.retrieve(bid)
                        output_file_id = getattr(remote, "output_file_id", None)
                        
                        # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                        if remote.status == "completed":
                            if not output_file_id:
                                self.append_log(f"  âš ï¸ {bid}: ì™„ë£Œ ìƒíƒœì§€ë§Œ output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                                upsert_batch_job(bid, status=remote.status, output_file_id=None)
                                continue
                        elif remote.status == "expired":
                            if not output_file_id:
                                self.append_log(f"  âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                                upsert_batch_job(bid, status=remote.status, output_file_id=None)
                                continue
                            else:
                                self.append_log(f"  â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                        else:
                            self.append_log(f"  âš ï¸ {bid}: ì•„ì§ completed ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                            upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id)
                            continue
                        
                        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                        base, _ = os.path.splitext(src_path)
                        out_jsonl = f"{base}_img_analysis_batch_output_{bid}.jsonl"
                        
                        try:
                            content = client.files.content(output_file_id).content
                        except AttributeError:
                            file_content = client.files.content(output_file_id)
                            if hasattr(file_content, "read"):
                                content = file_content.read()
                            elif hasattr(file_content, "iter_bytes"):
                                chunks = []
                                for ch in file_content.iter_bytes():
                                    chunks.append(ch)
                                content = b"".join(chunks)
                            else:
                                content = file_content
                        
                        with open(out_jsonl, "wb") as f:
                            f.write(content)
                        
                        upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id, output_jsonl=out_jsonl)
                        
                        # JSONL íŒŒì¼ ì½ì–´ì„œ ê²°ê³¼ ìˆ˜ì§‘
                        batch_in_tok = 0
                        batch_out_tok = 0
                        with open(out_jsonl, "r", encoding="utf-8") as f:
                            for line in f:
                                if not line.strip(): continue
                                data = json.loads(line)
                                cid = data.get("custom_id")
                                
                                usage = data.get("response", {}).get("body", {}).get("usage", {})
                                batch_in_tok += usage.get("prompt_tokens", 0)
                                batch_out_tok += usage.get("completion_tokens", 0)
                                
                                try:
                                    content_str = data["response"]["body"]["choices"][0]["message"]["content"].strip()
                                    result_data = json.loads(content_str)
                                    all_results_map[cid] = result_data
                                except:
                                    if cid:
                                        all_results_map[cid] = {}
                        
                        total_group_in += batch_in_tok
                        total_group_out += batch_out_tok
                        
                        # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                        pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                        cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                        cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                        total_group_cost += cost_in + cost_out
                        
                    except Exception as e:
                        self.append_log(f"  âŒ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if not all_results_map:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ë³‘í•©í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ì˜ ì „ì²´ ì²­í¬ ìˆ˜ í™•ì¸ ë° ê²€ì¦
                expected_total_chunks = first_job.get("total_chunks")
                if expected_total_chunks:
                    downloaded_batch_ids = []
                    for bid in batch_ids_sorted:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if local_job and local_job.get("status") in ["completed", "expired"]:
                            base, _ = os.path.splitext(src_path)
                            out_jsonl = local_job.get("output_jsonl") or f"{base}_img_analysis_batch_output_{bid}.jsonl"
                            if os.path.exists(out_jsonl):
                                downloaded_batch_ids.append(bid)
                    
                    if len(downloaded_batch_ids) < expected_total_chunks:
                        missing = expected_total_chunks - len(downloaded_batch_ids)
                        self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì˜ˆìƒ {expected_total_chunks}ê°œ ì¤‘ {len(downloaded_batch_ids)}ê°œë§Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({missing}ê°œ ëˆ„ë½ ê°€ëŠ¥)")
                
                # í†µí•© ê²°ê³¼ë¥¼ ì—‘ì…€ì— ë³‘í•©
                df = pd.read_excel(src_path)
                result_cols = [
                    "view_point", "subject_position", "subject_size", "lighting_condition",
                    "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
                    "bg_layout_hint_en"
                ]
                for col in result_cols:
                    if col not in df.columns:
                        df[col] = ""
                    df[col] = df[col].astype(str)
                
                cnt = 0
                for cid, result_data in all_results_map.items():
                    try:
                        idx = int(cid.split("_")[1])
                        if 0 <= idx < len(df):
                            for col in result_cols:
                                if col in result_data:
                                    val = result_data[col]
                                    if col == "is_flat_lay":
                                        df.at[idx, col] = str(val).lower() if isinstance(val, bool) else str(val)
                                    else:
                                        df.at[idx, col] = str(val)
                            cnt += 1
                    except:
                        pass
                
                # ì¤‘ê°„ íŒŒì¼ ì €ì¥
                base, ext = os.path.splitext(src_path)
                out_excel = f"{base}_img_analysis_batch_done{ext}"
                if not safe_save_excel(df, out_excel):
                    self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")
                    continue
                
                # I3 ë²„ì „ íŒŒì¼ë¡œ ì €ì¥
                try:
                    final_out_path = get_i3_output_path(src_path)
                    df_done = pd.read_excel(out_excel)
                    if safe_save_excel(df_done, final_out_path):
                        # ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
                        if out_excel != final_out_path and os.path.exists(out_excel):
                            try:
                                os.remove(out_excel)
                                self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                            except Exception as e:
                                self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                    else:
                        final_out_path = out_excel
                except Exception as e:
                    self.append_log(f"[WARN] I3 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                    final_out_path = out_excel
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ë¥¼ merged ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                for bid in batch_ids_sorted:
                    upsert_batch_job(bid, out_excel=final_out_path, status="merged")
                
                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
                try:
                    root_name = get_root_filename(src_path)
                    JobManager.update_status(root_name, img_s3_1_msg="I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)")
                except Exception as e:
                    self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")
                
                self.append_log(f"  [ê·¸ë£¹] ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(final_out_path)}")
                success_cnt += 1
                total_cost += total_group_cost
                success_folders.add(os.path.dirname(final_out_path))
                
            except Exception as e:
                self.append_log(f"âŒ ê·¸ë£¹ {group_id} ë³‘í•© ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
                continue
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        for bid in ungrouped_batches:
            self.append_log(f"--- ë³‘í•© ì‹œì‘: {bid} ---")
            try:
                jobs = load_batch_jobs()
                local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                
                if not local_job:
                    self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì‘ì—… ì´ë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                is_already_merged = local_job.get("status") == "merged"
                if is_already_merged:
                    self.append_log(f"â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤.")
                    continue

                # Batch ìƒíƒœ ë° ê²°ê³¼ íŒŒì¼ ID ì¡°íšŒ
                remote = client.batches.retrieve(bid)
                output_file_id = getattr(remote, "output_file_id", None)
                
                # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                if remote.status == "completed":
                    if not output_file_id:
                        self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì™„ë£Œ ìƒíƒœì§€ë§Œ output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        upsert_batch_job(bid, status=remote.status, output_file_id=None)
                        continue
                elif remote.status == "expired":
                    if not output_file_id:
                        self.append_log(f"âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                        upsert_batch_job(bid, status=remote.status, output_file_id=None)
                        continue
                    else:
                        self.append_log(f"â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                else:
                    self.append_log(f"âš ï¸ {bid}: ì•„ì§ completed ë˜ëŠ” expired ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                    upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id)
                    continue
                
                # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                try:
                    content = client.files.content(output_file_id).content
                except AttributeError:
                    file_content = client.files.content(output_file_id)
                    if hasattr(file_content, "read"):
                        content = file_content.read()
                    elif hasattr(file_content, "iter_bytes"):
                        chunks = []
                        for ch in file_content.iter_bytes():
                            chunks.append(ch)
                        content = b"".join(chunks)
                    else:
                        content = file_content
                
                if local_job and local_job.get("src_excel"):
                    src_path = local_job["src_excel"]
                    base, _ = os.path.splitext(src_path)
                    out_jsonl = f"{base}_img_analysis_batch_output.jsonl"
                else:
                    out_jsonl = f"output_{bid}.jsonl"
                    src_path = None

                with open(out_jsonl, "wb") as f:
                    f.write(content)
                
                # JSONL íŒŒì‹± ë° ì—‘ì…€ ë³‘í•©
                results_map = {}
                batch_in_tok = 0
                batch_out_tok = 0
                
                with open(out_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        
                        usage = data.get("response", {}).get("body", {}).get("usage", {})
                        batch_in_tok += usage.get("prompt_tokens", 0)
                        batch_out_tok += usage.get("completion_tokens", 0)
                        
                        try:
                            content_str = data["response"]["body"]["choices"][0]["message"]["content"].strip()
                            # JSON íŒŒì‹±
                            result_data = json.loads(content_str)
                            results_map[cid] = result_data
                        except: 
                            results_map[cid] = {}

                # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                model_name = local_job.get("model", "gpt-5-mini") if local_job else "gpt-5-mini"
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                cost_total = cost_in + cost_out
                total_cost += cost_total

                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    result_cols = [
                        "view_point", "subject_position", "subject_size", "lighting_condition",
                        "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
                        "bg_layout_hint_en"
                    ]
                    for col in result_cols:
                        if col not in df.columns:
                            df[col] = ""
                        df[col] = df[col].astype(str)
                    
                    cnt = 0
                    for cid, result_data in results_map.items():
                        try:
                            idx = int(cid.split("_")[1])
                            if 0 <= idx < len(df):
                                for col in result_cols:
                                    if col in result_data:
                                        val = result_data[col]
                                        # is_flat_layëŠ” booleanì´ë¯€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
                                        if col == "is_flat_lay":
                                            df.at[idx, col] = str(val).lower() if isinstance(val, bool) else str(val)
                                        else:
                                            df.at[idx, col] = str(val)
                                cnt += 1
                        except:
                            pass

                    # ì¤‘ê°„ íŒŒì¼ ì €ì¥
                    base, ext = os.path.splitext(src_path)
                    out_excel = f"{base}_img_analysis_batch_done{ext}"
                    if not safe_save_excel(df, out_excel):
                        self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")
                        continue

                    # I3 ë²„ì „ íŒŒì¼ë¡œ ì €ì¥
                    try:
                        final_out_path = get_i3_output_path(src_path)
                        df_done = pd.read_excel(out_excel)
                        if safe_save_excel(df_done, final_out_path):
                            # ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
                            if out_excel != final_out_path and os.path.exists(out_excel):
                                try:
                                    os.remove(out_excel)
                                    self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                                except Exception as e:
                                    self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                        else:
                            final_out_path = out_excel
                    except Exception as e:
                        self.append_log(f"[WARN] I3 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                        final_out_path = out_excel

                    upsert_batch_job(bid, out_excel=final_out_path, status="merged")

                    # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— I3-1 ì™„ë£Œ ìƒíƒœ ê¸°ë¡ - img ìƒíƒœë§Œ I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)ë¡œ ì—…ë°ì´íŠ¸ (text ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                    try:
                        root_name = get_root_filename(src_path)
                        JobManager.update_status(root_name, img_s3_1_msg="I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)")
                    except Exception as e:
                        self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")

                    self.append_log(f"âœ… ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(final_out_path)}")
                    success_cnt += 1
                    # ì„±ê³µí•œ íŒŒì¼ì˜ í´ë” ê²½ë¡œ ì €ì¥
                    success_folders.add(os.path.dirname(final_out_path))
                else:
                    self.append_log(f"âš ï¸ ì›ë³¸ ì—†ìŒ. JSONLë§Œ ì €ì¥.")
            except Exception as e:
                self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: {e}")
        
        self.append_log(f"=== ì¼ê´„ ë³‘í•© ë (ì„±ê³µ: {success_cnt}, ë¹„ìš©: ${total_cost:.4f}) ===")
        self._load_jobs_all()
        
        # ì„±ê³µí•œ íŒŒì¼ì´ ìˆìœ¼ë©´ í´ë” ì—´ê¸°
        if success_folders:
            try:
                # ì²« ë²ˆì§¸ ì„±ê³µí•œ í´ë” ì—´ê¸° (ì—¬ëŸ¬ ê°œë©´ ì²« ë²ˆì§¸ë§Œ)
                folder_path = list(success_folders)[0]
                if platform.system() == "Windows":
                    os.startfile(folder_path)
                elif platform.system() == "Darwin":  # macOS
                    subprocess.run(["open", folder_path])
                else:  # Linux
                    subprocess.run(["xdg-open", folder_path])
                self.append_log(f"ğŸ“‚ ê²°ê³¼ í´ë” ì—´ê¸°: {folder_path}")
            except Exception as e:
                self.append_log(f"[WARN] í´ë” ì—´ê¸° ì‹¤íŒ¨: {e}")
        
        messagebox.showinfo("ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©: ${total_cost:.4f}\n\nê²°ê³¼ íŒŒì¼ì´ ì €ì¥ëœ í´ë”ë¥¼ ì—´ì—ˆìŠµë‹ˆë‹¤.")

    def _archive_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if ids and messagebox.askyesno("ë³´ê´€", f"{len(ids)}ê±´ íœ´ì§€í†µ ì´ë™?"):
            archive_batch_job(ids, True)
            self._load_jobs_all()
            self._load_archive_list()

    def _restore_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ë³µêµ¬", f"{len(ids)}ê±´ ë³µêµ¬?"):
            archive_batch_job(ids, False)
            self._load_jobs_all()
            self._load_archive_list()

    def _hard_delete_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ì‚­ì œ", "ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            hard_delete_batch_job(ids)
            self._load_archive_list()

    def _on_tree_double_click(self, event):
        """ë”ë¸”í´ë¦­ ì‹œ: ê·¸ë£¹ í—¤ë”ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°, ë°°ì¹˜ë©´ ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™"""
        sel = self.tree_active.selection()
        if not sel: return
        
        item = sel[0]
        vals = self.tree_active.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš°: ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
        if not batch_id:
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            children = self.tree_active.get_children(item)
            if children:
                # ìì‹ì´ ìˆìœ¼ë©´ ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
                if self.tree_active.item(item, 'open'):
                    self.tree_active.item(item, open=False)
                else:
                    self.tree_active.item(item, open=True)
        else:
            # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°: ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™
            self.batch_id_var.set(batch_id)
            self.main_tabs.select(self.tab_merge)

    # ----------------------------------------------------
    # Tab 3: Merge (Manual)
    # ----------------------------------------------------
    def _init_tab_merge(self):
        container = ttk.Frame(self.tab_merge, padding=20)
        container.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì„¹ì…˜
        f_retry = ttk.LabelFrame(container, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", padding=15)
        f_retry.pack(fill='x', pady=(0, 15))
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        f_list = ttk.Frame(f_retry)
        f_list.pack(fill='both', expand=True, pady=(0, 10))
        ttk.Label(f_list, text="ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(anchor='w')
        
        # Treeviewë¡œ ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        list_frame = ttk.Frame(f_list)
        list_frame.pack(fill='both', expand=True, pady=5)
        
        scrollbar = ttk.Scrollbar(list_frame)
        scrollbar.pack(side='right', fill='y')
        
        self.failed_chunks_tree = ttk.Treeview(list_frame, columns=("chunk_num", "file_name", "error_type"), 
                                                show='headings', height=4, yscrollcommand=scrollbar.set)
        scrollbar.config(command=self.failed_chunks_tree.yview)
        
        self.failed_chunks_tree.heading("chunk_num", text="ì²­í¬ ë²ˆí˜¸")
        self.failed_chunks_tree.heading("file_name", text="íŒŒì¼ëª…")
        self.failed_chunks_tree.heading("error_type", text="ì˜¤ë¥˜ ìœ í˜•")
        
        self.failed_chunks_tree.column("chunk_num", width=80, anchor="center")
        self.failed_chunks_tree.column("file_name", width=300, anchor="w")
        self.failed_chunks_tree.column("error_type", width=150, anchor="center")
        
        self.failed_chunks_tree.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ ì…ë ¥
        f_file = ttk.Frame(f_retry)
        f_file.pack(fill='x', pady=(10, 0))
        ttk.Label(f_file, text="ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼:", font=("ë§‘ì€ ê³ ë”•", 9)).pack(side='left')
        self.failed_chunks_file_var = tk.StringVar()
        ttk.Entry(f_file, textvariable=self.failed_chunks_file_var, width=50, font=("Consolas", 9)).pack(side='left', padx=5, fill='x', expand=True)
        btn_select = ttk.Button(f_file, text="ğŸ“‚ ì°¾ê¸°", command=self._select_failed_chunks_file)
        btn_select.pack(side='left', padx=5)
        btn_retry = ttk.Button(f_file, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", command=self._retry_failed_chunks, style="Success.TButton")
        btn_retry.pack(side='left', padx=5)
        
        f_in = ttk.LabelFrame(container, text="ê°œë³„ ì‘ì—…", padding=15)
        f_in.pack(fill='x', pady=(0, 15))
        ttk.Label(f_in, text="Batch ID:").pack(side='left')
        ttk.Entry(f_in, textvariable=self.batch_id_var, width=45, font=("Consolas", 10)).pack(side='left', padx=10)
        
        f_btn = ttk.Frame(container)
        f_btn.pack(fill='x', pady=20)
        ttk.Button(f_btn, text="1. ê²°ê³¼ ë³‘í•©", command=self._start_merge, style="Primary.TButton").pack(fill='x', pady=5)

    def _start_merge(self):
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        bid = self.batch_id_var.get().strip()
        if not bid:
            messagebox.showwarning("ì˜¤ë¥˜", "Batch IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        t = threading.Thread(target=self._run_merge)
        t.daemon = True
        t.start()

    def _run_merge(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_merge_multi([bid])
    
    def _handle_failed_chunks(self, failed_info_path, failed_chunk_files):
        """ì‹¤íŒ¨í•œ ì²­í¬ê°€ ìˆì„ ë•Œ GUIì— ìë™ìœ¼ë¡œ í‘œì‹œí•˜ê³  ì•Œë¦¼"""
        # ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ìë™ ì„¤ì •
        self.failed_chunks_file_var.set(failed_info_path)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ì„ Treeviewì— í‘œì‹œ
        for item in self.failed_chunks_tree.get_children():
            self.failed_chunks_tree.delete(item)
        
        for failed_info in failed_chunk_files:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
            file_name = os.path.basename(chunk_file)
            
            self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        
        # ì•Œë¦¼ ë©”ì‹œì§€ ë° íƒ­ ì „í™˜
        failed_count = len(failed_chunk_files)
        token_limit_count = sum(1 for f in failed_chunk_files if f.get("is_token_limit", False))
        
        msg = f"âš ï¸ {failed_count}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\n"
        if token_limit_count > 0:
            msg += f"â€¢ í† í° ì œí•œ ì˜¤ë¥˜: {token_limit_count}ê°œ\n"
        msg += f"â€¢ ì‹¤íŒ¨ ì •ë³´ê°€ ìë™ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        msg += f"â€¢ '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        messagebox.showwarning("ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨", msg)
        
        # ì¬ì‹œë„ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
        self.main_tabs.select(self.tab_merge)
    
    def _select_failed_chunks_file(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ"""
        path = filedialog.askopenfilename(
            title="ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ",
            filetypes=[("JSON íŒŒì¼", "*.json"), ("ëª¨ë“  íŒŒì¼", "*.*")]
        )
        if path:
            self.failed_chunks_file_var.set(path)
            # íŒŒì¼ì„ ì„ íƒí•˜ë©´ ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(path)
    
    def _load_failed_chunks_from_file(self, file_path):
        """ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ì„ ì½ì–´ì„œ ëª©ë¡ì— í‘œì‹œ"""
        if not os.path.exists(file_path):
            return
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ê¸°ì¡´ ëª©ë¡ ì‚­ì œ
            for item in self.failed_chunks_tree.get_children():
                self.failed_chunks_tree.delete(item)
            
            # ëª©ë¡ì— ì¶”ê°€
            for failed_info in failed_chunks:
                chunk_num = failed_info.get("chunk_num", 0)
                chunk_file = failed_info.get("chunk_file", "")
                error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
                file_name = os.path.basename(chunk_file)
                
                self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        except Exception as e:
            self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    def _retry_failed_chunks(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„"""
        failed_file = self.failed_chunks_file_var.get().strip()
        if not failed_file:
            messagebox.showwarning("ê²½ê³ ", "ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        if not os.path.exists(failed_file):
            messagebox.showerror("ì˜¤ë¥˜", f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{failed_file}")
            return
        
        if not self.api_key_var.get():
            messagebox.showwarning("ê²½ê³ ", "API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        try:
            with open(failed_file, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(failed_file)
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨:\n{e}")
            return
        
        if not failed_chunks:
            messagebox.showinfo("ì•Œë¦¼", "ì¬ì‹œë„í•  ì‹¤íŒ¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if messagebox.askyesno("í™•ì¸", f"{len(failed_chunks)}ê°œ ì‹¤íŒ¨í•œ ì²­í¬ë¥¼ ì¬ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_retry_failed_chunks, args=(failed_chunks,))
            t.daemon = True
            t.start()
    
    def _run_retry_failed_chunks(self, failed_chunks):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì‹¤í–‰"""
        key = self.api_key_var.get().strip()
        import httpx
        timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
        
        self.append_log(f"[RETRY] ì‹¤íŒ¨í•œ ì²­í¬ {len(failed_chunks)}ê°œ ì¬ì‹œë„ ì‹œì‘...")
        
        retry_batch_ids = []
        for failed_info in failed_chunks:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            excel_path = failed_info.get("excel_path", "")
            model_name = failed_info.get("model_name", DEFAULT_MODEL)
            effort = failed_info.get("effort", "low")
            batch_group_id = failed_info.get("batch_group_id", "")
            
            if not os.path.exists(chunk_file):
                self.append_log(f"âš ï¸ ì²­í¬ {chunk_num}: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_file}")
                continue
            
            self.append_log(f"[RETRY] ì²­í¬ {chunk_num} ì¬ì‹œë„ ì¤‘... ({os.path.basename(chunk_file)})")
            
            try:
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=chunk_file,
                    excel_path=excel_path,
                    model_name=model_name,
                    reasoning_effort=effort,
                )
                
                batch_id = batch.id
                retry_batch_ids.append(batch_id)
                self.append_log(f"âœ… ì²­í¬ {chunk_num} ì¬ì‹œë„ ì„±ê³µ: {batch_id}")
                
                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=excel_path,
                    jsonl_path=chunk_file,
                    model=model_name,
                    effort=effort,
                    status=batch.status,
                    output_file_id=None,
                    batch_group_id=batch_group_id,
                    chunk_index=chunk_num,
                )
                
            except Exception as e:
                self.append_log(f"âŒ ì²­í¬ {chunk_num} ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        if retry_batch_ids:
            self.append_log(f"âœ… ì¬ì‹œë„ ì™„ë£Œ: {len(retry_batch_ids)}ê°œ ë°°ì¹˜ ìƒì„±ë¨")
            # ë°°ì¹˜ ëª©ë¡ ê°±ì‹  ë° ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
            self.after(0, lambda: [
                self._load_jobs_all(),
                self._load_archive_list(),
                self.main_tabs.select(self.tab_manage),  # ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
                messagebox.showinfo("ì™„ë£Œ", f"{len(retry_batch_ids)}ê°œ ì²­í¬ ì¬ì‹œë„ ì„±ê³µ:\n{', '.join(retry_batch_ids[:5])}{'...' if len(retry_batch_ids) > 5 else ''}\n\në°°ì¹˜ ê´€ë¦¬ íƒ­ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.")
            ])
        else:
            self.append_log(f"âš ï¸ ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.after(0, lambda: messagebox.showwarning("ê²½ê³ ", "ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤."))


if __name__ == "__main__":
    app = ImageAnalysisBatchGUI()
    app.mainloop()

