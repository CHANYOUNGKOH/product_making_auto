"""
bg_Batch_prompt_gui_Casche.py

Stage 3-2: ì „ì²˜ë¦¬ -> AIë°°ê²½ìƒì„± ë¶„ì„ ì‘ì—… (ë°°ì¹˜/ëŒ€ëŸ‰) - ìºì‹± ìµœì í™” ë²„ì „
- ê¸°ëŠ¥: Batch JSONL ìƒì„± -> ì—…ë¡œë“œ -> ì‹¤í–‰ -> ê²°ê³¼ ë³‘í•©
- bg_prompt_core_Casche.pyë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬
- ì…ë ¥: I2 ë˜ëŠ” I3 íŒŒì¼ë§Œ í—ˆìš©
- ì¶œë ¥: í•­ìƒ I4ë¡œ ê³ ì •
- ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™”: OpenAI Prompt Caching ê°€ì´ë“œì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì¬êµ¬ì„±
  * ì •ì  ì½˜í…ì¸ (ì—­í• , ì œì•½, ê·œì¹™)ë¥¼ system í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * ë™ì  ì½˜í…ì¸ (ì…ë ¥ ë°ì´í„°)ë¥¼ user í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * prompt_cache_key ì‚¬ìš©ìœ¼ë¡œ ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ (í† í° ë¹„ìš© ìµœëŒ€ 90% ì ˆê° ê°€ëŠ¥)
"""

import os
import json
import re
import threading
import subprocess
import platform
from datetime import datetime
from typing import Optional

import pandas as pd
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, Menu
from tkinter.scrolledtext import ScrolledText

from openai import OpenAI

# [í•„ìˆ˜ ì˜ì¡´ì„±] bg_prompt_core_Casche.py
# ìºì‹± ìµœì í™” ë²„ì „ ì‚¬ìš© (bg_prompt_core_Casche.py)
try:
    from bg_prompt_core_Casche import (
        API_KEY_FILE,
        DEFAULT_MODEL,
        load_api_key_from_file,
        save_api_key_to_file,
        build_bg_prompt_messages,
        build_bg_prompt_batch_payload,  # Batch APIìš© payload ë¹Œë” (ìºì‹± ìµœì í™”)
        MODEL_PRICING_USD_PER_MTOK,
    )
    CACHE_MODE_CORE = True
except ImportError:
    # ìºì‹± ë²„ì „ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ë²„ì „ ì‚¬ìš©
    try:
        from bg_prompt_core import (
            API_KEY_FILE,
            DEFAULT_MODEL,
            load_api_key_from_file,
            save_api_key_to_file,
            build_bg_prompt_messages,
            MODEL_PRICING_USD_PER_MTOK,
        )
        CACHE_MODE_CORE = False
        def build_bg_prompt_batch_payload(*args, **kwargs): return None
    except ImportError:
        # ì˜ì¡´ì„± íŒŒì¼ ë¶€ì¬ ì‹œ ë¹„ìƒìš© ë”ë¯¸
        CACHE_MODE_CORE = False
        API_KEY_FILE = ".openai_api_key_bg_prompt"
        DEFAULT_MODEL = "gpt-5-mini"
        MODEL_PRICING_USD_PER_MTOK = {}
        def load_api_key_from_file(x): return ""
        def save_api_key_to_file(x, y): pass
        def build_bg_prompt_messages(*args, **kwargs): return []
        def build_bg_prompt_batch_payload(*args, **kwargs): return None

# ========================================================
# ë©”ì¸ ëŸ°ì²˜ ì—°ë™ìš© JobManager & íŒŒì¼ëª… ìœ í‹¸
# ========================================================
def get_root_filename(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´(_T*_I* ë˜ëŠ” _t*_i*, T4(ì™„)_I* í¬í•¨) ë° ê¸°íƒ€ ê¼¬ë¦¬í‘œë¥¼ ë–¼ê³  ì›ë³¸ëª…(Key)ë§Œ ì¶”ì¶œ
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0.xlsx -> ë‚˜ì´í‚¤.xlsx
    ì˜ˆ: ì•„ë””ë‹¤ìŠ¤_T3_I3.xlsx -> ì•„ë””ë‹¤ìŠ¤.xlsx
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0(ì—…ì™„).xlsx -> ë‚˜ì´í‚¤.xlsx
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0_T1_I1.xlsx -> ë‚˜ì´í‚¤.xlsx (ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ ì œê±°)
    ì˜ˆ: ë‚˜ì´í‚¤_T4(ì™„)_I3.xlsx -> ë‚˜ì´í‚¤.xlsx
    """
    name = os.path.basename(filename)
    base, ext = os.path.splitext(name)
    
    # 1. ë²„ì „ íŒ¨í„´ (_Tìˆ«ì(ê´„í˜¸)?_Iìˆ«ì ë˜ëŠ” _tìˆ«ì(ê´„í˜¸)?_iìˆ«ì) ë°˜ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    # íŒ¨í„´ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì œê±°, T4(ì™„)_I* íŒ¨í„´ë„ í¬í•¨
    while True:
        new_base = re.sub(r"_[Tt]\d+\([^)]*\)_[Ii]\d+", "", base, flags=re.IGNORECASE)  # T4(ì™„)_I* íŒ¨í„´ ì œê±°
        new_base = re.sub(r"_[Tt]\d+_[Ii]\d+", "", new_base, flags=re.IGNORECASE)  # ì¼ë°˜ T*_I* íŒ¨í„´ ì œê±°
        if new_base == base:
            break
        base = new_base
    
    # 2. ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„), (ì™„ë£Œ) ë“±) - ë²„ì „ íŒ¨í„´ì˜ ê´„í˜¸ëŠ” ì´ë¯¸ ì œê±°ë¨
    base = re.sub(r"\([^)]*\)", "", base)
    
    # 3. ê¸°íƒ€ êµ¬í˜• ê¼¬ë¦¬í‘œ ì œê±° (í˜¸í™˜ì„± ìœ ì§€)
    suffixes = ["_bg_prompt_done", "_bg_prompt_batch_done", "_stage1_mapping", "_stage1_img_mapping", "_stage2_analysis", "_stage3_done", "_stage4_2_done", "_with_images"]
    for s in suffixes:
        base = base.replace(s, "")
    
    # 4. ëì— ë‚¨ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    base = base.rstrip("_")
        
    return base + ext


def get_i4_output_path(input_path: str) -> str:
    """
    ì…ë ¥ íŒŒì¼ëª…ì„ ë¶„ì„í•´ì„œ I4ë¡œ ê³ ì •ëœ ì¶œë ¥ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì…ë ¥: I2 ë˜ëŠ” I3 íŒŒì¼ (ì˜ˆ: ìƒí’ˆ_T3_I2.xlsx, ìƒí’ˆ_T3_I3.xlsx, ìƒí’ˆ_T4(ì™„)_I3.xlsx)
    ì¶œë ¥: í•­ìƒ I4 (ì˜ˆ: ìƒí’ˆ_T3_I4.xlsx, ìƒí’ˆ_T4(ì™„)_I4.xlsx)
    """
    dir_name = os.path.dirname(input_path)
    base_name = os.path.basename(input_path)
    name_only, ext = os.path.splitext(base_name)

    # T4(ì™„)_I* ë˜ëŠ” ì¼ë°˜ _T*_I* íŒ¨í„´ ë§¤ì¹­
    pattern = r"_T(\d+)(\([^)]+\))?_I(\d+)$"
    match = re.search(pattern, name_only, re.IGNORECASE)

    if match:
        current_t = int(match.group(1))
        t_suffix = match.group(2) or ""  # (ì™„) ë¶€ë¶„ì´ ìˆìœ¼ë©´ ìœ ì§€
        original_name = name_only[: match.start()]
    else:
        # ë²„ì „ ì •ë³´ê°€ ì—†ìœ¼ë©´ T ë²„ì „ ì¶”ì¶œ ì‹œë„ (ê´„í˜¸ í¬í•¨ ê°€ëŠ¥)
        t_match = re.search(r"_T(\d+)(\([^)]+\))?", name_only, re.IGNORECASE)
        if t_match:
            current_t = int(t_match.group(1))
            t_suffix = t_match.group(2) or ""
            original_name = name_only[: t_match.start()]
        else:
            current_t = 0
            t_suffix = ""
            original_name = name_only

    # í•­ìƒ I4ë¡œ ê³ ì •, T ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì˜ˆ: T4(ì™„) ë˜ëŠ” T4)
    new_filename = f"{original_name}_T{current_t}{t_suffix}_I4{ext}"
    return os.path.join(dir_name, new_filename)


class JobManager:
    DB_FILE = None

    @classmethod
    def find_db_path(cls):
        if cls.DB_FILE and os.path.exists(cls.DB_FILE):
            return cls.DB_FILE

        current_dir = os.path.dirname(os.path.abspath(__file__))
        search_dirs = [
            current_dir,
            os.path.abspath(os.path.join(current_dir, "..")),
            os.path.abspath(os.path.join(current_dir, "..", "..")),
        ]

        for d in search_dirs:
            target = os.path.join(d, "job_history.json")
            if os.path.exists(target):
                cls.DB_FILE = target
                return target

        default_path = os.path.abspath(os.path.join(current_dir, "..", "job_history.json"))
        cls.DB_FILE = default_path
        return default_path

    @classmethod
    def load_jobs(cls):
        db_path = cls.find_db_path()
        if not os.path.exists(db_path):
            return {}
        try:
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    @classmethod
    def update_status(cls, filename, text_msg=None, img_msg=None, img_s3_1_msg=None, img_s3_2_msg=None):
        """
        ì‘ì—… ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            filename: íŒŒì¼ëª… (root filename)
            text_msg: í…ìŠ¤íŠ¸ ìƒíƒœ ë©”ì‹œì§€
            img_msg: ì´ë¯¸ì§€ ì „ì²´ ìƒíƒœ ë©”ì‹œì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
            img_s3_1_msg: Stage 3-1 (ì¸ë„¤ì¼ ë¶„ì„) ìƒíƒœ ë©”ì‹œì§€
            img_s3_2_msg: Stage 3-2 (ì „ì²˜ë¦¬) ìƒíƒœ ë©”ì‹œì§€
        """
        db_path = cls.find_db_path()
        data = cls.load_jobs()
        now = datetime.now().strftime("%m-%d %H:%M")

        if filename not in data:
            data[filename] = {
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "text_status": "ëŒ€ê¸°",
                "text_time": "-",
                "image_status": "ëŒ€ê¸°",
                "image_time": "-",
                "image_s3_1_status": "-",  # Stage 3-1: ì¸ë„¤ì¼ ë¶„ì„
                "image_s3_1_time": "-",
                "image_s3_2_status": "-",  # Stage 3-2: ì „ì²˜ë¦¬
                "image_s3_2_time": "-",
                "memo": "",
            }

        if text_msg:
            data[filename]["text_status"] = text_msg
            data[filename]["text_time"] = now
        
        if img_msg:
            # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ image_statusë„ ì—…ë°ì´íŠ¸
            data[filename]["image_status"] = img_msg
            data[filename]["image_time"] = now
        
        if img_s3_1_msg:
            data[filename]["image_s3_1_status"] = img_s3_1_msg
            data[filename]["image_s3_1_time"] = now
            # image_status í†µí•© ì—…ë°ì´íŠ¸ (S3-1, S3-2 ì ‘ë‘ì‚¬ ì œê±°)
            parts = []
            if data[filename].get("image_s3_1_status", "-") != "-":
                parts.append(data[filename]['image_s3_1_status'])  # "I3-1 (ì§„í–‰ì¤‘)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if data[filename].get("image_s3_2_status", "-") != "-":
                parts.append(data[filename]['image_s3_2_status'])  # "I3-2 (ì™„ë£Œ)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if parts:
                data[filename]["image_status"] = " / ".join(parts)
                data[filename]["image_time"] = now
        
        if img_s3_2_msg:
            data[filename]["image_s3_2_status"] = img_s3_2_msg
            data[filename]["image_s3_2_time"] = now
            # image_status í†µí•© ì—…ë°ì´íŠ¸ (S3-1, S3-2 ì ‘ë‘ì‚¬ ì œê±°)
            parts = []
            if data[filename].get("image_s3_1_status", "-") != "-":
                parts.append(data[filename]['image_s3_1_status'])  # "I3-1 (ì§„í–‰ì¤‘)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if data[filename].get("image_s3_2_status", "-") != "-":
                parts.append(data[filename]['image_s3_2_status'])  # "I3-2 (ì™„ë£Œ)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if parts:
                data[filename]["image_status"] = " / ".join(parts)
                data[filename]["image_time"] = now

        data[filename]["last_update"] = now

        try:
            with open(db_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"[JobManager Error] {e}")


def safe_save_excel(df: pd.DataFrame, path: str) -> bool:
    """ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆì–´ ì €ì¥ì´ ì•ˆ ë  ë•Œ ì¬ì‹œë„ë¥¼ ìœ ë„í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            df.to_excel(path, index=False)
            return True
        except PermissionError:
            if not messagebox.askretrycancel(
                "ì €ì¥ ì‹¤íŒ¨",
                f"ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤!\n[{os.path.basename(path)}]\n\n"
                "íŒŒì¼ì„ ë‹«ê³  'ë‹¤ì‹œ ì‹œë„'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
            ):
                return False
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False


# === ê¸°ë³¸ ì„¤ì • ===
BATCH_JOBS_FILE = os.path.join(os.path.dirname(__file__), "bg_prompt_batch_jobs.json")

# --- UI ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ---
COLOR_BG = "#F8F9FA"
COLOR_WHITE = "#FFFFFF"
COLOR_PRIMARY = "#4A90E2"
COLOR_PRIMARY_HOVER = "#357ABD"
COLOR_SUCCESS = "#28A745"
COLOR_SUCCESS_HOVER = "#218838"
COLOR_DANGER = "#DC3545"
COLOR_DANGER_HOVER = "#C82333"
COLOR_TEXT = "#333333"
COLOR_HEADER = "#E9ECEF"

def get_seoul_now():
    try:
        from pytz import timezone
        return datetime.now(timezone("Asia/Seoul"))
    except:
        return datetime.now()

# ========================================================
# ë°°ì¹˜ ì¡ ê´€ë¦¬ (JSON DB)
# ========================================================
def load_batch_jobs():
    if not os.path.exists(BATCH_JOBS_FILE):
        return []
    try:
        with open(BATCH_JOBS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return []

def save_batch_jobs(jobs):
    try:
        with open(BATCH_JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"[Error] ì¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_excel_name_from_path(path: str) -> str:
    """ì „ì²´ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ"""
    if not path:
        return "-"
    return os.path.basename(path)

def upsert_batch_job(batch_id, **kwargs):
    jobs = load_batch_jobs()
    found = False
    now_str = datetime.now().isoformat()
    
    for j in jobs:
        if j["batch_id"] == batch_id:
            if kwargs.get("status") == "completed" and j.get("status") != "completed":
                if "completed_at" not in kwargs:
                    j["completed_at"] = now_str
            j.update(kwargs)
            j["updated_at"] = now_str
            found = True
            break
            
    if not found:
        new_job = {
            "batch_id": batch_id,
            "created_at": now_str,
            "updated_at": now_str,
            "completed_at": "",
            "archived": False,
            **kwargs
        }
        jobs.insert(0, new_job)
    save_batch_jobs(jobs)

def archive_batch_job(batch_ids, archive=True):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    for j in jobs:
        if j["batch_id"] in batch_ids:
            j["archived"] = archive
    save_batch_jobs(jobs)

def hard_delete_batch_job(batch_ids):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    jobs = [j for j in jobs if j["batch_id"] not in batch_ids]
    save_batch_jobs(jobs)

# ========================================================
# GUI Class
# ========================================================
class BGPromptBatchGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Stage 3-2: ì „ì²˜ë¦¬ -> AIë°°ê²½ìƒì„± ë¶„ì„ ì‘ì—… (ë°°ì¹˜/ëŒ€ëŸ‰) ğŸš€ ìºì‹± ìµœì í™” ë²„ì „")
        self.geometry("1250x950")
        
        self.api_key_var = tk.StringVar()
        
        # íŒŒì¼ ë³€ìˆ˜
        self.src_file_var = tk.StringVar()
        self.skip_exist_var = tk.BooleanVar(value=True)
        self.jsonl_file_var = tk.StringVar()  # ìƒì„±ëœ JSONL íŒŒì¼ ê²½ë¡œ
        
        # ëª¨ë¸ ì„¤ì • ë³€ìˆ˜
        self.model_var = tk.StringVar(value="gpt-5-mini")
        self.effort_var = tk.StringVar(value="low")
        
        # íƒ­ 3 ë³€ìˆ˜
        self.batch_id_var = tk.StringVar()
        self.failed_chunks_file_var = tk.StringVar()
        
        self._configure_styles()
        self._init_ui()
        self._load_key()

    def _configure_styles(self):
        style = ttk.Style()
        try: style.theme_use('clam')
        except: pass
        
        self.configure(background=COLOR_BG)
        
        style.configure("TFrame", background=COLOR_BG)
        style.configure("TLabel", background=COLOR_BG, foreground=COLOR_TEXT, font=("ë§‘ì€ ê³ ë”•", 10))
        style.configure("Header.TLabel", font=("ë§‘ì€ ê³ ë”•", 11, "bold"), foreground="#444")
        
        style.configure("TLabelframe", background=COLOR_BG, bordercolor="#D0D7DE")
        style.configure("TLabelframe.Label", background=COLOR_BG, foreground="#0056b3", font=("ë§‘ì€ ê³ ë”•", 10, "bold"))

        style.configure("TNotebook", background=COLOR_BG, borderwidth=0)
        style.configure("TNotebook.Tab", background="#E1E4E8", padding=[12, 5], font=("ë§‘ì€ ê³ ë”•", 10))
        style.map("TNotebook.Tab", background=[("selected", COLOR_WHITE)], foreground=[("selected", COLOR_PRIMARY)])
        
        style.configure("Treeview", background=COLOR_WHITE, fieldbackground=COLOR_WHITE, font=("ë§‘ì€ ê³ ë”•", 9), rowheight=28)
        style.configure("Treeview.Heading", background=COLOR_HEADER, foreground="#333", font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        style.map("Treeview", background=[('selected', '#CCE5FF')], foreground=[('selected', 'black')])

        style.configure("TButton", font=("ë§‘ì€ ê³ ë”•", 9), padding=5, borderwidth=1)
        style.configure("Primary.TButton", background=COLOR_PRIMARY, foreground="white", bordercolor=COLOR_PRIMARY)
        style.map("Primary.TButton", background=[("active", COLOR_PRIMARY_HOVER)])
        style.configure("Success.TButton", background=COLOR_SUCCESS, foreground="white", bordercolor=COLOR_SUCCESS)
        style.map("Success.TButton", background=[("active", COLOR_SUCCESS_HOVER)])
        style.configure("Danger.TButton", background=COLOR_DANGER, foreground="white", bordercolor=COLOR_DANGER)
        style.map("Danger.TButton", background=[("active", COLOR_DANGER_HOVER)])

    def _init_ui(self):
        main_container = ttk.Frame(self, padding=15)
        main_container.pack(fill='both', expand=True)

        # 1. ìƒë‹¨ API Key
        f_top = ttk.LabelFrame(main_container, text="ğŸ”‘ API ì„¤ì •", padding=10)
        f_top.pack(fill='x', pady=(0, 10))
        ttk.Label(f_top, text="Batch API Key:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(side='left')
        entry_key = ttk.Entry(f_top, textvariable=self.api_key_var, show="*", width=50, font=("Consolas", 10))
        entry_key.pack(side='left', padx=10)
        ttk.Button(f_top, text="ì €ì¥", command=self._save_key, style="Primary.TButton").pack(side='left')

        # 2. ë©”ì¸ íƒ­
        self.main_tabs = ttk.Notebook(main_container)
        self.main_tabs.pack(fill='both', expand=True, pady=5)
        
        self.tab_create = ttk.Frame(self.main_tabs)
        self.tab_manage = ttk.Frame(self.main_tabs) 
        self.tab_merge = ttk.Frame(self.main_tabs)
        
        self.main_tabs.add(self.tab_create, text=" 1. ë°°ì¹˜ ìƒì„± & ì—…ë¡œë“œ ")
        self.main_tabs.add(self.tab_manage, text=" 2. ë°°ì¹˜ ê´€ë¦¬ (ëª©ë¡/ë³‘í•©) ")
        self.main_tabs.add(self.tab_merge, text=" 3. ê°œë³„ ë³‘í•© (ìˆ˜ë™) ")
        
        self._init_tab_create()
        self._init_tab_manage()
        self._init_tab_merge()
        
        # 3. ë¡œê·¸
        f_log = ttk.LabelFrame(main_container, text="ğŸ“‹ ì‹œìŠ¤í…œ ë¡œê·¸", padding=10)
        f_log.pack(fill='both', expand=True, pady=(10, 0))
        self.log_widget = ScrolledText(f_log, height=22, state='disabled', font=("Consolas", 9), bg="#F1F3F5")
        self.log_widget.pack(fill='both', expand=True)

    def _load_key(self):
        loaded = load_api_key_from_file(API_KEY_FILE)
        if loaded: self.api_key_var.set(loaded)

    def _save_key(self):
        k = self.api_key_var.get().strip()
        if k:
            save_api_key_to_file(k, API_KEY_FILE)
            messagebox.showinfo("ì €ì¥", "API Key ì €ì¥ ì™„ë£Œ")

    def append_log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        self.log_widget.config(state='normal')
        self.log_widget.insert('end', f"[{ts}] {msg}\n")
        self.log_widget.see('end')
        self.log_widget.config(state='disabled')

    # ----------------------------------------------------
    # Tab 1: Create (ìƒì„±)
    # ----------------------------------------------------
    def _init_tab_create(self):
        container = ttk.Frame(self.tab_create, padding=20)
        container.pack(fill='both', expand=True)
        
        # Step 1: íŒŒì¼
        f_file = ttk.LabelFrame(container, text="1. ì‘ì—… ëŒ€ìƒ íŒŒì¼ (ST2_JSON í¬í•¨, I2 ë˜ëŠ” I3)", padding=15)
        f_file.pack(fill='x', pady=(0, 15))
        ttk.Entry(f_file, textvariable=self.src_file_var, font=("ë§‘ì€ ê³ ë”•", 10)).pack(side='left', fill='x', expand=True)
        ttk.Button(f_file, text="ğŸ“‚ íŒŒì¼ ì°¾ê¸°", command=self._select_src_file).pack(side='right', padx=5)
        
        # Step 2: ëª¨ë¸ ì„¤ì •
        f_opt = ttk.LabelFrame(container, text="2. ëª¨ë¸ ì„¤ì •", padding=15)
        f_opt.pack(fill='x', pady=5)

        # ëª¨ë¸ & Effort
        fr1 = ttk.Frame(f_opt)
        fr1.pack(fill='x', pady=5)
        ttk.Label(fr1, text="ëª¨ë¸ (Model):", width=12).pack(side='left')
        models = list(MODEL_PRICING_USD_PER_MTOK.keys()) if MODEL_PRICING_USD_PER_MTOK else ["gpt-5-mini", "gpt-5", "gpt-5-nano"]
        cb_model = ttk.Combobox(fr1, textvariable=self.model_var, values=models, state="readonly", width=20)
        cb_model.pack(side='left', padx=5)
        
        ttk.Label(fr1, text="ì¶”ë¡  ê°•ë„:", width=10).pack(side='left', padx=(20, 5))
        ttk.Combobox(fr1, textvariable=self.effort_var, values=["none", "low", "medium", "high"], state="readonly", width=12).pack(side='left', padx=5)
        
        # ì²´í¬ë°•ìŠ¤
        f_row_chk = ttk.Frame(f_opt)
        f_row_chk.pack(fill='x', pady=10)
        ttk.Checkbutton(f_row_chk, text=" ì´ë¯¸ bg_positive_en ë“±ì´ ìˆëŠ” í–‰ ê±´ë„ˆë›°ê¸°", variable=self.skip_exist_var).pack(side='left')
        
        # Step 3: JSONL ìƒì„±
        f_step3 = ttk.LabelFrame(container, text="3. JSONL ìƒì„±", padding=15)
        f_step3.pack(fill='x', pady=15)
        
        # ìƒì„±ëœ JSONL íŒŒì¼ ê²½ë¡œ í‘œì‹œ
        f_jsonl = ttk.Frame(f_step3)
        f_jsonl.pack(fill='x', pady=(0, 10))
        ttk.Label(f_jsonl, text="JSONL íŒŒì¼ ê²½ë¡œ:", font=("ë§‘ì€ ê³ ë”•", 9)).pack(side='left')
        ttk.Entry(f_jsonl, textvariable=self.jsonl_file_var, font=("Consolas", 9), width=60).pack(side='left', padx=5, fill='x', expand=True)
        ttk.Button(f_jsonl, text="ğŸ“‚ ì°¾ê¸°", command=self._select_jsonl_file).pack(side='right', padx=5)
        
        # ë¶„ë¦¬ëœ ë²„íŠ¼ë“¤
        f_btn_separated = ttk.Frame(f_step3)
        f_btn_separated.pack(fill='x', pady=5)
        btn_create = ttk.Button(f_btn_separated, text="ğŸ“„ JSONL ìƒì„±ë§Œ (Create JSONL)", command=self._create_jsonl_only, style="Primary.TButton")
        btn_create.pack(side='left', fill='x', expand=True, padx=(0, 5), ipady=6)
        btn_upload = ttk.Button(f_btn_separated, text="â¬†ï¸ ë°°ì¹˜ ì—…ë¡œë“œ (Upload Batch)", command=self._upload_batch_from_jsonl, style="Success.TButton")
        btn_upload.pack(side='right', fill='x', expand=True, padx=(5, 0), ipady=6)
        
        # êµ¬ë¶„ì„ 
        ttk.Separator(f_step3, orient='horizontal').pack(fill='x', pady=15)
        
        # í†µí•© ë²„íŠ¼ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
        f_step4 = ttk.LabelFrame(container, text="4. í†µí•© ì‹¤í–‰ (ê¸°ì¡´ ë°©ì‹)", padding=15)
        f_step4.pack(fill='x', pady=15)
        btn_integrated = ttk.Button(f_step4, text="ğŸš€ JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (í†µí•©)", command=self._start_create_batch, style="Success.TButton")
        btn_integrated.pack(fill='x', ipady=8)
        ttk.Label(container, text="â€» ë°°ì¹˜ APIëŠ” ê²°ê³¼ ìˆ˜ì‹ ê¹Œì§€ ìµœëŒ€ 24ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. (ë¹„ìš© 50% ì ˆê°)", foreground="#666").pack(pady=(5, 0))

    def _select_jsonl_file(self):
        """JSONL íŒŒì¼ ì„ íƒ"""
        p = filedialog.askopenfilename(
            title="JSONL íŒŒì¼ ì„ íƒ",
            filetypes=[("JSONL", "*.jsonl"), ("ëª¨ë“  íŒŒì¼", "*.*")]
        )
        if p:
            self.jsonl_file_var.set(p)
            self.append_log(f"JSONL íŒŒì¼ ì„ íƒë¨: {os.path.basename(p)}")
    
    def _select_src_file(self):
        p = filedialog.askopenfilename(
            title="ë°°ê²½ ë¶„ì„ ì—‘ì…€ ì„ íƒ (T2 ì´ìƒ _I3 ë²„ì „ë§Œ ê°€ëŠ¥)",
            filetypes=[("Excel", "*.xlsx;*.xls")]
        )
        if p:
            base_name = os.path.basename(p)
            
            # T ë²„ì „ê³¼ I ë²„ì „ ê²€ì¦
            t_match = re.search(r"_T(\d+)", base_name, re.IGNORECASE)
            i_match = re.search(r"_I(\d+)", base_name, re.IGNORECASE)
            
            if not t_match or not i_match:
                messagebox.showerror(
                    "ì˜¤ë¥˜",
                    f"íŒŒì¼ëª…ì— ë²„ì „ ì •ë³´(_T*_I*)ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
                    f"ì„ íƒí•œ íŒŒì¼: {base_name}\n\n"
                    f"T2 ì´ìƒ _I3 ë²„ì „ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\n"
                    f"(ì˜ˆ: T2_I3, T4_I3, T4(ì™„)_I3)"
                )
                return
            
            t_version = int(t_match.group(1))
            i_version = int(i_match.group(1))
            
            # T2 ì´ìƒì´ê³  I3ì¸ì§€ ê²€ì¦
            if t_version < 2:
                messagebox.showerror(
                    "ì˜¤ë¥˜",
                    f"ì…ë ¥ íŒŒì¼ì€ T2 ì´ìƒ _I3 ë²„ì „ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.\n\n"
                    f"ì„ íƒí•œ íŒŒì¼: {base_name}\n"
                    f"í˜„ì¬ ë²„ì „: T{t_version}_I{i_version}\n\n"
                    f"T2 ì´ìƒ _I3 ë²„ì „ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\n"
                    f"(ì˜ˆ: T2_I3, T4_I3, T4(ì™„)_I3)"
                )
                return
            
            if i_version != 3:
                messagebox.showerror(
                    "ì˜¤ë¥˜",
                    f"ì…ë ¥ íŒŒì¼ì€ T2 ì´ìƒ _I3 ë²„ì „ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.\n\n"
                    f"ì„ íƒí•œ íŒŒì¼: {base_name}\n"
                    f"í˜„ì¬ ë²„ì „: T{t_version}_I{i_version}\n\n"
                    f"T2 ì´ìƒ _I3 ë²„ì „ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\n"
                    f"(ì˜ˆ: T2_I3, T4_I3, T4(ì™„)_I3)"
                )
                return
            
            # ì—‘ì…€ íŒŒì¼ ê²€ì¦
            try:
                df_check = pd.read_excel(p)
                
                # view_point ì»¬ëŸ¼ í•„ìˆ˜ ì²´í¬
                if "view_point" not in df_check.columns:
                    messagebox.showerror(
                        "í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½",
                        f"'{base_name}' íŒŒì¼ì— 'view_point' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"
                        f"I3 ì¸ë„¤ì¼ êµ¬ë„ ë¶„ì„ ì‘ì—…ì„ ë¨¼ì € ì™„ë£Œí•´ì£¼ì„¸ìš”."
                    )
                    return
                
                # view_pointê°€ ë¹„ì–´ìˆëŠ” í–‰ì´ ìˆëŠ”ì§€ ì²´í¬
                view_point_empty = df_check["view_point"].isna() | (df_check["view_point"].astype(str).str.strip() == "")
                if view_point_empty.any():
                    empty_count = view_point_empty.sum()
                    if messagebox.askyesno(
                        "ê²½ê³ ",
                        f"'{base_name}' íŒŒì¼ì— 'view_point'ê°€ ë¹„ì–´ìˆëŠ” í–‰ì´ {empty_count}ê°œ ìˆìŠµë‹ˆë‹¤.\n\n"
                        f"ì´ í–‰ë“¤ì€ ì²˜ë¦¬ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n"
                        f"ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
                    ) == False:
                        return
                
                # bg_positive_en ì»¬ëŸ¼ì— ì´ë¯¸ ë‚´ìš©ì´ ìˆëŠ”ì§€ ì²´í¬
                if "bg_positive_en" in df_check.columns:
                    has_content = df_check["bg_positive_en"].notna() & (df_check["bg_positive_en"].astype(str).str.strip() != "")
                    if has_content.any():
                        content_count = has_content.sum()
                        if messagebox.askyesno(
                            "ê²½ê³ ",
                            f"'{base_name}' íŒŒì¼ì— 'bg_positive_en'ì— ì´ë¯¸ ë‚´ìš©ì´ ìˆëŠ” í–‰ì´ {content_count}ê°œ ìˆìŠµë‹ˆë‹¤.\n\n"
                            f"ì´ë¯¸ ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆë›°ê¸° ì˜µì…˜ì´ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ í–‰ì€ ì²˜ë¦¬ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n"
                            f"ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
                        ) == False:
                            return
                
                # ê²€ì¦ í†µê³¼
                self.src_file_var.set(p)
                self.append_log(f"íŒŒì¼ ì„ íƒë¨: {base_name} (I3)")
            except Exception as e:
                messagebox.showerror("íŒŒì¼ ì½ê¸° ì˜¤ë¥˜", f"ì—‘ì…€ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")
                return

    def _create_jsonl_only(self):
        """JSONL ìƒì„±ë§Œ ìˆ˜í–‰ (ë¶„ë¦¬ëœ ë²„íŠ¼)"""
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        t = threading.Thread(target=self._run_create_jsonl)
        t.daemon = True
        t.start()
    
    def _upload_batch_from_jsonl(self):
        """JSONL íŒŒì¼ì—ì„œ ë°°ì¹˜ ì—…ë¡œë“œë§Œ ìˆ˜í–‰ (ë¶„ë¦¬ëœ ë²„íŠ¼)"""
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # JSONL íŒŒì¼ ê²½ë¡œ í™•ì¸
        jsonl_path = self.jsonl_file_var.get().strip()
        if not jsonl_path or not os.path.exists(jsonl_path):
            messagebox.showwarning("ì˜¤ë¥˜", "JSONL íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\n\në¨¼ì € 'JSONL ìƒì„±ë§Œ' ë²„íŠ¼ì„ ì‹¤í–‰í•˜ê±°ë‚˜, ê¸°ì¡´ JSONL íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # ì›ë³¸ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ í™•ì¸ (ë©”íƒ€ë°ì´í„° ì €ì¥ìš©)
        if not self.src_file_var.get():
            # JSONL íŒŒì¼ëª…ì—ì„œ ì›ë³¸ íŒŒì¼ ê²½ë¡œ ì¶”ë¡  ì‹œë„
            jsonl_basename = os.path.basename(jsonl_path)
            if "_bg_prompt_batch_input.jsonl" in jsonl_basename:
                base_path = jsonl_path.replace("_bg_prompt_batch_input.jsonl", "")
                # ê°€ëŠ¥í•œ í™•ì¥ì ì‹œë„
                for ext in [".xlsx", ".xls"]:
                    candidate = base_path + ext
                    if os.path.exists(candidate):
                        self.src_file_var.set(candidate)
                        break
        
        t = threading.Thread(target=self._run_upload_batch, args=(jsonl_path,))
        t.daemon = True
        t.start()
    
    def _start_create_batch(self):
        """í†µí•© ë²„íŠ¼: JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)"""
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        t = threading.Thread(target=self._run_create_batch)
        t.daemon = True
        t.start()
    
    def _run_create_jsonl(self):
        """JSONL ìƒì„±ë§Œ ìˆ˜í–‰í•˜ëŠ” ë¡œì§"""
        src = self.src_file_var.get().strip()
        model_name = self.model_var.get().strip() or "gpt-5-mini"
        reasoning_effort = self.effort_var.get().strip() or "low"
        
        try:
            self.append_log(f"ì—‘ì…€ ë¡œë“œ ì¤‘... {os.path.basename(src)}")
            df = pd.read_excel(src)
            
            if "ST2_JSON" not in df.columns:
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼(ST2_JSON)ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. Stage 2ë¥¼ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.")
            
            # ìºì‹± ëª¨ë“œ í™•ì¸ ë° ë¡œê·¸
            if CACHE_MODE_CORE:
                self.append_log(f"[INFO] ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ëª¨ë“œ í™œì„±í™” (bg_prompt_core_Casche.py)")
            else:
                self.append_log(f"[INFO] âš ï¸ ì¼ë°˜ ëª¨ë“œ (bg_prompt_core.py) - ìºì‹± ìµœì í™” ë¯¸ì ìš©")
            
            self.append_log(f"ì„¤ì •: ëª¨ë¸={model_name}, effort={reasoning_effort}")

            # ì „ì²´ ëŒ€ìƒ ìš”ì²­ ìˆ˜ ê³„ì‚°
            target_rows = 0
            result_cols = ["bg_positive_en", "bg_negative_en", "video_motion_prompt_en", "video_full_prompt_en"]
            
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get():
                    has_result = False
                    for col in result_cols:
                        val = str(row.get(col, "")).strip()
                        if val and val != "nan":
                            has_result = True
                            break
                    if has_result:
                        continue
                
                # ST2_JSON í™•ì¸
                st2_json_raw = str(row.get("ST2_JSON", "")).strip()
                if not st2_json_raw or st2_json_raw == "nan":
                    continue
                
                # view_point í•„ìˆ˜ ì²´í¬
                view_point_val = str(row.get("view_point", "")).strip()
                if not view_point_val or view_point_val == "nan":
                    continue
                
                target_rows += 1

            # ë²„í‚· ìˆ˜ ê³„ì‚° (í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™”)
            if CACHE_MODE_CORE and target_rows > 0:
                PROMPT_CACHE_BUCKETS = 1
                self.append_log(f"[INFO] í”„ë¡¬í”„íŠ¸ ìºì‹±: í‚¤ ê³ ì • ì „ëµ ì‚¬ìš© (ëª¨ë“  ìš”ì²­ì´ 'bg_prompt_v1' í‚¤ ê³µìœ )")
                self.append_log(f"[INFO] ì˜ˆìƒ ìš”ì²­ ìˆ˜: {target_rows}ê°œ, ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ ì˜ˆìƒ")
            else:
                PROMPT_CACHE_BUCKETS = 1

            jsonl_lines = []
            skipped_cnt = 0
            seen_custom_ids = set()
            duplicate_count = 0
            
            # JSONL ë¼ì¸ ìƒì„±
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get():
                    has_result = False
                    for col in result_cols:
                        val = str(row.get(col, "")).strip()
                        if val and val != "nan":
                            has_result = True
                            break
                    if has_result:
                        skipped_cnt += 1
                        continue
                
                # ST2_JSON í™•ì¸
                st2_json_raw = str(row.get("ST2_JSON", "")).strip()
                if not st2_json_raw or st2_json_raw == "nan":
                    skipped_cnt += 1
                    continue
                
                # view_point í•„ìˆ˜ ì²´í¬
                view_point_val = str(row.get("view_point", "")).strip()
                if not view_point_val or view_point_val == "nan":
                    self.append_log(f"[Row {idx+1}] view_pointê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¸ë„¤ì¼ êµ¬ë„ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    skipped_cnt += 1
                    continue
                
                try:
                    # ST2_JSON íŒŒì‹± ì‹œë„
                    try:
                        st2_parsed = json.loads(st2_json_raw)
                        st2_for_model = json.dumps(st2_parsed, ensure_ascii=False, indent=2)
                    except json.JSONDecodeError:
                        st2_for_model = st2_json_raw
                    
                    # IMG_ANALYSIS_JSON êµ¬ì„±
                    img_analysis_data = None
                    img_analysis_cols = [
                        "view_point", "subject_position", "subject_size", "lighting_condition",
                        "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
                        "bg_layout_hint_en"
                    ]
                    if any(col in df.columns for col in img_analysis_cols):
                        img_analysis_data = {}
                        for col in img_analysis_cols:
                            if col in df.columns:
                                val = str(row.get(col, "")).strip()
                                if val and val != "nan":
                                    if col == "is_flat_lay":
                                        img_analysis_data[col] = val.lower() in ["true", "1", "yes", "y"]
                                    else:
                                        img_analysis_data[col] = val
                    
                    # ìºì‹± ìµœì í™” ëª¨ë“œ: build_bg_prompt_batch_payload ì‚¬ìš©
                    if CACHE_MODE_CORE:
                        request_obj = build_bg_prompt_batch_payload(
                            row_index=idx,
                            st2_json_raw=st2_for_model,
                            model_name=model_name,
                            reasoning_effort=reasoning_effort,
                            img_analysis_data=img_analysis_data,
                            use_cache_optimization=True
                        )
                        
                        if request_obj and "body" in request_obj:
                            custom_id = request_obj.get("custom_id", f"row_{idx}")
                            
                            # ì¤‘ë³µ custom_id ì²´í¬
                            if custom_id in seen_custom_ids:
                                duplicate_count += 1
                                continue
                            seen_custom_ids.add(custom_id)
                            
                            # prompt_cache_key: í‚¤ ê³ ì • ì „ëµ
                            request_obj["body"]["prompt_cache_key"] = "bg_prompt_v1"
                            
                            # prompt_cache_retention ì„¤ì •
                            if model_name in ["gpt-5.1", "gpt-5.1-codex", "gpt-5.1-codex-mini", "gpt-5.1-chat-latest", "gpt-5", "gpt-5-codex", "gpt-4.1"]:
                                request_obj["body"]["prompt_cache_retention"] = "extended"
                            elif model_name not in ["gpt-5-mini", "gpt-5-nano"]:
                                request_obj["body"]["prompt_cache_retention"] = "in_memory"
                            
                            # text.format: JSON ì¶œë ¥ ê°•ì œ
                            request_obj["body"]["text"] = {"format": {"type": "json_object"}}
                    else:
                        # ì¼ë°˜ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ ìœ ì§€
                        messages = build_bg_prompt_messages(st2_for_model, img_analysis_data)
                        
                        body = {
                            "model": model_name,
                            "messages": messages,
                        }
                        
                        is_reasoning = any(x in model_name for x in ["gpt-5", "o1", "o3"])
                        if is_reasoning and reasoning_effort != "none":
                            body["reasoning_effort"] = reasoning_effort

                        request_obj = {
                            "custom_id": f"row_{idx}",
                            "method": "POST",
                            "url": "/v1/chat/completions",
                            "body": body
                        }
                    
                    jsonl_lines.append(json.dumps(request_obj, ensure_ascii=False))
                except Exception as e:
                    self.append_log(f"[Row {idx+1}] ìŠ¤í‚µ: {e}")
                    skipped_cnt += 1
                    continue
            
            if duplicate_count > 0:
                self.append_log(f"[WARN] âš ï¸ ì¤‘ë³µ ìš”ì²­ {duplicate_count}ê°œê°€ ê°ì§€ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if not jsonl_lines:
                self.append_log("ìƒì„±í•  ìš”ì²­ ì—†ìŒ.")
                messagebox.showinfo("ì•Œë¦¼", "ìƒì„±í•  ìš”ì²­ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            # JSONL íŒŒì¼ ì €ì¥
            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_bg_prompt_batch_input.jsonl"
            with open(jsonl_path, "w", encoding="utf-8") as f:
                f.write("\n".join(jsonl_lines))
            
            self.append_log(f"âœ… JSONL ìƒì„± ì™„ë£Œ: {len(jsonl_lines)}ê±´ (ìŠ¤í‚µ {skipped_cnt}ê±´)")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ìš”ì²­ ìˆ˜: {len(jsonl_lines)}ê°œ")
            
            # JSONL íŒŒì¼ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ì €ì¥
            self.after(0, lambda: self.jsonl_file_var.set(jsonl_path))
            
            # ì„±ê³µ ë©”ì‹œì§€
            self.after(0, lambda: messagebox.showinfo(
                "JSONL ìƒì„± ì™„ë£Œ",
                f"JSONL íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                f"íŒŒì¼: {os.path.basename(jsonl_path)}\n"
                f"ìš”ì²­ ìˆ˜: {len(jsonl_lines)}ê±´\n"
                f"í¬ê¸°: {jsonl_size_mb:.2f} MB\n\n"
                f"ì´ì œ 'ë°°ì¹˜ ì—…ë¡œë“œ' ë²„íŠ¼ì„ ëˆŒëŸ¬ ì—…ë¡œë“œí•˜ì„¸ìš”."
            ))
            
        except Exception as e:
            self.append_log(f"âŒ JSONL ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            self.after(0, lambda: messagebox.showerror("ì˜¤ë¥˜", f"JSONL ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}"))
    
    def _run_upload_batch(self, jsonl_path):
        """JSONL íŒŒì¼ë¡œë¶€í„° ë°°ì¹˜ ì—…ë¡œë“œë§Œ ìˆ˜í–‰í•˜ëŠ” ë¡œì§"""
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        model_name = self.model_var.get().strip() or "gpt-5-mini"
        reasoning_effort = self.effort_var.get().strip() or "low"
        
        try:
            if not os.path.exists(jsonl_path):
                raise FileNotFoundError(f"JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")
            
            self.append_log(f"JSONL íŒŒì¼ ë¡œë“œ ì¤‘... {os.path.basename(jsonl_path)}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB")
            
            MAX_FILE_SIZE_MB = 180
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            import httpx
            timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
            client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸°ê°€ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤: {jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB")
                batch_ids = self._create_batch_chunks(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src if src else "",
                    model_name=model_name,
                    effort=reasoning_effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=999999,
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                self.after(0, lambda: messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}"))
            else:
                # ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src if src else "",
                    model_name=model_name,
                    reasoning_effort=reasoning_effort,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src if src else "",
                    jsonl_path=jsonl_path,
                    model=model_name,
                    effort=reasoning_effort,
                    status=batch.status,
                    output_file_id=None,
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
                if src:
                    try:
                        root_name = get_root_filename(src)
                        JobManager.update_status(root_name, img_s3_2_msg="I3-2 (ì§„í–‰ì¤‘)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> I3-2 (ì§„í–‰ì¤‘)")
                    except Exception:
                        pass
                
                self.after(0, lambda bid=batch_id: messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {bid}"))
            
            self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])

        except Exception as e:
            self.append_log(f"âŒ ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            self.after(0, lambda err=str(e): messagebox.showerror("ì˜¤ë¥˜", f"ë°°ì¹˜ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{err}"))

    def _run_create_batch(self):
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        
        model_name = self.model_var.get().strip() or "gpt-5-mini"
        reasoning_effort = self.effort_var.get().strip() or "low"
        
        try:
            client = OpenAI(api_key=key)
            self.append_log(f"ì—‘ì…€ ë¡œë“œ ì¤‘... {os.path.basename(src)}")
            df = pd.read_excel(src)
            
            if "ST2_JSON" not in df.columns:
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼(ST2_JSON)ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. Stage 2ë¥¼ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.")
            
            # ìºì‹± ëª¨ë“œ í™•ì¸ ë° ë¡œê·¸
            if CACHE_MODE_CORE:
                self.append_log(f"[INFO] ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ëª¨ë“œ í™œì„±í™” (bg_prompt_core_Casche.py)")
            else:
                self.append_log(f"[INFO] âš ï¸ ì¼ë°˜ ëª¨ë“œ (bg_prompt_core.py) - ìºì‹± ìµœì í™” ë¯¸ì ìš©")
            
            self.append_log(f"ì„¤ì •: ëª¨ë¸={model_name}, effort={reasoning_effort}")

            # ë¨¼ì € ì „ì²´ ëŒ€ìƒ ìš”ì²­ ìˆ˜ë¥¼ ê³„ì‚° (ë²„í‚· ìˆ˜ ê²°ì •ìš©)
            target_rows = 0
            result_cols = ["bg_positive_en", "bg_negative_en", "video_motion_prompt_en", "video_full_prompt_en"]
            
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get():
                    has_result = False
                    for col in result_cols:
                        val = str(row.get(col, "")).strip()
                        if val and val != "nan":
                            has_result = True
                            break
                    if has_result:
                        continue
                
                # ST2_JSON í™•ì¸
                st2_json_raw = str(row.get("ST2_JSON", "")).strip()
                if not st2_json_raw or st2_json_raw == "nan":
                    continue
                
                # view_point í•„ìˆ˜ ì²´í¬
                view_point_val = str(row.get("view_point", "")).strip()
                if not view_point_val or view_point_val == "nan":
                    continue
                
                target_rows += 1

            # ë²„í‚· ìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚° (ëª¨ë“  ìš”ì²­ì— ë™ì¼í•˜ê²Œ ì ìš©)
            if CACHE_MODE_CORE and target_rows > 0:
                # [ë²„í‚· ìˆ˜ ê³„ì‚° ì „ëµ - ì£¼ì˜: OpenAI ê³µì‹ ê¸°ì¤€ì´ ì•„ë‹Œ ì¶”ì •ì¹˜ì…ë‹ˆë‹¤]
                # [í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ì „ëµ - í‚¤ ê³ ì •]
                # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼: ë²„í‚· ë¶„ì‚° ì‹œ ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ì•„ì§ (10% ìˆ˜ì¤€)
                # í•´ê²°ì±…: prompt_cache_keyë¥¼ í•˜ë‚˜ë¡œ ê³ ì •í•˜ì—¬ ëª¨ë“  ìš”ì²­ì´ ê°™ì€ ìºì‹œ í’€ ê³µìœ 
                # Batch APIëŠ” 24ì‹œê°„ì— ê±¸ì³ ì²˜ë¦¬ë˜ë¯€ë¡œ overflow ìš°ë ¤ëŠ” ë‚®ìŒ
                PROMPT_CACHE_BUCKETS = 1
                
                self.append_log(f"[INFO] í”„ë¡¬í”„íŠ¸ ìºì‹±: í‚¤ ê³ ì • ì „ëµ ì‚¬ìš© (ëª¨ë“  ìš”ì²­ì´ 'bg_prompt_v1' í‚¤ ê³µìœ )")
                self.append_log(f"[INFO] ì˜ˆìƒ ìš”ì²­ ìˆ˜: {target_rows}ê°œ, ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ ì˜ˆìƒ")
            else:
                PROMPT_CACHE_BUCKETS = 1

            jsonl_lines = []
            skipped_cnt = 0
            seen_custom_ids = set()
            duplicate_count = 0
            
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get():
                    has_result = False
                    for col in result_cols:
                        val = str(row.get(col, "")).strip()
                        if val and val != "nan":
                            has_result = True
                            break
                    if has_result:
                        skipped_cnt += 1
                        continue
                
                # ST2_JSON í™•ì¸
                st2_json_raw = str(row.get("ST2_JSON", "")).strip()
                if not st2_json_raw or st2_json_raw == "nan":
                    skipped_cnt += 1
                    continue
                
                # view_point í•„ìˆ˜ ì²´í¬
                view_point_val = str(row.get("view_point", "")).strip()
                if not view_point_val or view_point_val == "nan":
                    self.append_log(f"[Row {idx+1}] view_pointê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¸ë„¤ì¼ êµ¬ë„ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    skipped_cnt += 1
                    continue
                
                try:
                    # ST2_JSON íŒŒì‹± ì‹œë„
                    try:
                        st2_parsed = json.loads(st2_json_raw)
                        st2_for_model = json.dumps(st2_parsed, ensure_ascii=False, indent=2)
                    except json.JSONDecodeError:
                        st2_for_model = st2_json_raw
                    
                    # IMG_ANALYSIS_JSON êµ¬ì„± (ì´ë¯¸ì§€ ë¶„ì„ ì»¬ëŸ¼ì—ì„œ ì½ê¸°)
                    img_analysis_data = None
                    img_analysis_cols = [
                        "view_point", "subject_position", "subject_size", "lighting_condition",
                        "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
                        "bg_layout_hint_en"
                    ]
                    if any(col in df.columns for col in img_analysis_cols):
                        img_analysis_data = {}
                        for col in img_analysis_cols:
                            if col in df.columns:
                                val = str(row.get(col, "")).strip()
                                if val and val != "nan":
                                    # is_flat_layëŠ” booleanìœ¼ë¡œ ë³€í™˜ ì‹œë„
                                    if col == "is_flat_lay":
                                        img_analysis_data[col] = val.lower() in ["true", "1", "yes", "y"]
                                    else:
                                        img_analysis_data[col] = val
                    
                    # ìºì‹± ìµœì í™” ëª¨ë“œ: build_bg_prompt_batch_payload ì‚¬ìš©
                    if CACHE_MODE_CORE:
                        request_obj = build_bg_prompt_batch_payload(
                            row_index=idx,
                            st2_json_raw=st2_for_model,
                            model_name=model_name,
                            reasoning_effort=reasoning_effort,
                            img_analysis_data=img_analysis_data,
                            use_cache_optimization=True
                        )
                        
                        if request_obj and "body" in request_obj:
                            custom_id = request_obj.get("custom_id", f"row_{idx}")
                            
                            # ì¤‘ë³µ custom_id ì²´í¬
                            if custom_id in seen_custom_ids:
                                duplicate_count += 1
                                continue
                            seen_custom_ids.add(custom_id)
                            
                            # prompt_cache_key: í‚¤ ê³ ì • ì „ëµ (ëª¨ë“  ìš”ì²­ì´ ë™ì¼í•œ í‚¤ ì‚¬ìš©)
                            request_obj["body"]["prompt_cache_key"] = "bg_prompt_v1"
                            
                            # prompt_cache_retention: ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                            # Extended retention ì§€ì› ëª¨ë¸: gpt-5.1, gpt-5.1-codex, gpt-5.1-codex-mini, gpt-5.1-chat-latest, gpt-5, gpt-5-codex, gpt-4.1
                            # gpt-5-mini, gpt-5-nanoëŠ” prompt_cache_retention íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
                            if model_name in ["gpt-5.1", "gpt-5.1-codex", "gpt-5.1-codex-mini", "gpt-5.1-chat-latest", "gpt-5", "gpt-5-codex", "gpt-4.1"]:
                                request_obj["body"]["prompt_cache_retention"] = "extended"  # 24ì‹œê°„ retention
                            elif model_name not in ["gpt-5-mini", "gpt-5-nano"]:
                                # ê¸°íƒ€ ëª¨ë¸ì€ in-memory ì‚¬ìš© (5~10ë¶„ inactivity, ìµœëŒ€ 1ì‹œê°„)
                                request_obj["body"]["prompt_cache_retention"] = "in_memory"
                            
                            # text.format: JSON ì¶œë ¥ ê°•ì œ (Structured Outputs)
                            request_obj["body"]["text"] = {"format": {"type": "json_object"}}
                    else:
                        # ì¼ë°˜ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ ìœ ì§€
                        messages = build_bg_prompt_messages(st2_for_model, img_analysis_data)
                        
                        body = {
                            "model": model_name,
                            "messages": messages,
                        }
                        
                        is_reasoning = any(x in model_name for x in ["gpt-5", "o1", "o3"])
                        if is_reasoning and reasoning_effort != "none":
                            body["reasoning_effort"] = reasoning_effort

                        request_obj = {
                            "custom_id": f"row_{idx}",
                            "method": "POST",
                            "url": "/v1/chat/completions",
                            "body": body
                        }
                    
                    jsonl_lines.append(json.dumps(request_obj, ensure_ascii=False))
                except Exception as e:
                    self.append_log(f"[Row {idx+1}] ìŠ¤í‚µ: {e}")
                    skipped_cnt += 1
                    continue
            
            if duplicate_count > 0:
                self.append_log(f"[WARN] âš ï¸ ì¤‘ë³µ ìš”ì²­ {duplicate_count}ê°œê°€ ê°ì§€ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if not jsonl_lines:
                self.append_log("ìƒì„±í•  ìš”ì²­ ì—†ìŒ.")
                return

            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_bg_prompt_batch_input.jsonl"
            with open(jsonl_path, "w", encoding="utf-8") as f:
                f.write("\n".join(jsonl_lines))
            
            self.append_log(f"JSONL ìƒì„± ì™„ë£Œ: {len(jsonl_lines)}ê±´ (ìŠ¤í‚µ {skipped_cnt}ê±´)")
            
            # JSONL íŒŒì¼ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ì €ì¥ (í†µí•© ê¸°ëŠ¥ì—ì„œë„)
            self.after(0, lambda: self.jsonl_file_var.set(jsonl_path))
            
            # íŒŒì¼ í¬ê¸° ë° ìš”ì²­ ìˆ˜ í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            info = {
                'num_requests': len(jsonl_lines),
                'file_size_mb': jsonl_size_mb
            }
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ìš”ì²­ ìˆ˜: {info['num_requests']}ê°œ")
            
            # ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ : 180MB ì´ìƒì´ë©´ ë¶„í•  ì²˜ë¦¬ (OpenAI Batch API ì œí•œ: 200MB)
            # ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨ (500ê°œ ì œí•œ ì œê±°)
            MAX_FILE_SIZE_MB = 180
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸°ê°€ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤: {jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client_with_timeout = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch_ids = self._create_batch_chunks(
                    client=client_with_timeout,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    effort=reasoning_effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=999999,  # ìš”ì²­ ìˆ˜ ì œí•œ ê±°ì˜ ì œê±° (ìš©ëŸ‰ì´ ìš°ì„ )
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                self.after(0, lambda: messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}"))
            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    reasoning_effort=reasoning_effort,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src,
                    jsonl_path=jsonl_path,
                    model=model_name,
                    effort=reasoning_effort,
                    status=batch.status,
                    output_file_id=None,
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— I3-2 ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡ - img ìƒíƒœë§Œ ì—…ë°ì´íŠ¸ (text ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                try:
                    root_name = get_root_filename(src)
                    JobManager.update_status(root_name, img_s3_2_msg="I3-2 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> I3-2 (ì§„í–‰ì¤‘)")
                except Exception:
                    pass
                self.after(0, lambda bid=batch_id: messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {bid}"))
            
            self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            self.after(0, lambda err=str(e): messagebox.showerror("ì—ëŸ¬", err))
    
    def _create_batch_from_jsonl(self, client, jsonl_path, excel_path, model_name, reasoning_effort):
        """JSONL íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë°°ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        if not os.path.exists(jsonl_path):
            raise FileNotFoundError(f"ì…ë ¥ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")

        with open(jsonl_path, "rb") as f:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
            up_file = client.files.create(file=f, purpose="batch", timeout=600)

        # ìºì‹± ëª¨ë“œì— ë”°ë¼ endpoint ê²°ì •
        endpoint = "/v1/responses" if CACHE_MODE_CORE else "/v1/chat/completions"
        
        batch = client.batches.create(
            input_file_id=up_file.id,
            endpoint=endpoint,
            completion_window="24h"
        )
        return batch
    
    def _create_batch_chunks(self, client, jsonl_path, excel_path, model_name, effort, max_size_mb=180, max_requests=999999):
        """
        í° JSONL íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ batch_group_idë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        """
        import uuid
        
        # ë°°ì¹˜ ê·¸ë£¹ ID ìƒì„± (ê°™ì€ ì—‘ì…€ì—ì„œ ë¶„í• ëœ ë°°ì¹˜ë“¤ì„ ë¬¶ìŒ)
        batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
        
        # JSONL íŒŒì¼ ì½ê¸°
        requests = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    requests.append(json.loads(line))
        
        total_requests = len(requests)
        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (íŒŒì¼ í¬ê¸°ì™€ ìš”ì²­ ìˆ˜ ëª¨ë‘ ê³ ë ¤)
        original_file_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
        estimated_chunks_by_size = max(1, int(original_file_size_mb / max_size_mb) + 1)
        estimated_chunks_by_count = (total_requests + max_requests - 1) // max_requests
        estimated_total_chunks = max(estimated_chunks_by_size, estimated_chunks_by_count)
        
        self.append_log(f"[INFO] ì´ {total_requests}ê°œ ìš”ì²­ì„ ë¶„í• í•©ë‹ˆë‹¤... (ì˜ˆìƒ: ì•½ {estimated_total_chunks}ê°œ ì²­í¬, ê·¸ë£¹ ID: {batch_group_id})")
        
        # base ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜ (ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹œ ì‚¬ìš©)
        base, ext = os.path.splitext(jsonl_path)
        
        batch_ids = []
        chunk_num = 0
        chunk_files_created = []  # ìƒì„±ëœ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì •ë¦¬ìš©)
        failed_chunk_files = []  # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì¬ì‹œë„ìš©)
        
        i = 0
        while i < total_requests:
            chunk_num += 1
            chunk_requests = []
            chunk_size_bytes = 0  # ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì •í™•íˆ ê³„ì‚°
            
            # ì²­í¬ ìƒì„± (ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ , ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
            while i < total_requests:
                req_json = json.dumps(requests[i], ensure_ascii=False)
                req_size_bytes = len(req_json.encode('utf-8')) + 1  # +1 for newline
                
                # ìš©ëŸ‰ì´ ìš°ì„ : ë‹¤ìŒ ìš”ì²­ì„ ì¶”ê°€í•˜ë©´ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ ë§ˆì§„ 5% í¬í•¨)
                if chunk_size_bytes + req_size_bytes > max_size_mb * 1024 * 1024 * 0.95:
                    break
                
                chunk_requests.append(requests[i])
                chunk_size_bytes += req_size_bytes
                i += 1
            
            if not chunk_requests:
                break
            
            # ì²­í¬ JSONL íŒŒì¼ ìƒì„±
            chunk_jsonl_path = f"{base}_chunk{chunk_num:03d}{ext}"
            chunk_files_created.append(chunk_jsonl_path)
            
            with open(chunk_jsonl_path, "w", encoding="utf-8") as f:
                for req in chunk_requests:
                    f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            chunk_size_mb = os.path.getsize(chunk_jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] ì²­í¬ {chunk_num}: {len(chunk_requests)}ê°œ ìš”ì²­, {chunk_size_mb:.2f} MB")
            
            # ë°°ì¹˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 3
            retry_count = 0
            batch_created = False
            
            while retry_count < max_retries and not batch_created:
                try:
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹œë„ ì¤‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                    batch = self._create_batch_from_jsonl(
                        client=client,
                        jsonl_path=chunk_jsonl_path,
                        excel_path=excel_path,
                        model_name=model_name,
                        reasoning_effort=effort,
                    )
                    
                    batch_id = batch.id
                    batch_ids.append(batch_id)
                    self.append_log(f"âœ… ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_id}")
                    
                    batch_created = True
                    
                    # ì‘ì—… ì´ë ¥ ê¸°ë¡ (ê·¸ë£¹ ì •ë³´ í¬í•¨)
                    upsert_batch_job(
                        batch_id=batch_id,
                        src_excel=excel_path,
                        jsonl_path=chunk_jsonl_path,
                        model=model_name,
                        effort=effort,
                        status=batch.status,
                        output_file_id=None,
                        batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                        chunk_index=chunk_num,  # ì²­í¬ ë²ˆí˜¸
                        total_chunks=chunk_num,  # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì²­í¬ ìˆ˜ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨)
                    )
                except Exception as e:
                    error_str = str(e).lower()
                    is_token_limit_error = "enqueued token limit" in error_str or "token limit reached" in error_str
                    
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                        self.append_log(f"âš ï¸ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                        if is_token_limit_error:
                            self.append_log(f"[WARN] í† í° ì œí•œ ì˜¤ë¥˜ ê°ì§€. ë” ê¸´ ëŒ€ê¸° ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            wait_time = max(wait_time, 30)  # í† í° ì œí•œ ì˜¤ë¥˜ëŠ” ìµœì†Œ 30ì´ˆ ëŒ€ê¸°
                        self.append_log(f"[INFO] {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        import time
                        time.sleep(wait_time)
                    else:
                        self.append_log(f"âŒ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}")
                        if is_token_limit_error:
                            self.append_log(f"[INFO] í† í° ì œí•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì¼ë¶€ ë°°ì¹˜ê°€ ì™„ë£Œëœ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                            self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼: {chunk_jsonl_path}")
                            self.append_log(f"[INFO] ë‚˜ì¤‘ì— '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì´ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        import traceback
                        self.append_log(traceback.format_exc())
                        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìœ ì§€ (ìˆ˜ë™ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                        failed_chunk_files.append({
                            "chunk_num": chunk_num,
                            "chunk_file": chunk_jsonl_path,
                            "error": str(e),
                            "is_token_limit": is_token_limit_error,
                            "excel_path": excel_path,
                            "model_name": model_name,
                            "effort": effort,
                            "batch_group_id": batch_group_id,
                        })
                        # í•˜ì§€ë§Œ ë°°ì¹˜ IDëŠ” ì¶”ê°€ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ batch_idsì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ
        
        # ëª¨ë“  ì²­í¬ ìƒì„± ì™„ë£Œ í›„, total_chunksë¥¼ ì‹¤ì œ ìƒì„±ëœ ë°°ì¹˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        actual_total_chunks = len(batch_ids)
        if actual_total_chunks > 0:
            self.append_log(f"[INFO] ì´ {actual_total_chunks}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ. ì‘ì—… ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
            jobs = load_batch_jobs()
            updated_count = 0
            for j in jobs:
                if j.get("batch_group_id") == batch_group_id:
                    j["total_chunks"] = actual_total_chunks
                    updated_count += 1
            if updated_count > 0:
                save_batch_jobs(jobs)
                self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {actual_total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.append_log(f"âš ï¸ ìƒì„±ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì„±ê³µí•œ ë°°ì¹˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²­í¬ íŒŒì¼ ì •ë¦¬ ì˜µì…˜ ì œê³µ (í˜„ì¬ëŠ” ìœ ì§€)
        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìˆ˜ë™ ì¬ì‹œë„ë¥¼ ìœ„í•´ ìœ ì§€
        if actual_total_chunks < chunk_num:
            failed_chunks = chunk_num - actual_total_chunks
            self.append_log(f"âš ï¸ {failed_chunks}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²­í¬ íŒŒì¼ì€ ìœ ì§€ë©ë‹ˆë‹¤.")
            
            # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ì„ ëª…í™•íˆ í‘œì‹œ
            if failed_chunk_files:
                self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡:")
                for failed_info in failed_chunk_files:
                    self.append_log(f"  - ì²­í¬ {failed_info['chunk_num']}: {os.path.basename(failed_info['chunk_file'])}")
                    if failed_info['is_token_limit']:
                        self.append_log(f"    â†’ í† í° ì œí•œ ì˜¤ë¥˜. ì¼ë¶€ ë°°ì¹˜ ì™„ë£Œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                
                # ì‹¤íŒ¨ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                failed_info_path = f"{base}_failed_chunks.json"
                try:
                    with open(failed_info_path, "w", encoding="utf-8") as f:
                        json.dump(failed_chunk_files, f, ensure_ascii=False, indent=2)
                    self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ ì €ì¥: {os.path.basename(failed_info_path)}")
                    self.append_log(f"[INFO] ë‚˜ì¤‘ì— ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤íŒ¨í•œ ì²­í¬ë§Œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    # GUIì— ìë™ìœ¼ë¡œ ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ì•Œë¦¼
                    self.after(0, lambda: self._handle_failed_chunks(failed_info_path, failed_chunk_files))
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
        try:
            root_name = get_root_filename(excel_path)
            JobManager.update_status(root_name, img_s3_2_msg="I3-2 (ì§„í–‰ì¤‘)")
            self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> I3-2 (ì§„í–‰ì¤‘)")
        except Exception:
            pass
        
        return batch_ids

    # ----------------------------------------------------
    # Tab 2: Manage (List & Trash)
    # ----------------------------------------------------
    def _init_tab_manage(self):
        container = ttk.Frame(self.tab_manage, padding=10)
        container.pack(fill='both', expand=True)

        sub_tabs = ttk.Notebook(container)
        sub_tabs.pack(fill='both', expand=True)
        
        self.sub_active = ttk.Frame(sub_tabs, padding=10)
        self.sub_archive = ttk.Frame(sub_tabs, padding=10)
        
        sub_tabs.add(self.sub_active, text=" â–¶ ì§„í–‰ì¤‘ / ì™„ë£Œ (Active) ")
        sub_tabs.add(self.sub_archive, text=" ğŸ—‘ íœ´ì§€í†µ (Archive) ")
        
        # --- Active Tab UI ---
        f_ctrl = ttk.Frame(self.sub_active)
        f_ctrl.pack(fill='x', pady=(0, 10))
        
        ttk.Button(f_ctrl, text="ğŸ”„ ì„ íƒ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ“¥ ì„ íƒ ì¼ê´„ ë³‘í•©", command=self._merge_selected, style="Primary.TButton").pack(side='left', padx=10)
        ttk.Button(f_ctrl, text="ğŸ—‘ íœ´ì§€í†µ ì´ë™", command=self._archive_selected, style="Danger.TButton").pack(side='right', padx=2)
        
        # ì»¬ëŸ¼ ì •ì˜: batch_id | excel_name | memo | status | created | completed | model | effort | counts | group
        cols = ("batch_id", "excel_name", "memo", "status", "created", "completed", "model", "effort", "counts", "group")
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš© (íŠ¸ë¦¬ ì•„ì´ì½˜ + ì»¬ëŸ¼ í—¤ë”)
        self.tree_active = ttk.Treeview(self.sub_active, columns=cols, show='tree headings', height=12, selectmode='extended')
        
        self.tree_active.tag_configure('odd', background=COLOR_WHITE)
        self.tree_active.tag_configure('even', background='#F2F7FF')
        self.tree_active.tag_configure('group', background='#E8F4FD')
        self.tree_active.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_active.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_active.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_active.heading("memo", text="ë©”ëª¨")
        self.tree_active.heading("status", text="ìƒíƒœ")
        self.tree_active.heading("created", text="ìƒì„±ì¼ì‹œ")
        self.tree_active.heading("completed", text="ì™„ë£Œì¼ì‹œ")
        self.tree_active.heading("model", text="ëª¨ë¸")
        self.tree_active.heading("effort", text="Effort")
        self.tree_active.heading("counts", text="ì™„ë£Œ/ì „ì²´")
        self.tree_active.heading("group", text="ê·¸ë£¹")
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree_active.column("batch_id", width=180, anchor="w")
        self.tree_active.column("excel_name", width=200, anchor="w")
        self.tree_active.column("memo", width=150, anchor="w")
        self.tree_active.column("status", width=80, anchor="center")
        self.tree_active.column("created", width=120, anchor="center")
        self.tree_active.column("completed", width=120, anchor="center")
        self.tree_active.column("model", width=80, anchor="center")
        self.tree_active.column("effort", width=60, anchor="center")
        self.tree_active.column("counts", width=80, anchor="center")
        self.tree_active.column("group", width=80, anchor="center")
        
        self.tree_active.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ë²„íŠ¼
        f_group_ctrl = ttk.Frame(self.sub_active)
        f_group_ctrl.pack(fill='x', padx=5, pady=(0, 5))
        ttk.Button(f_group_ctrl, text="ğŸ“‚ ëª¨ë“  ê·¸ë£¹ í¼ì¹˜ê¸°", command=lambda: self._expand_all_groups(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_group_ctrl, text="ğŸ“ ëª¨ë“  ê·¸ë£¹ ì ‘ê¸°", command=lambda: self._collapse_all_groups(self.tree_active)).pack(side='left', padx=2)
        
        # ìš°í´ë¦­ ë©”ë‰´
        self.menu_active = Menu(self, tearoff=0)
        self.menu_active.add_command(label="ìƒíƒœ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ê²°ê³¼ ë³‘í•©", command=self._merge_selected)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="íœ´ì§€í†µìœ¼ë¡œ ì´ë™", command=self._archive_selected)
        
        self.tree_active.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_active, self.menu_active))
        self.tree_active.bind("<Double-1>", self._on_tree_double_click)

        # --- Archive Tab UI ---
        f_arch_ctrl = ttk.Frame(self.sub_archive)
        f_arch_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_arch_ctrl, text="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected, style="Primary.TButton").pack(side='left')
        ttk.Button(f_arch_ctrl, text="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected, style="Danger.TButton").pack(side='right')
        
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš©
        self.tree_arch = ttk.Treeview(self.sub_archive, columns=cols, show='tree headings', height=12, selectmode='extended')
        self.tree_arch.tag_configure('odd', background=COLOR_WHITE)
        self.tree_arch.tag_configure('even', background='#FFF2F2')
        self.tree_arch.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_arch.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_arch.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_arch.heading("memo", text="ë©”ëª¨")
        self.tree_arch.heading("status", text="ìƒíƒœ")
        self.tree_arch.heading("created", text="ìƒì„±ì¼ì‹œ")
        self.tree_arch.heading("completed", text="ì™„ë£Œì¼ì‹œ")
        self.tree_arch.heading("model", text="ëª¨ë¸")
        self.tree_arch.heading("effort", text="Effort")
        self.tree_arch.heading("counts", text="ì™„ë£Œ/ì „ì²´")
        self.tree_arch.heading("group", text="ê·¸ë£¹")
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree_arch.column("batch_id", width=180, anchor="w")
        self.tree_arch.column("excel_name", width=200, anchor="w")
        self.tree_arch.column("memo", width=150, anchor="w")
        self.tree_arch.column("status", width=80, anchor="center")
        self.tree_arch.column("created", width=120, anchor="center")
        self.tree_arch.column("completed", width=120, anchor="center")
        self.tree_arch.column("model", width=80, anchor="center")
        self.tree_arch.column("effort", width=60, anchor="center")
        self.tree_arch.column("counts", width=80, anchor="center")
        self.tree_arch.column("group", width=80, anchor="center")
        
        self.tree_arch.pack(fill='both', expand=True)
        
        # Archive ìš°í´ë¦­ ë©”ë‰´
        self.menu_arch = Menu(self, tearoff=0)
        self.menu_arch.add_command(label="ë³µêµ¬", command=self._restore_selected)
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_arch))
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected)
        self.tree_arch.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_arch, self.menu_arch))
        
        self._load_jobs_all()
        self._load_archive_list()

    def _show_context_menu(self, event, tree, menu):
        item = tree.identify_row(event.y)
        if item:
            if item not in tree.selection():
                tree.selection_set(item)
            menu.post(event.x_root, event.y_root)

    def _expand_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ í¼ì¹©ë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=True)
    
    def _collapse_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ ì ‘ìŠµë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=False)
    
    def _get_selected_ids(self, tree):
        """
        ì„ íƒëœ í•­ëª©ì—ì„œ ë°°ì¹˜ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê·¸ë£¹ í—¤ë”ë¥¼ ì„ íƒí•˜ë©´ ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        selection = tree.selection()
        ids = []
        
        for item in selection:
            vals = tree.item(item)['values']
            batch_id = vals[0] if vals else ""
            
            # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
            if not batch_id:
                # ê·¸ë£¹ í—¤ë”ì˜ ìì‹ ë…¸ë“œë“¤(ë°°ì¹˜ë“¤)ì„ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
                children = tree.get_children(item)
                for child in children:
                    child_vals = tree.item(child)['values']
                    if child_vals and child_vals[0]:
                        ids.append(child_vals[0])
            else:
                # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°
                ids.append(batch_id)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(ids))
    
    def _edit_memo(self, tree):
        """ì„ íƒëœ ë°°ì¹˜ì˜ ë©”ëª¨ë¥¼ í¸ì§‘í•©ë‹ˆë‹¤."""
        selection = tree.selection()
        if not selection:
            messagebox.showinfo("ì•ˆë‚´", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        item = selection[0]
        vals = tree.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        if not batch_id:
            messagebox.showinfo("ì•ˆë‚´", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # í˜„ì¬ ë©”ëª¨ ê°€ì ¸ì˜¤ê¸°
        jobs = load_batch_jobs()
        current_memo = ""
        for j in jobs:
            if j["batch_id"] == batch_id:
                current_memo = j.get("memo", "") or ""
                break
        
        # ë©”ëª¨ í¸ì§‘ ë‹¤ì´ì–¼ë¡œê·¸
        dialog = tk.Toplevel(self)
        dialog.title("ë©”ëª¨ í¸ì§‘")
        dialog.geometry("500x200")
        dialog.transient(self)
        dialog.grab_set()
        
        # ë°°ì¹˜ ID í‘œì‹œ
        tk.Label(dialog, text=f"ë°°ì¹˜ ID: {batch_id[:30]}...", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(10, 5))
        
        # ë©”ëª¨ ì…ë ¥ í•„ë“œ
        tk.Label(dialog, text="ë©”ëª¨:", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(5, 0))
        memo_entry = tk.Text(dialog, height=5, width=60, font=("ë§‘ì€ ê³ ë”•", 9))
        memo_entry.pack(fill="both", expand=True, padx=10, pady=5)
        memo_entry.insert("1.0", current_memo)
        memo_entry.focus()
        
        # ë²„íŠ¼
        btn_frame = tk.Frame(dialog)
        btn_frame.pack(fill="x", padx=10, pady=10)
        
        def save_memo():
            new_memo = memo_entry.get("1.0", "end-1c").strip()
            upsert_batch_job(batch_id, memo=new_memo)
            self.append_log(f"[INFO] ë°°ì¹˜ {batch_id[:20]}... ë©”ëª¨ ì—…ë°ì´íŠ¸: {new_memo[:50]}...")
            self._load_jobs_all()
            self._load_archive_list()
            dialog.destroy()
            messagebox.showinfo("ì™„ë£Œ", "ë©”ëª¨ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        tk.Button(btn_frame, text="ì €ì¥", command=save_memo, bg="#4CAF50", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)
        tk.Button(btn_frame, text="ì·¨ì†Œ", command=dialog.destroy, bg="#f44336", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)

    def _load_jobs_all(self):
        if not hasattr(self, 'tree_active'): return
        for i in self.tree_active.get_children(): self.tree_active.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ì„ ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0]
            created_at = first_job.get("created_at", "")
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = dt.strftime("%m-%d %H:%M")
                except:
                    date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
            else:
                date_str = "-"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
            group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
            excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
            memo = first_job.get("memo", "") or "-"
            group_node = self.tree_active.insert("", "end", 
                text=group_header_text,
                values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                memo = j.get("memo", "") or "-"
                tag = 'group'
                self.tree_active.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, 
                        j.get("model"), j.get("effort", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
                idx += 1
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_active.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), 
                    c_at, f_at, 
                    j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))
            idx += 1

    def _load_archive_list(self):
        if not hasattr(self, 'tree_arch'): return
        for i in self.tree_arch.get_children(): self.tree_arch.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if not j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs)) if group_jobs else len(group_jobs)
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0] if group_jobs else None
            if first_job:
                created_at = first_job.get("created_at", "")
                if created_at:
                    try:
                        dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                        date_str = dt.strftime("%m-%d %H:%M")
                    except:
                        date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
                else:
                    date_str = "-"
                
                # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
                group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
                excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
                memo = first_job.get("memo", "") or "-"
                group_node = self.tree_arch.insert("", "end", 
                    text=group_header_text,
                    values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                    tags=('group_header',),
                    open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
                )
                
                # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
                for j in group_jobs:
                    cnt = "-"
                    if "request_counts" in j and j["request_counts"]:
                        rc = j["request_counts"]
                        cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                    c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                    f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                    chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                    group_display = f"ì²­í¬ {chunk_info}"
                    excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                    memo = j.get("memo", "") or "-"
                    tag = 'group'
                    self.tree_arch.insert(group_node, "end", 
                        text=f"  â””â”€ {j['batch_id'][:20]}...",
                        values=(
                            j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, 
                            j.get("model"), j.get("effort", "-"), cnt, group_display
                        ), 
                        tags=(tag,))
                    idx += 1
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_arch.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), 
                    c_at, f_at, 
                    j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))
            idx += 1

    # --- Batch Actions ---
    def _refresh_selected(self, tree):
        ids = self._get_selected_ids(tree)
        if not ids: return
        
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        jobs = load_batch_jobs()
        completed = [bid for bid in ids if next((x for x in jobs if x["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        
        if completed:
            if messagebox.askyesno("í™•ì¸", f"{len(completed)}ê±´ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\nì œì™¸í•˜ê³  ë¯¸ì™„ë£Œ ê±´ë§Œ ê°±ì‹ í• ê¹Œìš”?"):
                ids = [i for i in ids if i not in completed]
        
        if not ids:
            messagebox.showinfo("ì·¨ì†Œ", "ê°±ì‹ í•  ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        t = threading.Thread(target=self._run_refresh_ids, args=(ids,))
        t.daemon = True
        t.start()

    def _run_refresh_ids(self, ids):
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return
        
        self.append_log(f"ì„ íƒëœ {len(ids)}ê±´ ê°±ì‹  ì¤‘...")
        success_cnt = 0
        fail_cnt = 0
        
        for bid in ids:
            try:
                remote = client.batches.retrieve(bid)
                rc = None
                if remote.request_counts:
                    rc = {"total": remote.request_counts.total, "completed": remote.request_counts.completed, "failed": remote.request_counts.failed}
                
                # expired ìƒíƒœë„ ê°±ì‹  ê°€ëŠ¥ (output_file_id í™•ì¸ì„ ìœ„í•´)
                # output_file_id ì¶”ì¶œ: ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
                output_file_id = getattr(remote, "output_file_id", None)
                if not output_file_id:
                    # output_file ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                    output_file = getattr(remote, "output_file", None)
                    if output_file:
                        if isinstance(output_file, str):
                            output_file_id = output_file
                        else:
                            output_file_id = getattr(output_file, "id", None) or getattr(output_file, "file_id", None)
                
                # model_dump()ë¥¼ í†µí•œ ì¶”ê°€ í™•ì¸ (ê°±ì‹  ì‹œì—ë„ ì ìš©)
                if not output_file_id and remote.status == "completed":
                    try:
                        if hasattr(remote, "model_dump"):
                            dump = remote.model_dump()
                            if "output_file_id" in dump and dump["output_file_id"]:
                                output_file_id = dump["output_file_id"]
                            elif "output_file" in dump:
                                of = dump["output_file"]
                                if isinstance(of, str) and of:
                                    output_file_id = of
                                elif isinstance(of, dict) and "id" in of:
                                    output_file_id = of["id"]
                    except Exception:
                        pass
                
                upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id, request_counts=rc)
                
                if remote.status == "expired" and output_file_id:
                    self.append_log(f"â„¹ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ì§€ë§Œ output_file_idê°€ ìˆìŠµë‹ˆë‹¤. (ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")
                elif remote.status == "completed":
                    if output_file_id:
                        self.append_log(f"âœ… {bid}: {remote.status} (output_file_id: {output_file_id})")
                    else:
                        self.append_log(f"âš ï¸ {bid}: {remote.status} (output_file_id ì—†ìŒ - ë””ë²„ê¹… í•„ìš”)")
                else:
                    self.append_log(f"âœ… {bid}: {remote.status}")
                success_cnt += 1
            except Exception as e:
                self.append_log(f"âŒ {bid} ê°±ì‹  ì‹¤íŒ¨: {e}")
                fail_cnt += 1
        
        self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])
        if fail_cnt > 0:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}, ì‹¤íŒ¨: {fail_cnt})")
            self.after(0, lambda: messagebox.showinfo("ê°±ì‹  ì™„ë£Œ", f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}ê±´, ì‹¤íŒ¨: {fail_cnt}ê±´)\n\n[2. ë°°ì¹˜ ê´€ë¦¬] íƒ­ì—ì„œ 'ì„ íƒ ì¼ê´„ ë³‘í•©'ì„ ì§„í–‰í•˜ì„¸ìš”."))
        else:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}ê±´)")
            self.after(0, lambda: messagebox.showinfo("ê°±ì‹  ì™„ë£Œ", f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}ê±´)\n\n[2. ë°°ì¹˜ ê´€ë¦¬] íƒ­ì—ì„œ 'ì„ íƒ ì¼ê´„ ë³‘í•©'ì„ ì§„í–‰í•˜ì„¸ìš”."))

    def _merge_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        jobs = load_batch_jobs()
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        group_ids = set()
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if job:
                group_id = job.get("batch_group_id")
                if group_id:
                    group_ids.add(group_id)
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        all_target_ids = set(ids)
        for group_id in group_ids:
            if group_id:
                # completed ë˜ëŠ” expired ìƒíƒœì¸ ë°°ì¹˜ í¬í•¨ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
                group_batches = [j for j in jobs if j.get("batch_group_id") == group_id and j.get("status") in ["completed", "expired"]]
                for j in group_batches:
                    all_target_ids.add(j["batch_id"])
        
        if len(all_target_ids) > len(ids):
            group_info = f"\n\nê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ {len(all_target_ids) - len(ids)}ê°œê°€ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤."
        else:
            group_info = ""
        
        # completed, expired ë˜ëŠ” merged ìƒíƒœì¸ ë°°ì¹˜ ëª¨ë‘ ì„ íƒ ê°€ëŠ¥ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
        targets = [bid for bid in all_target_ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") in ["completed", "expired", "merged"]]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ë³‘í•©í•  ìˆ˜ ìˆëŠ” 'completed', 'expired' ë˜ëŠ” 'merged' ìƒíƒœì˜ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        msg = f"ì„ íƒí•œ {len(targets)}ê±´ì„ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ?{group_info}"
        if messagebox.askyesno("ë³‘í•©", msg):
            t = threading.Thread(target=self._run_merge_multi, args=(targets,))
            t.daemon = True
            t.start()

    def _run_merge_multi(self, ids):
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return
        
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ë¶„ë¥˜
        groups_to_merge = {}  # {group_id: [batch_ids]}
        ungrouped_batches = []  # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤
        
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not job:
                continue
            
            group_id = job.get("batch_group_id")
            if group_id:
                if group_id not in groups_to_merge:
                    groups_to_merge[group_id] = []
                groups_to_merge[group_id].append(bid)
            else:
                ungrouped_batches.append(bid)
        
        success_cnt = 0
        total_cost = 0.0
        success_folders = set()  # ì„±ê³µí•œ íŒŒì¼ë“¤ì´ ì €ì¥ëœ í´ë”ë“¤ ì¶”ì 
        
        # ê·¸ë£¹ë³„ ë³‘í•© ì²˜ë¦¬
        for group_id, batch_ids in groups_to_merge.items():
            self.append_log(f"--- ê·¸ë£¹ ë³‘í•© ì‹œì‘: {group_id} ({len(batch_ids)}ê°œ ë°°ì¹˜) ---")
            try:
                # ê·¸ë£¹ ë‚´ ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì›ë³¸ ì—‘ì…€ ê²½ë¡œ ì‚¬ìš©
                first_job = next((j for j in jobs if j["batch_id"] == batch_ids[0]), None)
                if not first_job:
                    continue
                
                src_path = first_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ì„ì‹œ JSONLì— ìˆ˜ì§‘
                all_output_lines = []
                model_name = first_job.get("model", "gpt-5-mini")
                total_group_cost = 0.0
                
                # ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ (chunk_indexê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ)
                def get_chunk_index(bid):
                    job = next((j for j in jobs if j["batch_id"] == bid), None)
                    if job:
                        idx = job.get("chunk_index")
                        return idx if idx is not None else 999999
                    return 999999
                
                batch_ids_sorted = sorted(batch_ids, key=get_chunk_index)
                
                for bid in batch_ids_sorted:
                    self.append_log(f"  [ê·¸ë£¹] ë°°ì¹˜ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    try:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if not local_job:
                            continue
                        
                        # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                        if local_job.get("status") == "merged":
                            self.append_log(f"  â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        # Batch ìƒíƒœ í™•ì¸
                        remote = client.batches.retrieve(bid)
                        # output_file_id ì¶”ì¶œ: ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
                        output_file_id = getattr(remote, "output_file_id", None)
                        if not output_file_id:
                            # output_file ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                            output_file = getattr(remote, "output_file", None)
                            if output_file:
                                if isinstance(output_file, str):
                                    output_file_id = output_file
                                else:
                                    output_file_id = getattr(output_file, "id", None) or getattr(output_file, "file_id", None)
                        
                        # model_dump()ë¥¼ í†µí•œ ì¶”ê°€ í™•ì¸
                        if not output_file_id and remote.status == "completed":
                            try:
                                if hasattr(remote, "model_dump"):
                                    dump = remote.model_dump()
                                    if "output_file_id" in dump and dump["output_file_id"]:
                                        output_file_id = dump["output_file_id"]
                                    elif "output_file" in dump:
                                        of = dump["output_file"]
                                        if isinstance(of, str) and of:
                                            output_file_id = of
                                        elif isinstance(of, dict) and "id" in of:
                                            output_file_id = of["id"]
                            except Exception:
                                pass
                        
                        # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                        if remote.status == "completed":
                            if not output_file_id:
                                self.append_log(f"  âš ï¸ {bid}: ì™„ë£Œ ìƒíƒœì§€ë§Œ output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                                upsert_batch_job(bid, status=remote.status, output_file_id=None)
                                continue
                        elif remote.status == "expired":
                            if not output_file_id:
                                self.append_log(f"  âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                                upsert_batch_job(bid, status=remote.status, output_file_id=None)
                                continue
                            else:
                                self.append_log(f"  â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                        else:
                            self.append_log(f"  âš ï¸ {bid}: ì•„ì§ completed ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                            upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id)
                            continue
                        
                        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                        base_dir = os.path.dirname(src_path)
                        base_name, _ = os.path.splitext(os.path.basename(src_path))
                        out_jsonl = os.path.join(base_dir, f"{base_name}_bg_prompt_batch_output_{bid}.jsonl")
                        
                        try:
                            content = client.files.content(output_file_id).content
                        except AttributeError:
                            file_content = client.files.content(output_file_id)
                            if hasattr(file_content, "read"):
                                content = file_content.read()
                            elif hasattr(file_content, "iter_bytes"):
                                chunks = []
                                for ch in file_content.iter_bytes():
                                    chunks.append(ch)
                                content = b"".join(chunks)
                            else:
                                content = file_content
                        
                        with open(out_jsonl, "wb") as f:
                            f.write(content)
                        
                        upsert_batch_job(bid, status="completed", output_file_id=output_file_id, output_jsonl=out_jsonl)
                        
                        # JSONL íŒŒì¼ ì½ì–´ì„œ ìˆ˜ì§‘ ë° ìºì‹± í†µê³„ ìˆ˜ì§‘
                        batch_cached_tok = 0
                        batch_total_requests = 0
                        batch_cache_hits = 0
                        if os.path.exists(out_jsonl):
                            with open(out_jsonl, "r", encoding="utf-8") as f:
                                for line in f:
                                    line = line.strip()
                                    if line:
                                        all_output_lines.append(line)
                                        # ìºì‹± í†µê³„ ìˆ˜ì§‘
                                        try:
                                            data = json.loads(line)
                                            response_body = data.get("response", {}).get("body", {})
                                            usage = response_body.get("usage", {})
                                            input_tokens_details = usage.get("input_tokens_details", {})
                                            cached_tokens = input_tokens_details.get("cached_tokens", 0)
                                            batch_cached_tok += cached_tokens
                                            batch_total_requests += 1
                                            if cached_tokens > 0:
                                                batch_cache_hits += 1
                                        except:
                                            pass
                        
                        # ë°°ì¹˜ë³„ ìºì‹± í†µê³„ ì¶œë ¥
                        if batch_total_requests > 0:
                            cache_hit_rate = (batch_cache_hits / batch_total_requests * 100)
                            cache_savings_pct = (batch_cached_tok / (batch_cached_tok + (batch_total_requests * 1000))) * 100 if batch_cached_tok > 0 else 0  # ëŒ€ëµì  ê³„ì‚°
                            self.append_log(f"  [ìºì‹±] {bid}: ìš”ì²­ {batch_total_requests}ê±´, íˆíŠ¸ {batch_cache_hits}ê±´ ({cache_hit_rate:.1f}%), ìºì‹œ í† í° {batch_cached_tok:,}")
                        
                    except Exception as e:
                        self.append_log(f"  âŒ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if not all_output_lines:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ë³‘í•©í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ì„ì‹œ í†µí•© JSONL íŒŒì¼ ìƒì„±
                base_dir = os.path.dirname(src_path)
                base_name, _ = os.path.splitext(os.path.basename(src_path))
                merged_jsonl = os.path.join(base_dir, f"{base_name}_bg_prompt_batch_output_merged_{group_id}.jsonl")
                
                with open(merged_jsonl, "w", encoding="utf-8") as f:
                    for line in all_output_lines:
                        f.write(line + "\n")
                
                self.append_log(f"  [ê·¸ë£¹] í†µí•© JSONL ìƒì„±: {len(all_output_lines)}ê°œ ê²°ê³¼")
                
                # í†µí•© JSONLì„ ì—‘ì…€ì— ë³‘í•©
                results_map = {}
                total_group_in = 0
                total_group_out = 0
                total_group_cached = 0
                total_group_requests = 0
                total_group_cache_hits = 0
                
                with open(merged_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        
                        # /v1/responses API í˜•ì‹ ì²˜ë¦¬
                        response_body = data.get("response", {}).get("body", {})
                        usage = response_body.get("usage", {})
                        input_tokens = usage.get("input_tokens", 0) or usage.get("prompt_tokens", 0)  # í˜¸í™˜ì„±
                        output_tokens = usage.get("output_tokens", 0) or usage.get("completion_tokens", 0)  # í˜¸í™˜ì„±
                        total_group_in += input_tokens
                        total_group_out += output_tokens
                        
                        # ìºì‹± í†µê³„ ìˆ˜ì§‘
                        input_tokens_details = usage.get("input_tokens_details", {})
                        cached_tokens = input_tokens_details.get("cached_tokens", 0)
                        total_group_cached += cached_tokens
                        total_group_requests += 1
                        if cached_tokens > 0:
                            total_group_cache_hits += 1
                        
                        try:
                            # /v1/responses API: output ë°°ì—´ì—ì„œ message íƒ€ì… ì°¾ê¸°
                            output_array = response_body.get("output", [])
                            content_str = None
                            
                            for output_item in output_array:
                                if output_item.get("type") == "message":
                                    content_array = output_item.get("content", [])
                                    for content_item in content_array:
                                        if content_item.get("type") == "output_text":
                                            content_str = content_item.get("text", "").strip()
                                            break
                                    if content_str:
                                        break
                            
                            # Fallback: ê¸°ì¡´ choices í˜•ì‹ (í˜¸í™˜ì„±)
                            if not content_str:
                                content_str = data.get("response", {}).get("body", {}).get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                            
                            if content_str:
                                result_data = json.loads(content_str)
                                results_map[cid] = result_data
                            else:
                                results_map[cid] = {}
                        except Exception as e:
                            results_map[cid] = {}
                            self.append_log(f"  [WARN] {cid} íŒŒì‹± ì‹¤íŒ¨: {e}")
                
                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    result_cols = ["bg_positive_en", "bg_negative_en", "video_motion_prompt_en", "video_full_prompt_en"]
                    for col in result_cols:
                        if col not in df.columns:
                            df[col] = ""
                        df[col] = df[col].astype(str)
                    
                    cnt = 0
                    for cid, result_data in results_map.items():
                        try:
                            idx = int(cid.split("_")[1])
                            if 0 <= idx < len(df):
                                for col in result_cols:
                                    if col in result_data:
                                        df.at[idx, col] = str(result_data[col])
                                cnt += 1
                        except:
                            pass

                    # ì¤‘ê°„ íŒŒì¼ ì €ì¥
                    base, ext = os.path.splitext(src_path)
                    out_excel = f"{base}_bg_prompt_batch_done{ext}"
                    if not safe_save_excel(df, out_excel):
                        self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")
                        continue

                    # I4 ë²„ì „ íŒŒì¼ë¡œ ì €ì¥
                    try:
                        final_out_path = get_i4_output_path(src_path)
                        df_done = pd.read_excel(out_excel)
                        if safe_save_excel(df_done, final_out_path):
                            # ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
                            if out_excel != final_out_path and os.path.exists(out_excel):
                                try:
                                    os.remove(out_excel)
                                    self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                                except Exception as e:
                                    self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                        else:
                            final_out_path = out_excel
                    except Exception as e:
                        self.append_log(f"[WARN] I4 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                        final_out_path = out_excel
                    
                    # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                    cost_in = (total_group_in / 1_000_000) * pricing["input"] * 0.5
                    cost_out = (total_group_out / 1_000_000) * pricing["output"] * 0.5
                    cost_total = cost_in + cost_out
                    total_group_cost = cost_total
                    total_cost += total_group_cost
                    
                    # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ë¥¼ merged ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                    for bid in batch_ids:
                        upsert_batch_job(
                            batch_id=bid,
                            out_excel=final_out_path,
                            status="merged",
                        )
                    
                    # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
                    try:
                        root_name = get_root_filename(src_path)
                        JobManager.update_status(root_name, img_s3_2_msg="I3-2(ë°°ê²½ë¶„ì„ì™„ë£Œ)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> I3-2(ë°°ê²½ë¶„ì„ì™„ë£Œ)")
                    except Exception as e:
                        self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")
                    
                    self.append_log(f"âœ… ê·¸ë£¹ {group_id} ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(final_out_path)}")
                    self.append_log(f"  [ê·¸ë£¹ ìºì‹± í†µê³„] ìš”ì²­ {total_group_requests:,}ê±´, íˆíŠ¸ {total_group_cache_hits:,}ê±´ ({group_cache_hit_rate:.1f}%), ìºì‹œ í† í° {total_group_cached:,} ({group_cache_savings_pct:.1f}%)")
                    if group_cache_savings > 0:
                        self.append_log(f"  [ê·¸ë£¹ ë¹„ìš©ì ˆê°] ìºì‹±ìœ¼ë¡œ ì´ ${group_cache_savings:.4f} ì ˆê°")
                    success_cnt += 1
                    success_folders.add(os.path.dirname(final_out_path))
                else:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                self.append_log(f"âŒ ê·¸ë£¹ {group_id} ë³‘í•© ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        for bid in ungrouped_batches:
            self.append_log(f"--- ë³‘í•© ì‹œì‘: {bid} ---")
            try:
                local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                
                if not local_job:
                    self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì‘ì—… ì´ë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                is_already_merged = local_job.get("status") == "merged"
                if is_already_merged:
                    self.append_log(f"â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤.")
                    continue

                # Batch ìƒíƒœ ë° ê²°ê³¼ íŒŒì¼ ID ì¡°íšŒ
                remote = client.batches.retrieve(bid)
                # output_file_id ì¶”ì¶œ: ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
                output_file_id = getattr(remote, "output_file_id", None)
                if not output_file_id:
                    # output_file ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                    output_file = getattr(remote, "output_file", None)
                    if output_file:
                        if isinstance(output_file, str):
                            output_file_id = output_file
                        else:
                            output_file_id = getattr(output_file, "id", None) or getattr(output_file, "file_id", None)
                
                # model_dump()ë¥¼ í†µí•œ ì¶”ê°€ í™•ì¸
                if not output_file_id and remote.status == "completed":
                    try:
                        if hasattr(remote, "model_dump"):
                            dump = remote.model_dump()
                            if "output_file_id" in dump and dump["output_file_id"]:
                                output_file_id = dump["output_file_id"]
                            elif "output_file" in dump:
                                of = dump["output_file"]
                                if isinstance(of, str) and of:
                                    output_file_id = of
                                elif isinstance(of, dict) and "id" in of:
                                    output_file_id = of["id"]
                    except Exception:
                        pass
                
                # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                if remote.status == "completed":
                    if not output_file_id:
                        self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì™„ë£Œ ìƒíƒœì§€ë§Œ output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        upsert_batch_job(bid, status=remote.status, output_file_id=None)
                        continue
                elif remote.status == "expired":
                    if not output_file_id:
                        self.append_log(f"âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                        upsert_batch_job(bid, status=remote.status, output_file_id=None)
                        continue
                    else:
                        self.append_log(f"â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                else:
                    self.append_log(f"âš ï¸ {bid}: ì•„ì§ completed ë˜ëŠ” expired ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                    upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id)
                    continue
                
                # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                try:
                    content = client.files.content(output_file_id).content
                except AttributeError:
                    file_content = client.files.content(output_file_id)
                    if hasattr(file_content, "read"):
                        content = file_content.read()
                    elif hasattr(file_content, "iter_bytes"):
                        chunks = []
                        for ch in file_content.iter_bytes():
                            chunks.append(ch)
                        content = b"".join(chunks)
                    else:
                        content = file_content
                
                if local_job and local_job.get("src_excel"):
                    src_path = local_job["src_excel"]
                    base, _ = os.path.splitext(src_path)
                    out_jsonl = f"{base}_bg_prompt_batch_output.jsonl"
                else:
                    out_jsonl = f"output_{bid}.jsonl"
                    src_path = None

                with open(out_jsonl, "wb") as f:
                    f.write(content)
                
                # JSONL íŒŒì‹± ë° ì—‘ì…€ ë³‘í•©
                results_map = {}
                batch_in_tok = 0
                batch_out_tok = 0
                batch_cached_tok = 0
                batch_total_requests = 0
                batch_cache_hits = 0
                
                with open(out_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        
                        # /v1/responses API í˜•ì‹ ì²˜ë¦¬
                        response_body = data.get("response", {}).get("body", {})
                        usage = response_body.get("usage", {})
                        input_tokens = usage.get("input_tokens", 0) or usage.get("prompt_tokens", 0)  # í˜¸í™˜ì„±
                        output_tokens = usage.get("output_tokens", 0) or usage.get("completion_tokens", 0)  # í˜¸í™˜ì„±
                        batch_in_tok += input_tokens
                        batch_out_tok += output_tokens
                        
                        # ìºì‹± í†µê³„ ìˆ˜ì§‘
                        input_tokens_details = usage.get("input_tokens_details", {})
                        cached_tokens = input_tokens_details.get("cached_tokens", 0)
                        batch_cached_tok += cached_tokens
                        batch_total_requests += 1
                        if cached_tokens > 0:
                            batch_cache_hits += 1
                        
                        try:
                            # /v1/responses API: output ë°°ì—´ì—ì„œ message íƒ€ì… ì°¾ê¸°
                            output_array = response_body.get("output", [])
                            content_str = None
                            
                            for output_item in output_array:
                                if output_item.get("type") == "message":
                                    content_array = output_item.get("content", [])
                                    for content_item in content_array:
                                        if content_item.get("type") == "output_text":
                                            content_str = content_item.get("text", "").strip()
                                            break
                                    if content_str:
                                        break
                            
                            # Fallback: ê¸°ì¡´ choices í˜•ì‹ (í˜¸í™˜ì„±)
                            if not content_str:
                                content_str = data.get("response", {}).get("body", {}).get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                            
                            if content_str:
                                result_data = json.loads(content_str)
                                results_map[cid] = result_data
                            else:
                                results_map[cid] = {}
                        except Exception as e:
                            results_map[cid] = {}
                            self.append_log(f"  [WARN] {cid} íŒŒì‹± ì‹¤íŒ¨: {e}")

                # ìºì‹± í†µê³„ ì¶œë ¥
                cache_hit_rate = (batch_cache_hits / batch_total_requests * 100) if batch_total_requests > 0 else 0
                cache_savings_pct = (batch_cached_tok / batch_in_tok * 100) if batch_in_tok > 0 else 0
                self.append_log(f"  [ìºì‹±] {bid}: ìš”ì²­ {batch_total_requests}ê±´, íˆíŠ¸ {batch_cache_hits}ê±´ ({cache_hit_rate:.1f}%), ìºì‹œ í† í° {batch_cached_tok:,} ({cache_savings_pct:.1f}%)")
                
                # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                model_name = local_job.get("model", "gpt-5-mini") if local_job else "gpt-5-mini"
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                cost_total = cost_in + cost_out
                total_cost += cost_total
                
                # ìºì‹œë¡œ ì ˆê°ëœ ë¹„ìš© ê³„ì‚° (ìºì‹œëœ í† í°ì€ ë¹„ìš©ì´ 0)
                cache_savings = (batch_cached_tok / 1_000_000) * pricing["input"] * 0.5
                if cache_savings > 0:
                    self.append_log(f"  [ë¹„ìš©ì ˆê°] {bid}: ìºì‹±ìœ¼ë¡œ ${cache_savings:.4f} ì ˆê°")

                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    result_cols = ["bg_positive_en", "bg_negative_en", "video_motion_prompt_en", "video_full_prompt_en"]
                    for col in result_cols:
                        if col not in df.columns:
                            df[col] = ""
                        df[col] = df[col].astype(str)
                    
                    cnt = 0
                    for cid, result_data in results_map.items():
                        try:
                            idx = int(cid.split("_")[1])
                            if 0 <= idx < len(df):
                                for col in result_cols:
                                    if col in result_data:
                                        df.at[idx, col] = str(result_data[col])
                                cnt += 1
                        except:
                            pass

                    # ì¤‘ê°„ íŒŒì¼ ì €ì¥
                    base, ext = os.path.splitext(src_path)
                    out_excel = f"{base}_bg_prompt_batch_done{ext}"
                    if not safe_save_excel(df, out_excel):
                        self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")
                        continue

                    # I4 ë²„ì „ íŒŒì¼ë¡œ ì €ì¥
                    try:
                        final_out_path = get_i4_output_path(src_path)
                        df_done = pd.read_excel(out_excel)
                        if safe_save_excel(df_done, final_out_path):
                            # ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
                            if out_excel != final_out_path and os.path.exists(out_excel):
                                try:
                                    os.remove(out_excel)
                                    self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                                except Exception as e:
                                    self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                        else:
                            final_out_path = out_excel
                    except Exception as e:
                        self.append_log(f"[WARN] I4 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                        final_out_path = out_excel

                    upsert_batch_job(bid, out_excel=final_out_path, status="merged")

                    # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— I3-2 ì™„ë£Œ ìƒíƒœ ê¸°ë¡ - img ìƒíƒœë§Œ I3-2(ë°°ê²½ë¶„ì„ì™„ë£Œ)ë¡œ ì—…ë°ì´íŠ¸ (text ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                    try:
                        root_name = get_root_filename(src_path)
                        JobManager.update_status(root_name, img_s3_2_msg="I3-2(ë°°ê²½ë¶„ì„ì™„ë£Œ)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> I3-2(ë°°ê²½ë¶„ì„ì™„ë£Œ)")
                    except Exception as e:
                        self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")

                    self.append_log(f"âœ… ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(final_out_path)}")
                    success_cnt += 1
                    # ì„±ê³µí•œ íŒŒì¼ì˜ í´ë” ê²½ë¡œ ì €ì¥
                    success_folders.add(os.path.dirname(final_out_path))
                else:
                    self.append_log(f"âš ï¸ ì›ë³¸ ì—†ìŒ. JSONLë§Œ ì €ì¥.")
            except Exception as e:
                self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: {e}")
        
        self.append_log(f"=== ì¼ê´„ ë³‘í•© ë (ì„±ê³µ: {success_cnt}, ë¹„ìš©: ${total_cost:.4f}) ===")
        self._load_jobs_all()
        
        # ì„±ê³µí•œ íŒŒì¼ì´ ìˆìœ¼ë©´ í´ë” ì—´ê¸° ì—¬ë¶€ í™•ì¸
        if success_folders:
            folder_path = list(success_folders)[0]  # ì²« ë²ˆì§¸ ì„±ê³µí•œ í´ë”
            if messagebox.askyesno("ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©: ${total_cost:.4f}\n\nê²°ê³¼ íŒŒì¼ì´ ì €ì¥ëœ í´ë”ë¥¼ ì—´ê¹Œìš”?"):
                try:
                    if platform.system() == "Windows":
                        os.startfile(folder_path)
                    elif platform.system() == "Darwin":  # macOS
                        subprocess.run(["open", folder_path])
                    else:  # Linux
                        subprocess.run(["xdg-open", folder_path])
                    self.append_log(f"ğŸ“‚ ê²°ê³¼ í´ë” ì—´ê¸°: {folder_path}")
                except Exception as e:
                    self.append_log(f"[WARN] í´ë” ì—´ê¸° ì‹¤íŒ¨: {e}")
                    messagebox.showerror("ì˜¤ë¥˜", f"í´ë”ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{e}")
        else:
            messagebox.showinfo("ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©: ${total_cost:.4f}")

    def _archive_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if ids and messagebox.askyesno("ë³´ê´€", f"{len(ids)}ê±´ íœ´ì§€í†µ ì´ë™?"):
            archive_batch_job(ids, True)
            self._load_jobs_all()
            self._load_archive_list()

    def _restore_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ë³µêµ¬", f"{len(ids)}ê±´ ë³µêµ¬?"):
            archive_batch_job(ids, False)
            self._load_jobs_all()
            self._load_archive_list()

    def _hard_delete_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ì‚­ì œ", "ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            hard_delete_batch_job(ids)
            self._load_archive_list()

    def _on_tree_double_click(self, event):
        """ë”ë¸”í´ë¦­ ì‹œ: ê·¸ë£¹ í—¤ë”ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°, ë°°ì¹˜ë©´ ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™"""
        sel = self.tree_active.selection()
        if not sel: return
        
        item = sel[0]
        vals = self.tree_active.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš°: ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
        if not batch_id:
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            children = self.tree_active.get_children(item)
            if children:
                # ìì‹ì´ ìˆìœ¼ë©´ ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
                if self.tree_active.item(item, 'open'):
                    self.tree_active.item(item, open=False)
                else:
                    self.tree_active.item(item, open=True)
        else:
            # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°: ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™
            self.batch_id_var.set(batch_id)
            self.main_tabs.select(self.tab_merge)

    # ----------------------------------------------------
    # Tab 3: Merge (Manual)
    # ----------------------------------------------------
    def _init_tab_merge(self):
        container = ttk.Frame(self.tab_merge, padding=20)
        container.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì„¹ì…˜
        f_retry = ttk.LabelFrame(container, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", padding=15)
        f_retry.pack(fill='x', pady=(0, 15))
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        f_list = ttk.Frame(f_retry)
        f_list.pack(fill='both', expand=True, pady=(0, 10))
        ttk.Label(f_list, text="ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(anchor='w')
        
        # Treeviewë¡œ ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        list_frame = ttk.Frame(f_list)
        list_frame.pack(fill='both', expand=True, pady=5)
        
        scrollbar = ttk.Scrollbar(list_frame)
        scrollbar.pack(side='right', fill='y')
        
        self.failed_chunks_tree = ttk.Treeview(list_frame, columns=("chunk_num", "file_name", "error_type"), 
                                                show='headings', height=4, yscrollcommand=scrollbar.set)
        scrollbar.config(command=self.failed_chunks_tree.yview)
        
        self.failed_chunks_tree.heading("chunk_num", text="ì²­í¬ ë²ˆí˜¸")
        self.failed_chunks_tree.heading("file_name", text="íŒŒì¼ëª…")
        self.failed_chunks_tree.heading("error_type", text="ì˜¤ë¥˜ ìœ í˜•")
        
        self.failed_chunks_tree.column("chunk_num", width=80, anchor="center")
        self.failed_chunks_tree.column("file_name", width=300, anchor="w")
        self.failed_chunks_tree.column("error_type", width=150, anchor="center")
        
        self.failed_chunks_tree.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ ì…ë ¥
        f_file = ttk.Frame(f_retry)
        f_file.pack(fill='x', pady=(10, 0))
        ttk.Label(f_file, text="ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼:", font=("ë§‘ì€ ê³ ë”•", 9)).pack(side='left')
        self.failed_chunks_file_var = tk.StringVar()
        ttk.Entry(f_file, textvariable=self.failed_chunks_file_var, width=50, font=("Consolas", 9)).pack(side='left', padx=5, fill='x', expand=True)
        btn_select = ttk.Button(f_file, text="ğŸ“‚ ì°¾ê¸°", command=self._select_failed_chunks_file)
        btn_select.pack(side='left', padx=5)
        btn_retry = ttk.Button(f_file, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", command=self._retry_failed_chunks, style="Success.TButton")
        btn_retry.pack(side='left', padx=5)
        
        f_in = ttk.LabelFrame(container, text="ê°œë³„ ì‘ì—…", padding=15)
        f_in.pack(fill='x', pady=(0, 15))
        ttk.Label(f_in, text="Batch ID:").pack(side='left')
        ttk.Entry(f_in, textvariable=self.batch_id_var, width=45, font=("Consolas", 10)).pack(side='left', padx=10)
        
        f_btn = ttk.Frame(container)
        f_btn.pack(fill='x', pady=20)
        ttk.Button(f_btn, text="1. ê²°ê³¼ ë³‘í•©", command=self._start_merge, style="Primary.TButton").pack(fill='x', pady=5)

    def _start_merge(self):
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        bid = self.batch_id_var.get().strip()
        if not bid:
            messagebox.showwarning("ì˜¤ë¥˜", "Batch IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        t = threading.Thread(target=self._run_merge)
        t.daemon = True
        t.start()

    def _run_merge(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_merge_multi([bid])
    
    def _handle_failed_chunks(self, failed_info_path, failed_chunk_files):
        """ì‹¤íŒ¨í•œ ì²­í¬ê°€ ìˆì„ ë•Œ GUIì— ìë™ìœ¼ë¡œ í‘œì‹œí•˜ê³  ì•Œë¦¼"""
        # ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ìë™ ì„¤ì •
        self.failed_chunks_file_var.set(failed_info_path)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ì„ Treeviewì— í‘œì‹œ
        for item in self.failed_chunks_tree.get_children():
            self.failed_chunks_tree.delete(item)
        
        for failed_info in failed_chunk_files:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
            file_name = os.path.basename(chunk_file)
            
            self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        
        # ì•Œë¦¼ ë©”ì‹œì§€ ë° íƒ­ ì „í™˜
        failed_count = len(failed_chunk_files)
        token_limit_count = sum(1 for f in failed_chunk_files if f.get("is_token_limit", False))
        
        msg = f"âš ï¸ {failed_count}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\n"
        if token_limit_count > 0:
            msg += f"â€¢ í† í° ì œí•œ ì˜¤ë¥˜: {token_limit_count}ê°œ\n"
        msg += f"â€¢ ì‹¤íŒ¨ ì •ë³´ê°€ ìë™ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        msg += f"â€¢ '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        messagebox.showwarning("ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨", msg)
        
        # ì¬ì‹œë„ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
        self.main_tabs.select(self.tab_merge)
    
    def _select_failed_chunks_file(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ"""
        path = filedialog.askopenfilename(
            title="ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ",
            filetypes=[("JSON íŒŒì¼", "*.json"), ("ëª¨ë“  íŒŒì¼", "*.*")]
        )
        if path:
            self.failed_chunks_file_var.set(path)
            # íŒŒì¼ì„ ì„ íƒí•˜ë©´ ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(path)
    
    def _load_failed_chunks_from_file(self, file_path):
        """ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ì„ ì½ì–´ì„œ ëª©ë¡ì— í‘œì‹œ"""
        if not os.path.exists(file_path):
            return
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ê¸°ì¡´ ëª©ë¡ ì‚­ì œ
            for item in self.failed_chunks_tree.get_children():
                self.failed_chunks_tree.delete(item)
            
            # ëª©ë¡ì— ì¶”ê°€
            for failed_info in failed_chunks:
                chunk_num = failed_info.get("chunk_num", 0)
                chunk_file = failed_info.get("chunk_file", "")
                error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
                file_name = os.path.basename(chunk_file)
                
                self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        except Exception as e:
            self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    def _retry_failed_chunks(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„"""
        failed_file = self.failed_chunks_file_var.get().strip()
        if not failed_file:
            messagebox.showwarning("ê²½ê³ ", "ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        if not os.path.exists(failed_file):
            messagebox.showerror("ì˜¤ë¥˜", f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{failed_file}")
            return
        
        if not self.api_key_var.get():
            messagebox.showwarning("ê²½ê³ ", "API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        try:
            with open(failed_file, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(failed_file)
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨:\n{e}")
            return
        
        if not failed_chunks:
            messagebox.showinfo("ì•Œë¦¼", "ì¬ì‹œë„í•  ì‹¤íŒ¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if messagebox.askyesno("í™•ì¸", f"{len(failed_chunks)}ê°œ ì‹¤íŒ¨í•œ ì²­í¬ë¥¼ ì¬ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_retry_failed_chunks, args=(failed_chunks,))
            t.daemon = True
            t.start()
    
    def _run_retry_failed_chunks(self, failed_chunks):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì‹¤í–‰"""
        key = self.api_key_var.get().strip()
        import httpx
        timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
        
        self.append_log(f"[RETRY] ì‹¤íŒ¨í•œ ì²­í¬ {len(failed_chunks)}ê°œ ì¬ì‹œë„ ì‹œì‘...")
        
        retry_batch_ids = []
        for failed_info in failed_chunks:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            excel_path = failed_info.get("excel_path", "")
            model_name = failed_info.get("model_name", DEFAULT_MODEL)
            effort = failed_info.get("effort", "low")
            batch_group_id = failed_info.get("batch_group_id", "")
            
            if not os.path.exists(chunk_file):
                self.append_log(f"âš ï¸ ì²­í¬ {chunk_num}: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_file}")
                continue
            
            self.append_log(f"[RETRY] ì²­í¬ {chunk_num} ì¬ì‹œë„ ì¤‘... ({os.path.basename(chunk_file)})")
            
            try:
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=chunk_file,
                    excel_path=excel_path,
                    model_name=model_name,
                    reasoning_effort=effort,
                )
                
                batch_id = batch.id
                retry_batch_ids.append(batch_id)
                self.append_log(f"âœ… ì²­í¬ {chunk_num} ì¬ì‹œë„ ì„±ê³µ: {batch_id}")
                
                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=excel_path,
                    jsonl_path=chunk_file,
                    model=model_name,
                    effort=effort,
                    status=batch.status,
                    output_file_id=None,
                    batch_group_id=batch_group_id,
                    chunk_index=chunk_num,
                )
                
            except Exception as e:
                self.append_log(f"âŒ ì²­í¬ {chunk_num} ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        if retry_batch_ids:
            self.append_log(f"âœ… ì¬ì‹œë„ ì™„ë£Œ: {len(retry_batch_ids)}ê°œ ë°°ì¹˜ ìƒì„±ë¨")
            # ë°°ì¹˜ ëª©ë¡ ê°±ì‹  ë° ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
            self.after(0, lambda: [
                self._load_jobs_all(),
                self._load_archive_list(),
                self.main_tabs.select(self.tab_manage),  # ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
                messagebox.showinfo("ì™„ë£Œ", f"{len(retry_batch_ids)}ê°œ ì²­í¬ ì¬ì‹œë„ ì„±ê³µ:\n{', '.join(retry_batch_ids[:5])}{'...' if len(retry_batch_ids) > 5 else ''}\n\në°°ì¹˜ ê´€ë¦¬ íƒ­ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.")
            ])
        else:
            self.append_log(f"âš ï¸ ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.after(0, lambda: messagebox.showwarning("ê²½ê³ ", "ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤."))


if __name__ == "__main__":
    app = BGPromptBatchGUI()
    app.mainloop()

