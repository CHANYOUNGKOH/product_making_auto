"""
IMG_Batch_analysis_gui_Casche.py

Stage 3-1: ì¸ë„¤ì¼ ì´ë¯¸ì§€ ë¶„ì„ (ë°°ì¹˜/ëŒ€ëŸ‰) - ìºì‹± ìµœì í™” ë²„ì „
- ê¸°ëŠ¥: Batch JSONL ìƒì„± -> ì—…ë¡œë“œ -> ì‹¤í–‰ -> ê²°ê³¼ ë³‘í•©
- IMG_analysis_core_Casche.pyë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬
- ì…ë ¥: I2 ë˜ëŠ” I3 íŒŒì¼ë§Œ í—ˆìš©
- ì¶œë ¥: í•­ìƒ I3ë¡œ ê³ ì •
- ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™”: OpenAI Prompt Caching ê°€ì´ë“œì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì¬êµ¬ì„±
  * ì •ì  ì½˜í…ì¸ (ì—­í• , ì œì•½, ê·œì¹™)ë¥¼ system í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * ë™ì  ì½˜í…ì¸ (ì´ë¯¸ì§€)ë¥¼ user í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * prompt_cache_key ì‚¬ìš©ìœ¼ë¡œ ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ (í† í° ë¹„ìš© ìµœëŒ€ 90% ì ˆê° ê°€ëŠ¥)
"""

import os
import json
import re
import threading
import subprocess
import platform
from datetime import datetime
from typing import Optional

import pandas as pd
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, Menu
from tkinter.scrolledtext import ScrolledText

from openai import OpenAI

# ToolTip í´ë˜ìŠ¤
class ToolTip:
    def __init__(self, widget, text):
        self.widget = widget
        self.text = text
        self.tipwindow = None
        self.id = None
        self.x = self.y = 0
        self.widget.bind('<Enter>', self.enter)
        self.widget.bind('<Leave>', self.leave)
        self.widget.bind('<ButtonPress>', self.leave)

    def enter(self, event=None):
        self.schedule()

    def leave(self, event=None):
        self.unschedule()
        self.hidetip()

    def schedule(self):
        self.unschedule()
        self.id = self.widget.after(500, self.showtip)

    def unschedule(self):
        id = self.id
        self.id = None
        if id:
            self.widget.after_cancel(id)

    def showtip(self, event=None):
        x, y, cx, cy = self.widget.bbox("insert") if hasattr(self.widget, 'bbox') else (0, 0, 0, 0)
        x += self.widget.winfo_rootx() + 25
        y += self.widget.winfo_rooty() + 20
        self.tipwindow = tw = tk.Toplevel(self.widget)
        tw.wm_overrideredirect(True)
        tw.wm_geometry("+%d+%d" % (x, y))
        label = tk.Label(tw, text=self.text, justify=tk.LEFT,
                         background="#ffffe0", relief=tk.SOLID, borderwidth=1,
                         font=("ë§‘ì€ ê³ ë”•", 9))
        label.pack(ipadx=1)

    def hidetip(self):
        tw = self.tipwindow
        self.tipwindow = None
        if tw:
            tw.destroy()

# [í•„ìˆ˜ ì˜ì¡´ì„±] IMG_analysis_core_Casche.py
# ìºì‹± ìµœì í™” ë²„ì „ ì‚¬ìš© (IMG_analysis_core_Casche.py)
try:
    from IMG_analysis_core_Casche import (
        API_KEY_FILE,
        DEFAULT_MODEL,
        load_api_key_from_file,
        save_api_key_to_file,
        build_analysis_messages,
        build_analysis_batch_payload,  # Batch APIìš© payload ë¹Œë” (ìºì‹± ìµœì í™”)
        MODEL_PRICING_USD_PER_MTOK,
    )
    CACHE_MODE_CORE = True
except ImportError:
    # ìºì‹± ë²„ì „ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ë²„ì „ ì‚¬ìš©
    try:
        from IMG_analysis_core import (
            API_KEY_FILE,
            DEFAULT_MODEL,
            load_api_key_from_file,
            save_api_key_to_file,
            build_analysis_messages,
            MODEL_PRICING_USD_PER_MTOK,
        )
        CACHE_MODE_CORE = False
        def build_analysis_batch_payload(*args, **kwargs): return None
    except ImportError:
        # ì˜ì¡´ì„± íŒŒì¼ ë¶€ì¬ ì‹œ ë¹„ìƒìš© ë”ë¯¸
        CACHE_MODE_CORE = False
        API_KEY_FILE = ".openai_api_key_img_analysis"
        DEFAULT_MODEL = "gpt-5-mini"
        MODEL_PRICING_USD_PER_MTOK = {}
        def load_api_key_from_file(x): return ""
        def save_api_key_to_file(x, y): pass
        def build_analysis_messages(*args, **kwargs): return []
        def build_analysis_batch_payload(*args, **kwargs): return None

# ========================================================
# ë©”ì¸ ëŸ°ì²˜ ì—°ë™ìš© JobManager & íŒŒì¼ëª… ìœ í‹¸
# ========================================================
def get_root_filename(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´(_T*_I* ë˜ëŠ” _t*_i*, T4(ì™„)_I* í¬í•¨) ë° ê¸°íƒ€ ê¼¬ë¦¬í‘œë¥¼ ë–¼ê³  ì›ë³¸ëª…(Key)ë§Œ ì¶”ì¶œ
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0.xlsx -> ë‚˜ì´í‚¤.xlsx
    ì˜ˆ: ì•„ë””ë‹¤ìŠ¤_T3_I2.xlsx -> ì•„ë””ë‹¤ìŠ¤.xlsx
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0(ì—…ì™„).xlsx -> ë‚˜ì´í‚¤.xlsx
    ì˜ˆ: ë‚˜ì´í‚¤_T0_I0_T1_I1.xlsx -> ë‚˜ì´í‚¤.xlsx (ì—¬ëŸ¬ ë²„ì „ íŒ¨í„´ ì œê±°)
    ì˜ˆ: ë‚˜ì´í‚¤_T4(ì™„)_I2.xlsx -> ë‚˜ì´í‚¤.xlsx
    """
    name = os.path.basename(filename)
    base, ext = os.path.splitext(name)
    
    # 1. ë²„ì „ íŒ¨í„´ (_Tìˆ«ì(ê´„í˜¸)?_Iìˆ«ì ë˜ëŠ” _tìˆ«ì(ê´„í˜¸)?_iìˆ«ì) ë°˜ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    # íŒ¨í„´ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì œê±°, T4(ì™„)_I* íŒ¨í„´ë„ í¬í•¨
    while True:
        new_base = re.sub(r"_[Tt]\d+\([^)]*\)_[Ii]\d+", "", base, flags=re.IGNORECASE)  # T4(ì™„)_I* íŒ¨í„´ ì œê±°
        new_base = re.sub(r"_[Tt]\d+_[Ii]\d+", "", new_base, flags=re.IGNORECASE)  # ì¼ë°˜ T*_I* íŒ¨í„´ ì œê±°
        if new_base == base:
            break
        base = new_base
    
    # 2. ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: (ì—…ì™„), (ì™„ë£Œ) ë“±) - ë²„ì „ íŒ¨í„´ì˜ ê´„í˜¸ëŠ” ì´ë¯¸ ì œê±°ë¨
    base = re.sub(r"\([^)]*\)", "", base)
    
    # 3. ê¸°íƒ€ êµ¬í˜• ê¼¬ë¦¬í‘œ ì œê±° (í˜¸í™˜ì„± ìœ ì§€)
    suffixes = ["_img_analysis_done", "_img_analysis_batch_done", "_stage1_mapping", "_stage1_img_mapping", "_stage2_analysis", "_stage3_done", "_stage4_2_done", "_with_images"]
    for s in suffixes:
        base = base.replace(s, "")
    
    # 4. ëì— ë‚¨ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    base = base.rstrip("_")
        
    return base + ext


def get_i3_output_path(input_path: str) -> str:
    """
    ì…ë ¥ íŒŒì¼ëª…ì„ ë¶„ì„í•´ì„œ I3ë¡œ ê³ ì •ëœ ì¶œë ¥ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì…ë ¥: I2 ë˜ëŠ” I3 íŒŒì¼ (ì˜ˆ: ìƒí’ˆ_T3_I2.xlsx, ìƒí’ˆ_T3_I3.xlsx, ìƒí’ˆ_T4(ì™„)_I2.xlsx)
    ì¶œë ¥: í•­ìƒ I3 (ì˜ˆ: ìƒí’ˆ_T3_I3.xlsx, ìƒí’ˆ_T4(ì™„)_I3.xlsx)
    """
    dir_name = os.path.dirname(input_path)
    base_name = os.path.basename(input_path)
    name_only, ext = os.path.splitext(base_name)

    # T4(ì™„)_I* ë˜ëŠ” ì¼ë°˜ _T*_I* íŒ¨í„´ ë§¤ì¹­
    pattern = r"_T(\d+)(\([^)]+\))?_I(\d+)$"
    match = re.search(pattern, name_only, re.IGNORECASE)

    if match:
        current_t = int(match.group(1))
        t_suffix = match.group(2) or ""  # (ì™„) ë¶€ë¶„ì´ ìˆìœ¼ë©´ ìœ ì§€
        original_name = name_only[: match.start()]
    else:
        # ë²„ì „ ì •ë³´ê°€ ì—†ìœ¼ë©´ T ë²„ì „ ì¶”ì¶œ ì‹œë„ (ê´„í˜¸ í¬í•¨ ê°€ëŠ¥)
        t_match = re.search(r"_T(\d+)(\([^)]+\))?", name_only, re.IGNORECASE)
        if t_match:
            current_t = int(t_match.group(1))
            t_suffix = t_match.group(2) or ""
            original_name = name_only[: t_match.start()]
        else:
            current_t = 0
            t_suffix = ""
            original_name = name_only

    # í•­ìƒ I3ë¡œ ê³ ì •, T ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì˜ˆ: T4(ì™„) ë˜ëŠ” T4)
    new_filename = f"{original_name}_T{current_t}{t_suffix}_I3{ext}"
    return os.path.join(dir_name, new_filename)


class JobManager:
    DB_FILE = None

    @classmethod
    def find_db_path(cls):
        if cls.DB_FILE and os.path.exists(cls.DB_FILE):
            return cls.DB_FILE

        current_dir = os.path.dirname(os.path.abspath(__file__))
        search_dirs = [
            current_dir,
            os.path.abspath(os.path.join(current_dir, "..")),
            os.path.abspath(os.path.join(current_dir, "..", "..")),
        ]

        for d in search_dirs:
            target = os.path.join(d, "job_history.json")
            if os.path.exists(target):
                cls.DB_FILE = target
                return target

        default_path = os.path.abspath(os.path.join(current_dir, "..", "job_history.json"))
        cls.DB_FILE = default_path
        return default_path

    @classmethod
    def load_jobs(cls):
        db_path = cls.find_db_path()
        if not os.path.exists(db_path):
            return {}
        try:
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    @classmethod
    def update_status(cls, filename, text_msg=None, img_msg=None, img_s3_1_msg=None, img_s3_2_msg=None):
        """
        ì‘ì—… ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            filename: íŒŒì¼ëª… (root filename)
            text_msg: í…ìŠ¤íŠ¸ ìƒíƒœ ë©”ì‹œì§€
            img_msg: ì´ë¯¸ì§€ ì „ì²´ ìƒíƒœ ë©”ì‹œì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
            img_s3_1_msg: Stage 3-1 (ì¸ë„¤ì¼ ë¶„ì„) ìƒíƒœ ë©”ì‹œì§€
            img_s3_2_msg: Stage 3-2 (ì „ì²˜ë¦¬) ìƒíƒœ ë©”ì‹œì§€
        """
        db_path = cls.find_db_path()
        data = cls.load_jobs()
        now = datetime.now().strftime("%m-%d %H:%M")

        if filename not in data:
            data[filename] = {
                "start_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "text_status": "ëŒ€ê¸°",
                "text_time": "-",
                "image_status": "ëŒ€ê¸°",
                "image_time": "-",
                "image_s3_1_status": "-",  # Stage 3-1: ì¸ë„¤ì¼ ë¶„ì„
                "image_s3_1_time": "-",
                "image_s3_2_status": "-",  # Stage 3-2: ì „ì²˜ë¦¬
                "image_s3_2_time": "-",
                "memo": "",
            }

        if text_msg:
            data[filename]["text_status"] = text_msg
            data[filename]["text_time"] = now
        
        if img_msg:
            # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ image_statusë„ ì—…ë°ì´íŠ¸
            data[filename]["image_status"] = img_msg
            data[filename]["image_time"] = now
        
        if img_s3_1_msg:
            data[filename]["image_s3_1_status"] = img_s3_1_msg
            data[filename]["image_s3_1_time"] = now
            # image_status í†µí•© ì—…ë°ì´íŠ¸ (S3-1, S3-2 ì ‘ë‘ì‚¬ ì œê±°)
            parts = []
            if data[filename].get("image_s3_1_status", "-") != "-":
                parts.append(data[filename]['image_s3_1_status'])  # "I3-1 (ì§„í–‰ì¤‘)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if data[filename].get("image_s3_2_status", "-") != "-":
                parts.append(data[filename]['image_s3_2_status'])  # "I3-2 (ì™„ë£Œ)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if parts:
                data[filename]["image_status"] = " / ".join(parts)
                data[filename]["image_time"] = now
        
        if img_s3_2_msg:
            data[filename]["image_s3_2_status"] = img_s3_2_msg
            data[filename]["image_s3_2_time"] = now
            # image_status í†µí•© ì—…ë°ì´íŠ¸ (S3-1, S3-2 ì ‘ë‘ì‚¬ ì œê±°)
            parts = []
            if data[filename].get("image_s3_1_status", "-") != "-":
                parts.append(data[filename]['image_s3_1_status'])  # "I3-1 (ì§„í–‰ì¤‘)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if data[filename].get("image_s3_2_status", "-") != "-":
                parts.append(data[filename]['image_s3_2_status'])  # "I3-2 (ì™„ë£Œ)" í˜•ì‹ ê·¸ëŒ€ë¡œ
            if parts:
                data[filename]["image_status"] = " / ".join(parts)
                data[filename]["image_time"] = now

        data[filename]["last_update"] = now

        try:
            with open(db_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"[JobManager Error] {e}")


def safe_save_excel(df: pd.DataFrame, path: str) -> bool:
    """ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆì–´ ì €ì¥ì´ ì•ˆ ë  ë•Œ ì¬ì‹œë„ë¥¼ ìœ ë„í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            df.to_excel(path, index=False)
            return True
        except PermissionError:
            if not messagebox.askretrycancel(
                "ì €ì¥ ì‹¤íŒ¨",
                f"ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤!\n[{os.path.basename(path)}]\n\n"
                "íŒŒì¼ì„ ë‹«ê³  'ë‹¤ì‹œ ì‹œë„'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
            ):
                return False
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False


# === ê¸°ë³¸ ì„¤ì • ===
BATCH_JOBS_FILE = os.path.join(os.path.dirname(__file__), "img_analysis_batch_jobs.json")
DEFAULT_SETTINGS_FILE = os.path.join(os.path.dirname(__file__), ".img_analysis_batch_defaults.json")

# --- UI ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ---
COLOR_BG = "#F8F9FA"
COLOR_WHITE = "#FFFFFF"
COLOR_PRIMARY = "#4A90E2"
COLOR_PRIMARY_HOVER = "#357ABD"
COLOR_SUCCESS = "#28A745"
COLOR_SUCCESS_HOVER = "#218838"
COLOR_DANGER = "#DC3545"
COLOR_DANGER_HOVER = "#C82333"
COLOR_TEXT = "#333333"
COLOR_HEADER = "#E9ECEF"

def get_seoul_now():
    try:
        from pytz import timezone
        return datetime.now(timezone("Asia/Seoul"))
    except:
        return datetime.now()

def load_default_settings():
    """ê¸°ë³¸ ì„¤ì •ê°’ ë¶ˆëŸ¬ì˜¤ê¸°"""
    default_settings = {
        "model": "gpt-5-mini",
        "effort": "low",
        "resize_mode": "B"
    }
    
    if os.path.exists(DEFAULT_SETTINGS_FILE):
        try:
            with open(DEFAULT_SETTINGS_FILE, "r", encoding="utf-8") as f:
                saved_settings = json.load(f)
                # ì €ì¥ëœ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ìœ íš¨í•œ ê°’ë§Œ)
                if "model" in saved_settings:
                    default_settings["model"] = saved_settings["model"]
                if "effort" in saved_settings:
                    default_settings["effort"] = saved_settings["effort"]
                if "resize_mode" in saved_settings:
                    default_settings["resize_mode"] = saved_settings["resize_mode"]
        except Exception as e:
            print(f"[WARN] ê¸°ë³¸ ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
    
    return default_settings

def save_default_settings(model, effort, resize_mode):
    """ê¸°ë³¸ ì„¤ì •ê°’ ì €ì¥"""
    settings = {
        "model": model,
        "effort": effort,
        "resize_mode": resize_mode
    }
    try:
        with open(DEFAULT_SETTINGS_FILE, "w", encoding="utf-8") as f:
            json.dump(settings, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"[ERROR] ê¸°ë³¸ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# ========================================================
# ë°°ì¹˜ ì¡ ê´€ë¦¬ (JSON DB)
# ========================================================
def load_batch_jobs():
    if not os.path.exists(BATCH_JOBS_FILE):
        return []
    try:
        with open(BATCH_JOBS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return []

def save_batch_jobs(jobs):
    try:
        with open(BATCH_JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"[Error] ì¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_excel_name_from_path(path: str) -> str:
    """ì „ì²´ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ"""
    if not path:
        return "-"
    return os.path.basename(path)

def upsert_batch_job(batch_id, **kwargs):
    jobs = load_batch_jobs()
    found = False
    now_str = datetime.now().isoformat()
    
    for j in jobs:
        if j["batch_id"] == batch_id:
            if kwargs.get("status") == "completed" and j.get("status") != "completed":
                if "completed_at" not in kwargs:
                    j["completed_at"] = now_str
            j.update(kwargs)
            j["updated_at"] = now_str
            found = True
            break
            
    if not found:
        new_job = {
            "batch_id": batch_id,
            "created_at": now_str,
            "updated_at": now_str,
            "completed_at": "",
            "archived": False,
            **kwargs
        }
        jobs.insert(0, new_job)
    save_batch_jobs(jobs)

def archive_batch_job(batch_ids, archive=True):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    for j in jobs:
        if j["batch_id"] in batch_ids:
            j["archived"] = archive
    save_batch_jobs(jobs)

def hard_delete_batch_job(batch_ids):
    if isinstance(batch_ids, str): batch_ids = [batch_ids]
    jobs = load_batch_jobs()
    jobs = [j for j in jobs if j["batch_id"] not in batch_ids]
    save_batch_jobs(jobs)

# ========================================================
# GUI Class
# ========================================================
class ImageAnalysisBatchGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Stage 3-1: ì¸ë„¤ì¼ ì´ë¯¸ì§€ ë¶„ì„ (ë°°ì¹˜/ëŒ€ëŸ‰) ğŸš€ ìºì‹± ìµœì í™” ë²„ì „")
        self.geometry("1250x950")
        
        self.api_key_var = tk.StringVar()
        
        # íŒŒì¼ ë³€ìˆ˜
        self.src_file_var = tk.StringVar()
        self.skip_exist_var = tk.BooleanVar(value=True)
        self.skip_bad_label_var = tk.BooleanVar(value=True)  # 'bad' ë¼ë²¨ í–‰ ì œì™¸ (ê¸°ë³¸ê°’: True)
        self.jsonl_file_var = tk.StringVar()  # ìƒì„±ëœ JSONL íŒŒì¼ ê²½ë¡œ
        
        # ëª¨ë¸ ì„¤ì • ë³€ìˆ˜ (ì €ì¥ëœ ê¸°ë³¸ê°’ ë¶ˆëŸ¬ì˜¤ê¸°)
        default_settings = load_default_settings()
        self.model_var = tk.StringVar(value=default_settings.get("model", "gpt-5-mini"))
        self.effort_var = tk.StringVar(value=default_settings.get("effort", "low"))
        # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ ë³€ìˆ˜ (ë‚´ë¶€ ê°’: A, B, C)
        self.resize_mode_var = tk.StringVar(value=default_settings.get("resize_mode", "B"))  # ê¸°ë³¸ê°’: B(512px)
        
        # ëª¨ë¸ ì„¤ì •ì°½ í‘œì‹œ/ìˆ¨ê¹€ ë³€ìˆ˜ (ê¸°ë³¸ê°’: ìˆ¨ê¹€)
        self.model_settings_visible = tk.BooleanVar(value=False)
        
        # JSONL ìƒì„± ì„¹ì…˜ í‘œì‹œ/ìˆ¨ê¹€ ë³€ìˆ˜ (ê¸°ë³¸ê°’: ìˆ¨ê¹€)
        self.jsonl_section_visible = tk.BooleanVar(value=False)
        
        # íƒ­ 3 ë³€ìˆ˜
        self.batch_id_var = tk.StringVar()
        self.failed_chunks_file_var = tk.StringVar()
        
        self._configure_styles()
        self._init_ui()
        self._load_key()

    def _configure_styles(self):
        style = ttk.Style()
        try: style.theme_use('clam')
        except: pass
        
        self.configure(background=COLOR_BG)
        
        style.configure("TFrame", background=COLOR_BG)
        style.configure("TLabel", background=COLOR_BG, foreground=COLOR_TEXT, font=("ë§‘ì€ ê³ ë”•", 10))
        style.configure("Header.TLabel", font=("ë§‘ì€ ê³ ë”•", 11, "bold"), foreground="#444")
        
        style.configure("TLabelframe", background=COLOR_BG, bordercolor="#D0D7DE")
        style.configure("TLabelframe.Label", background=COLOR_BG, foreground="#0056b3", font=("ë§‘ì€ ê³ ë”•", 10, "bold"))

        style.configure("TNotebook", background=COLOR_BG, borderwidth=0)
        style.configure("TNotebook.Tab", background="#E1E4E8", padding=[12, 5], font=("ë§‘ì€ ê³ ë”•", 10))
        style.map("TNotebook.Tab", background=[("selected", COLOR_WHITE)], foreground=[("selected", COLOR_PRIMARY)])
        
        style.configure("Treeview", background=COLOR_WHITE, fieldbackground=COLOR_WHITE, font=("ë§‘ì€ ê³ ë”•", 9), rowheight=28)
        style.configure("Treeview.Heading", background=COLOR_HEADER, foreground="#333", font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        style.map("Treeview", background=[('selected', '#CCE5FF')], foreground=[('selected', 'black')])

        style.configure("TButton", font=("ë§‘ì€ ê³ ë”•", 9), padding=5, borderwidth=1)
        style.configure("Primary.TButton", background=COLOR_PRIMARY, foreground="white", bordercolor=COLOR_PRIMARY)
        style.map("Primary.TButton", background=[("active", COLOR_PRIMARY_HOVER)])
        style.configure("Success.TButton", background=COLOR_SUCCESS, foreground="white", bordercolor=COLOR_SUCCESS)
        style.map("Success.TButton", background=[("active", COLOR_SUCCESS_HOVER)])
        style.configure("Danger.TButton", background=COLOR_DANGER, foreground="white", bordercolor=COLOR_DANGER)
        style.map("Danger.TButton", background=[("active", COLOR_DANGER_HOVER)])

    def _init_ui(self):
        main_container = ttk.Frame(self, padding=15)
        main_container.pack(fill='both', expand=True)

        # 1. ìƒë‹¨ API Key
        f_top = ttk.LabelFrame(main_container, text="ğŸ”‘ API ì„¤ì •", padding=10)
        f_top.pack(fill='x', pady=(0, 10))
        ttk.Label(f_top, text="Batch API Key:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(side='left')
        entry_key = ttk.Entry(f_top, textvariable=self.api_key_var, show="*", width=50, font=("Consolas", 10))
        entry_key.pack(side='left', padx=10)
        ttk.Button(f_top, text="ì €ì¥", command=self._save_key, style="Primary.TButton").pack(side='left')
        
        btn_save_defaults = ttk.Button(f_top, text="ğŸ’¾ ê¸°ë³¸ê°’ ì €ì¥", command=self._save_defaults, style="Success.TButton")
        btn_save_defaults.pack(side='left', padx=(5, 0))
        ToolTip(btn_save_defaults, "í˜„ì¬ ì„¤ì •ëœ ëª¨ë¸, ì¶”ë¡  ê°•ë„, ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\në‹¤ìŒ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")

        # 2. ë©”ì¸ íƒ­
        self.main_tabs = ttk.Notebook(main_container)
        self.main_tabs.pack(fill='both', expand=True, pady=5)
        
        self.tab_create = ttk.Frame(self.main_tabs)
        self.tab_manage = ttk.Frame(self.main_tabs) 
        self.tab_merge = ttk.Frame(self.main_tabs)
        
        self.main_tabs.add(self.tab_create, text=" 1. ë°°ì¹˜ ìƒì„± & ì—…ë¡œë“œ ")
        self.main_tabs.add(self.tab_manage, text=" 2. ë°°ì¹˜ ê´€ë¦¬ (ëª©ë¡/ë³‘í•©) ")
        self.main_tabs.add(self.tab_merge, text=" 3. ê°œë³„ ë³‘í•© (ìˆ˜ë™) ")
        
        self._init_tab_create()
        self._init_tab_manage()
        self._init_tab_merge()
        
        # 3. ë¡œê·¸
        f_log = ttk.LabelFrame(main_container, text="ğŸ“‹ ì‹œìŠ¤í…œ ë¡œê·¸", padding=10)
        f_log.pack(fill='both', expand=True, pady=(10, 0))
        self.log_widget = ScrolledText(f_log, height=22, state='disabled', font=("Consolas", 9), bg="#F1F3F5")
        self.log_widget.pack(fill='both', expand=True)

    def _load_key(self):
        loaded = load_api_key_from_file(API_KEY_FILE)
        if loaded: self.api_key_var.set(loaded)

    def _save_key(self):
        k = self.api_key_var.get().strip()
        if k:
            save_api_key_to_file(k, API_KEY_FILE)
            messagebox.showinfo("ì €ì¥", "API Key ì €ì¥ ì™„ë£Œ")
    
    def _save_defaults(self):
        """í˜„ì¬ ì„¤ì •ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì €ì¥"""
        model = self.model_var.get().strip()
        effort = self.effort_var.get().strip()
        resize_mode = self.resize_mode_var.get().strip()
        
        # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ í‘œì‹œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        resize_mode_display = self.resize_display_var.get() if hasattr(self, 'resize_display_var') else resize_mode
        
        if save_default_settings(model, effort, resize_mode):
            messagebox.showinfo(
                "ì €ì¥ ì™„ë£Œ", 
                f"ê¸°ë³¸ê°’ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n\n"
                f"â€¢ ëª¨ë¸: {model}\n"
                f"â€¢ ì¶”ë¡  ê°•ë„: {effort}\n"
                f"â€¢ ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ: {resize_mode_display}\n\n"
                f"ë‹¤ìŒ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
            )
        else:
            messagebox.showerror("ì €ì¥ ì‹¤íŒ¨", "ê¸°ë³¸ê°’ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    def _toggle_model_settings(self):
        """ëª¨ë¸ ì„¤ì •ì°½ í‘œì‹œ/ìˆ¨ê¹€ í† ê¸€"""
        if self.model_settings_visible.get():
            # í‘œì‹œ (í¼ì¹˜ê¸°)
            self.f_opt_content.pack(fill='x', pady=0)
        else:
            # ìˆ¨ê¹€ (ì ‘ê¸°)
            self.f_opt_content.pack_forget()
        self._update_model_settings_summary()
    
    def _update_model_settings_summary(self):
        """í˜„ì¬ ëª¨ë¸ ì„¤ì • ìš”ì•½ ì—…ë°ì´íŠ¸"""
        model = self.model_var.get()
        effort = self.effort_var.get()
        resize_mode = self.resize_mode_var.get()
        
        # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ í‘œì‹œ í…ìŠ¤íŠ¸
        resize_display = "ê¸°ë³¸"
        if resize_mode == "B":
            resize_display = "512px"
        elif resize_mode == "C":
            resize_display = "448px"
        
        summary_text = f"í˜„ì¬ ì„¤ì •: ëª¨ë¸={model}, effort={effort}, ë¦¬ì‚¬ì´ì¦ˆ={resize_display}"
        self.model_settings_summary_label.config(text=summary_text)
    
    def _toggle_jsonl_section(self):
        """JSONL ìƒì„± ì„¹ì…˜ í‘œì‹œ/ìˆ¨ê¹€ í† ê¸€"""
        if self.jsonl_section_visible.get():
            # í‘œì‹œ (í¼ì¹˜ê¸°)
            self.f_jsonl_content.pack(fill='x', pady=0)
        else:
            # ìˆ¨ê¹€ (ì ‘ê¸°)
            self.f_jsonl_content.pack_forget()

    def append_log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        self.log_widget.config(state='normal')
        self.log_widget.insert('end', f"[{ts}] {msg}\n")
        self.log_widget.see('end')
        self.log_widget.config(state='disabled')

    # ----------------------------------------------------
    # Tab 1: Create (ìƒì„±)
    # ----------------------------------------------------
    def _init_tab_create(self):
        container = ttk.Frame(self.tab_create, padding=20)
        container.pack(fill='both', expand=True)
        
        # Step 1: íŒŒì¼
        f_file = ttk.LabelFrame(container, text="1. ì‘ì—… ëŒ€ìƒ íŒŒì¼ (IMG_S1_ëˆ„ë¼ í¬í•¨, I2 ë˜ëŠ” I3)", padding=15)
        f_file.pack(fill='x', pady=(0, 10))
        ttk.Entry(f_file, textvariable=self.src_file_var, font=("ë§‘ì€ ê³ ë”•", 10)).pack(side='left', fill='x', expand=True)
        ttk.Button(f_file, text="ğŸ“‚ íŒŒì¼ ì°¾ê¸°", command=self._select_src_file).pack(side='right', padx=5)
        
        # Step 2: ëª¨ë¸ ì„¤ì • (ì ‘ê¸°/í¼ì¹˜ê¸° ê°€ëŠ¥)
        f_opt_outer = ttk.Frame(container)
        f_opt_outer.pack(fill='x', pady=(0, 10))
        
        # í—¤ë” í”„ë ˆì„ (í•­ìƒ í‘œì‹œ)
        f_opt_header = ttk.Frame(f_opt_outer)
        f_opt_header.pack(fill='x', pady=(0, 5))
        
        # ì²´í¬ë°•ìŠ¤ì™€ ë ˆì´ë¸”
        f_header_left = ttk.Frame(f_opt_header)
        f_header_left.pack(side='left', fill='x', expand=True)
        
        chk_show_settings = ttk.Checkbutton(
            f_header_left, 
            text="âš™ï¸ ëª¨ë¸ ì„¤ì • ë³´ê¸°/ìˆ¨ê¸°ê¸°", 
            variable=self.model_settings_visible,
            command=self._toggle_model_settings
        )
        chk_show_settings.pack(side='left', padx=5)
        ToolTip(chk_show_settings, "ëª¨ë¸, ì¶”ë¡  ê°•ë„, ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ ì„¤ì •ì„ í‘œì‹œí•˜ê±°ë‚˜ ìˆ¨ê¹ë‹ˆë‹¤.\nê¸°ë³¸ê°’ì€ ìˆ¨ê¹€ ìƒíƒœì…ë‹ˆë‹¤. (ë¡œê·¸ ì°½ì´ ë” ì˜ ë³´ì…ë‹ˆë‹¤)")
        
        # í˜„ì¬ ì„¤ì • ìš”ì•½ í‘œì‹œ
        self.model_settings_summary_label = ttk.Label(
            f_header_left, 
            text="", 
            font=("ë§‘ì€ ê³ ë”•", 9),
            foreground="#666"
        )
        self.model_settings_summary_label.pack(side='left', padx=(10, 0))
        self._update_model_settings_summary()
        
        # ëª¨ë¸ ì„¤ì • ë‚´ìš© í”„ë ˆì„ (ì ‘ê¸°/í¼ì¹˜ê¸° ëŒ€ìƒ)
        self.f_opt_content = ttk.LabelFrame(f_opt_outer, text="2. ëª¨ë¸ ì„¤ì •", padding=15)
        self.f_opt_content.pack(fill='x', pady=0)

        # ëª¨ë¸ & Effort
        fr1 = ttk.Frame(self.f_opt_content)
        fr1.pack(fill='x', pady=5)
        ttk.Label(fr1, text="ëª¨ë¸ (Model):", width=12).pack(side='left')
        models = list(MODEL_PRICING_USD_PER_MTOK.keys()) if MODEL_PRICING_USD_PER_MTOK else ["gpt-5-mini", "gpt-5", "gpt-5-nano"]
        cb_model = ttk.Combobox(fr1, textvariable=self.model_var, values=models, state="readonly", width=20)
        cb_model.pack(side='left', padx=5)
        
        # ëª¨ë¸ ë³€ê²½ ì‹œ ìš”ì•½ ì—…ë°ì´íŠ¸
        cb_model.bind("<<ComboboxSelected>>", lambda e: self._update_model_settings_summary())
        
        ttk.Label(fr1, text="ì¶”ë¡  ê°•ë„:", width=10).pack(side='left', padx=(20, 5))
        cb_effort = ttk.Combobox(fr1, textvariable=self.effort_var, values=["none", "low", "medium", "high"], state="readonly", width=12)
        cb_effort.pack(side='left', padx=5)
        
        # ì¶”ë¡  ê°•ë„ ë³€ê²½ ì‹œ ìš”ì•½ ì—…ë°ì´íŠ¸
        cb_effort.bind("<<ComboboxSelected>>", lambda e: self._update_model_settings_summary())
        
        # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ ì„ íƒ (ì‚¬ìš©ì ì¹œí™”ì  í‘œì‹œ)
        fr2 = ttk.Frame(self.f_opt_content)
        fr2.pack(fill='x', pady=5)
        ttk.Label(fr2, text="ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ:", width=12).pack(side='left')
        
        # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ ë§¤í•‘: í‘œì‹œ í…ìŠ¤íŠ¸ -> ë‚´ë¶€ ê°’ (A, B, C)
        self.resize_mode_display_map = {
            "A": "ê¸°ë³¸ ëª¨ë“œ (ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨) - ì›ë³¸ ì´ë¯¸ì§€ ê·¸ëŒ€ë¡œ ì‚¬ìš©",
            "B": "ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ 512px - ê°€ë¡œ 512pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€, í¬ë¡­ ê¸ˆì§€",
            "C": "ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ 448px - ê°€ë¡œ 448pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€, í¬ë¡­ ê¸ˆì§€"
        }
        
        # í˜„ì¬ ì„ íƒëœ ê°’ì„ í‘œì‹œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        current_mode = self.resize_mode_var.get()
        display_values = list(self.resize_mode_display_map.values())
        self.resize_display_var = tk.StringVar()
        
        # í˜„ì¬ ì„ íƒëœ ëª¨ë“œì— í•´ë‹¹í•˜ëŠ” í‘œì‹œ í…ìŠ¤íŠ¸ ì„¤ì •
        if current_mode == "A":
            self.resize_display_var.set(display_values[0])
        elif current_mode == "B":
            self.resize_display_var.set(display_values[1])
        elif current_mode == "C":
            self.resize_display_var.set(display_values[2])
        else:
            self.resize_display_var.set(display_values[1])  # ê¸°ë³¸ê°’ B
        
        cb_resize = ttk.Combobox(fr2, textvariable=self.resize_display_var, values=display_values, state="readonly", width=65)
        cb_resize.pack(side='left', padx=5)
        
        # ì½¤ë³´ë°•ìŠ¤ ê°’ ë³€ê²½ ì‹œ ë‚´ë¶€ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ ë° ìš”ì•½ ì—…ë°ì´íŠ¸
        def on_resize_mode_change(event=None):
            selected_text = self.resize_display_var.get()
            if "ê¸°ë³¸ ëª¨ë“œ" in selected_text or "ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨" in selected_text:
                self.resize_mode_var.set("A")
            elif "512px" in selected_text:
                self.resize_mode_var.set("B")
            elif "448px" in selected_text:
                self.resize_mode_var.set("C")
            self._update_model_settings_summary()
        cb_resize.bind("<<ComboboxSelected>>", on_resize_mode_change)
        
        # íˆ´íŒ ì¶”ê°€
        ToolTip(cb_resize, "ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ ì„ íƒ:\nâ€¢ ê¸°ë³¸ ëª¨ë“œ: ì›ë³¸ ì´ë¯¸ì§€ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í† í° ë¹„ìš© ë†’ìŒ)\nâ€¢ 512px ëª¨ë“œ: ê°€ë¡œ 512pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€ (í† í° ì ˆê°)\nâ€¢ 448px ëª¨ë“œ: ê°€ë¡œ 448pxë¡œ ì¶•ì†Œ, ë¹„ìœ¨ ìœ ì§€ (ìµœëŒ€ í† í° ì ˆê°)")
        
        # ì²´í¬ë°•ìŠ¤
        f_row_chk = ttk.Frame(self.f_opt_content)
        f_row_chk.pack(fill='x', pady=10)
        ttk.Checkbutton(f_row_chk, text=" ì´ë¯¸ view_point ë“±ì´ ìˆëŠ” í–‰ ê±´ë„ˆë›°ê¸°", variable=self.skip_exist_var).pack(side='left')
        
        f_row_chk2 = ttk.Frame(self.f_opt_content)
        f_row_chk2.pack(fill='x', pady=5)
        ttk.Checkbutton(f_row_chk2, text=" 'bad' ë¼ë²¨ì´ ìˆëŠ” í–‰ ì œì™¸ (IMG_S1_íœ´ë¨¼ë¼ë²¨ ë˜ëŠ” IMG_S1_AIë¼ë²¨ì´ 'bad'ì¸ ê²½ìš°)", variable=self.skip_bad_label_var).pack(side='left')
        
        # ê¸°ë³¸ê°’: ìˆ¨ê¹€ (ë¡œê·¸ ì°½ì´ ë” ì˜ ë³´ì´ë„ë¡)
        self._toggle_model_settings()
        
        # Step 3: JSONL ìƒì„± (ì ‘ê¸°/í¼ì¹˜ê¸° ê°€ëŠ¥)
        f_jsonl_outer = ttk.Frame(container)
        f_jsonl_outer.pack(fill='x', pady=(0, 10))
        
        # í—¤ë” í”„ë ˆì„ (í•­ìƒ í‘œì‹œ)
        f_jsonl_header = ttk.Frame(f_jsonl_outer)
        f_jsonl_header.pack(fill='x', pady=(0, 5))
        
        # ì²´í¬ë°•ìŠ¤ì™€ ë ˆì´ë¸”
        f_jsonl_header_left = ttk.Frame(f_jsonl_header)
        f_jsonl_header_left.pack(side='left', fill='x', expand=True)
        
        chk_show_jsonl = ttk.Checkbutton(
            f_jsonl_header_left, 
            text="ğŸ“„ JSONL ìƒì„± ì˜µì…˜ ë³´ê¸°/ìˆ¨ê¸°ê¸°", 
            variable=self.jsonl_section_visible,
            command=self._toggle_jsonl_section
        )
        chk_show_jsonl.pack(side='left', padx=5)
        ToolTip(chk_show_jsonl, "JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ ì˜µì…˜ì„ í‘œì‹œí•˜ê±°ë‚˜ ìˆ¨ê¹ë‹ˆë‹¤.\nê¸°ë³¸ê°’ì€ ìˆ¨ê¹€ ìƒíƒœì…ë‹ˆë‹¤. (í†µí•© ì‹¤í–‰ ë²„íŠ¼ë§Œ ì‚¬ìš© ê°€ëŠ¥)")
        
        # JSONL ì„¹ì…˜ ë‚´ìš© í”„ë ˆì„ (ì ‘ê¸°/í¼ì¹˜ê¸° ëŒ€ìƒ)
        self.f_jsonl_content = ttk.LabelFrame(f_jsonl_outer, text="3. JSONL ìƒì„± (ê°œë³„ ì‘ì—…)", padding=15)
        self.f_jsonl_content.pack(fill='x', pady=0)
        
        # ìƒì„±ëœ JSONL íŒŒì¼ ê²½ë¡œ í‘œì‹œ
        f_jsonl = ttk.Frame(self.f_jsonl_content)
        f_jsonl.pack(fill='x', pady=(0, 10))
        ttk.Label(f_jsonl, text="JSONL íŒŒì¼ ê²½ë¡œ:", font=("ë§‘ì€ ê³ ë”•", 9)).pack(side='left')
        ttk.Entry(f_jsonl, textvariable=self.jsonl_file_var, font=("Consolas", 9), width=60).pack(side='left', padx=5, fill='x', expand=True)
        ttk.Button(f_jsonl, text="ğŸ“‚ ì°¾ê¸°", command=self._select_jsonl_file).pack(side='right', padx=5)
        
        # ë¶„ë¦¬ëœ ë²„íŠ¼ë“¤
        f_btn_separated = ttk.Frame(self.f_jsonl_content)
        f_btn_separated.pack(fill='x', pady=5)
        btn_create = ttk.Button(f_btn_separated, text="ğŸ“„ JSONL ìƒì„±ë§Œ (Create JSONL)", command=self._create_jsonl_only, style="Primary.TButton")
        btn_create.pack(side='left', fill='x', expand=True, padx=(0, 5), ipady=6)
        btn_upload = ttk.Button(f_btn_separated, text="â¬†ï¸ ë°°ì¹˜ ì—…ë¡œë“œ (Upload Batch)", command=self._upload_batch_from_jsonl, style="Success.TButton")
        btn_upload.pack(side='right', fill='x', expand=True, padx=(5, 0), ipady=6)
        
        # ê¸°ë³¸ê°’: ìˆ¨ê¹€ (í†µí•© ì‹¤í–‰ë§Œ ì‚¬ìš©í•˜ë„ë¡)
        self._toggle_jsonl_section()
        
        # í†µí•© ë²„íŠ¼ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€, í•­ìƒ í‘œì‹œ)
        f_step4 = ttk.LabelFrame(container, text="4. í†µí•© ì‹¤í–‰ (ê¶Œì¥)", padding=15)
        f_step4.pack(fill='x', pady=15)
        btn_integrated = ttk.Button(f_step4, text="ğŸš€ JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (í†µí•©)", command=self._start_create_batch, style="Success.TButton")
        btn_integrated.pack(fill='x', ipady=8)
        ttk.Label(container, text="â€» ë°°ì¹˜ APIëŠ” ê²°ê³¼ ìˆ˜ì‹ ê¹Œì§€ ìµœëŒ€ 24ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. (ë¹„ìš© 50% ì ˆê°)", foreground="#666").pack(pady=(5, 0))

    def _select_jsonl_file(self):
        """JSONL íŒŒì¼ ì„ íƒ"""
        p = filedialog.askopenfilename(
            title="JSONL íŒŒì¼ ì„ íƒ",
            filetypes=[("JSONL", "*.jsonl"), ("ëª¨ë“  íŒŒì¼", "*.*")]
        )
        if p:
            self.jsonl_file_var.set(p)
            self.append_log(f"JSONL íŒŒì¼ ì„ íƒë¨: {os.path.basename(p)}")
    
    def _select_src_file(self):
        p = filedialog.askopenfilename(
            title="ì¸ë„¤ì¼ ë¶„ì„ ì—‘ì…€ ì„ íƒ (I2 ë²„ì „ë§Œ ê°€ëŠ¥)",
            filetypes=[("Excel", "*.xlsx;*.xls")]
        )
        if p:
            base_name = os.path.basename(p)
            # I2 í¬í•¨ ì—¬ë¶€ ê²€ì¦
            if not re.search(r"_I2", base_name, re.IGNORECASE):
                messagebox.showerror(
                    "ì˜¤ë¥˜", 
                    f"ì´ ë„êµ¬ëŠ” I2 ë²„ì „ì˜ ì—‘ì…€ íŒŒì¼ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                    f"ì„ íƒí•œ íŒŒì¼: {base_name}\n"
                    f"íŒŒì¼ëª…ì— '_I2' íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
                )
                return
            
            self.src_file_var.set(p)
            self.append_log(f"íŒŒì¼ ì„ íƒë¨: {base_name} (I2)")

    def _create_jsonl_only(self):
        """JSONL ìƒì„±ë§Œ ìˆ˜í–‰ (ë¶„ë¦¬ëœ ë²„íŠ¼)"""
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        t = threading.Thread(target=self._run_create_jsonl)
        t.daemon = True
        t.start()
    
    def _upload_batch_from_jsonl(self):
        """JSONL íŒŒì¼ì—ì„œ ë°°ì¹˜ ì—…ë¡œë“œë§Œ ìˆ˜í–‰ (ë¶„ë¦¬ëœ ë²„íŠ¼)"""
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # JSONL íŒŒì¼ ê²½ë¡œ í™•ì¸
        jsonl_path = self.jsonl_file_var.get().strip()
        if not jsonl_path or not os.path.exists(jsonl_path):
            messagebox.showwarning("ì˜¤ë¥˜", "JSONL íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\n\në¨¼ì € 'JSONL ìƒì„±ë§Œ' ë²„íŠ¼ì„ ì‹¤í–‰í•˜ê±°ë‚˜, ê¸°ì¡´ JSONL íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # ì›ë³¸ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ í™•ì¸ (ë©”íƒ€ë°ì´í„° ì €ì¥ìš©)
        if not self.src_file_var.get():
            # JSONL íŒŒì¼ëª…ì—ì„œ ì›ë³¸ íŒŒì¼ ê²½ë¡œ ì¶”ë¡  ì‹œë„
            jsonl_basename = os.path.basename(jsonl_path)
            if "_img_analysis_batch_input.jsonl" in jsonl_basename:
                base_path = jsonl_path.replace("_img_analysis_batch_input.jsonl", "")
                # ê°€ëŠ¥í•œ í™•ì¥ì ì‹œë„
                for ext in [".xlsx", ".xls"]:
                    candidate = base_path + ext
                    if os.path.exists(candidate):
                        self.src_file_var.set(candidate)
                        break
        
        t = threading.Thread(target=self._run_upload_batch, args=(jsonl_path,))
        t.daemon = True
        t.start()

    def _start_create_batch(self):
        """í†µí•© ë²„íŠ¼: JSONL ìƒì„± ë° ë°°ì¹˜ ì—…ë¡œë“œ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)"""
        if not self.api_key_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        if not self.src_file_var.get():
            messagebox.showwarning("ì˜¤ë¥˜", "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        t = threading.Thread(target=self._run_create_batch)
        t.daemon = True
        t.start()

    def _run_create_jsonl(self):
        """JSONL ìƒì„±ë§Œ ìˆ˜í–‰í•˜ëŠ” ë¡œì§"""
        src = self.src_file_var.get().strip()
        model_name = self.model_var.get().strip() or "gpt-5-mini"
        reasoning_effort = self.effort_var.get().strip() or "low"
        resize_mode = self.resize_mode_var.get().strip() or "B"
        
        # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œì— ë”°ë¥¸ target_width ê²°ì •
        target_width = None
        if resize_mode == "B":
            target_width = 512
        elif resize_mode == "C":
            target_width = 448
        # resize_mode == "A"ì´ë©´ target_widthëŠ” None (ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨)
        
        try:
            self.append_log(f"ì—‘ì…€ ë¡œë“œ ì¤‘... {os.path.basename(src)}")
            df = pd.read_excel(src)
            
            if "IMG_S1_ëˆ„ë¼" not in df.columns:
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼(IMG_S1_ëˆ„ë¼)ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ìºì‹± ëª¨ë“œ í™•ì¸ ë° ë¡œê·¸
            if CACHE_MODE_CORE:
                self.append_log(f"[INFO] ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ëª¨ë“œ í™œì„±í™” (IMG_analysis_core_Casche.py)")
            else:
                self.append_log(f"[INFO] âš ï¸ ì¼ë°˜ ëª¨ë“œ (IMG_analysis_core.py) - ìºì‹± ìµœì í™” ë¯¸ì ìš©")
            
            self.append_log(f"ì„¤ì •: ëª¨ë¸={model_name}, effort={reasoning_effort}, ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ={resize_mode}")

            # ì „ì²´ ëŒ€ìƒ ìš”ì²­ ìˆ˜ ê³„ì‚°
            target_rows = 0
            result_cols = [
                "view_point", "subject_position", "subject_size", "lighting_condition",
                "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
                "bg_layout_hint_en"
            ]
            
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get():
                    has_result = False
                    val = str(row.get("view_point", "")).strip()
                    if val and val != "nan" and val:
                        has_result = True
                    if has_result:
                        continue
                
                # 'bad' ë¼ë²¨ ì²´í¬
                if self.skip_bad_label_var.get():
                    human_label = str(row.get("IMG_S1_íœ´ë¨¼ë¼ë²¨", "")).strip().lower()
                    ai_label = str(row.get("IMG_S1_AIë¼ë²¨", "")).strip().lower()
                    
                    if human_label == "bad" or ai_label == "bad":
                        continue
                
                # ëˆ„ë¼ ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
                thumbnail_path = str(row.get("IMG_S1_ëˆ„ë¼", "")).strip()
                if not thumbnail_path or thumbnail_path == "nan":
                    continue
                
                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not os.path.exists(thumbnail_path):
                    continue
                
                target_rows += 1

            # ë²„í‚· ìˆ˜ ê³„ì‚° (í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™”)
            if CACHE_MODE_CORE and target_rows > 0:
                PROMPT_CACHE_BUCKETS = 1
                self.append_log(f"[INFO] í”„ë¡¬í”„íŠ¸ ìºì‹±: í‚¤ ê³ ì • ì „ëµ ì‚¬ìš© (ëª¨ë“  ìš”ì²­ì´ 'img_analysis_v1' í‚¤ ê³µìœ )")
                self.append_log(f"[INFO] ì˜ˆìƒ ìš”ì²­ ìˆ˜: {target_rows}ê°œ, ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ ì˜ˆìƒ")
            else:
                PROMPT_CACHE_BUCKETS = 1

            jsonl_lines = []
            skipped_cnt = 0
            seen_custom_ids = set()
            duplicate_count = 0
            
            # JSONL ë¼ì¸ ìƒì„±
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get():
                    has_result = False
                    val = str(row.get("view_point", "")).strip()
                    if val and val != "nan" and val:
                        has_result = True
                    if has_result:
                        skipped_cnt += 1
                        continue
                
                # 'bad' ë¼ë²¨ ì²´í¬
                if self.skip_bad_label_var.get():
                    human_label = str(row.get("IMG_S1_íœ´ë¨¼ë¼ë²¨", "")).strip().lower()
                    ai_label = str(row.get("IMG_S1_AIë¼ë²¨", "")).strip().lower()
                    
                    if human_label == "bad" or ai_label == "bad":
                        self.append_log(f"[Row {idx+1}] 'bad' ë¼ë²¨ì´ ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤. (íœ´ë¨¼ë¼ë²¨: {row.get('IMG_S1_íœ´ë¨¼ë¼ë²¨', '')}, AIë¼ë²¨: {row.get('IMG_S1_AIë¼ë²¨', '')})")
                        skipped_cnt += 1
                        continue
                
                # ëˆ„ë¼ ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
                thumbnail_path = str(row.get("IMG_S1_ëˆ„ë¼", "")).strip()
                if not thumbnail_path or thumbnail_path == "nan":
                    skipped_cnt += 1
                    continue
                
                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not os.path.exists(thumbnail_path):
                    self.append_log(f"[Row {idx+1}] ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {thumbnail_path}")
                    skipped_cnt += 1
                    continue
                
                try:
                    # ìºì‹± ìµœì í™” ëª¨ë“œ: build_analysis_batch_payload ì‚¬ìš©
                    if CACHE_MODE_CORE and build_analysis_batch_payload:
                        request_obj = build_analysis_batch_payload(
                            row_index=idx,
                            image_path=thumbnail_path,
                            model_name=model_name,
                            reasoning_effort=reasoning_effort,
                            use_cache_optimization=True,
                            max_width=target_width,
                            log_func=self.append_log
                        )
                        
                        if request_obj and "body" in request_obj:
                            custom_id = request_obj.get("custom_id", f"row_{idx}")
                            
                            # ì¤‘ë³µ custom_id ì²´í¬
                            if custom_id in seen_custom_ids:
                                duplicate_count += 1
                                continue
                            seen_custom_ids.add(custom_id)
                            
                            # prompt_cache_key: í‚¤ ê³ ì • ì „ëµ
                            request_obj["body"]["prompt_cache_key"] = "img_analysis_v1"
                            
                            # prompt_cache_retention ì„¤ì •
                            if model_name in ["gpt-5.1", "gpt-5.1-codex", "gpt-5.1-codex-mini", "gpt-5.1-chat-latest", "gpt-5", "gpt-5-codex", "gpt-4.1"]:
                                request_obj["body"]["prompt_cache_retention"] = "extended"
                            elif model_name not in ["gpt-5-mini", "gpt-5-nano"]:
                                request_obj["body"]["prompt_cache_retention"] = "in_memory"
                            
                            # text.format: JSON ì¶œë ¥ ê°•ì œ
                            request_obj["body"]["text"] = {"format": {"type": "json_object"}}
                    else:
                        # ì¼ë°˜ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ ìœ ì§€
                        messages = build_analysis_messages(thumbnail_path)
                        
                        body = {
                            "model": model_name,
                            "messages": messages,
                        }
                        
                        is_reasoning = any(x in model_name for x in ["gpt-5", "o1", "o3"])
                        if is_reasoning and reasoning_effort != "none":
                            body["reasoning_effort"] = reasoning_effort

                        request_obj = {
                            "custom_id": f"row_{idx}",
                            "method": "POST",
                            "url": "/v1/chat/completions",
                            "body": body
                        }
                    
                    jsonl_lines.append(json.dumps(request_obj, ensure_ascii=False))
                except Exception as e:
                    self.append_log(f"[Row {idx+1}] ìŠ¤í‚µ: {e}")
                    skipped_cnt += 1
                    continue
            
            if duplicate_count > 0:
                self.append_log(f"[WARN] âš ï¸ ì¤‘ë³µ ìš”ì²­ {duplicate_count}ê°œê°€ ê°ì§€ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if not jsonl_lines:
                self.append_log("ìƒì„±í•  ìš”ì²­ ì—†ìŒ.")
                self.after(0, lambda: messagebox.showinfo("ì•Œë¦¼", "ìƒì„±í•  ìš”ì²­ì´ ì—†ìŠµë‹ˆë‹¤."))
                return

            # JSONL íŒŒì¼ ì €ì¥
            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_img_analysis_batch_input.jsonl"
            with open(jsonl_path, "w", encoding="utf-8") as f:
                f.write("\n".join(jsonl_lines))
            
            self.append_log(f"âœ… JSONL ìƒì„± ì™„ë£Œ: {len(jsonl_lines)}ê±´ (ìŠ¤í‚µ {skipped_cnt}ê±´)")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ìš”ì²­ ìˆ˜: {len(jsonl_lines)}ê°œ")
            
            # JSONL íŒŒì¼ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ì €ì¥
            self.after(0, lambda: self.jsonl_file_var.set(jsonl_path))
            
            # ì„±ê³µ ë©”ì‹œì§€
            self.after(0, lambda: messagebox.showinfo(
                "JSONL ìƒì„± ì™„ë£Œ",
                f"JSONL íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                f"íŒŒì¼: {os.path.basename(jsonl_path)}\n"
                f"ìš”ì²­ ìˆ˜: {len(jsonl_lines)}ê±´\n"
                f"í¬ê¸°: {jsonl_size_mb:.2f} MB\n\n"
                f"ì´ì œ 'ë°°ì¹˜ ì—…ë¡œë“œ' ë²„íŠ¼ì„ ëˆŒëŸ¬ ì—…ë¡œë“œí•˜ì„¸ìš”."
            ))
            
        except Exception as e:
            self.append_log(f"âŒ JSONL ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            self.after(0, lambda: messagebox.showerror("ì˜¤ë¥˜", f"JSONL ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}"))
    
    def _run_upload_batch(self, jsonl_path):
        """JSONL íŒŒì¼ë¡œë¶€í„° ë°°ì¹˜ ì—…ë¡œë“œë§Œ ìˆ˜í–‰í•˜ëŠ” ë¡œì§"""
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        model_name = self.model_var.get().strip() or "gpt-5-mini"
        reasoning_effort = self.effort_var.get().strip() or "low"
        
        try:
            if not os.path.exists(jsonl_path):
                raise FileNotFoundError(f"JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")
            
            self.append_log(f"JSONL íŒŒì¼ ë¡œë“œ ì¤‘... {os.path.basename(jsonl_path)}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB")
            
            MAX_FILE_SIZE_MB = 180
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            import httpx
            timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
            client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸°ê°€ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤: {jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB")
                batch_ids = self._create_batch_chunks(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src if src else "",
                    model_name=model_name,
                    effort=reasoning_effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=999999,
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                self.after(0, lambda: messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}"))
            else:
                # ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src if src else "",
                    model_name=model_name,
                    reasoning_effort=reasoning_effort,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src if src else "",
                    jsonl_path=jsonl_path,
                    model=model_name,
                    effort=reasoning_effort,
                    status=batch.status,
                    output_file_id=None,
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
                if src:
                    try:
                        root_name = get_root_filename(src)
                        JobManager.update_status(root_name, img_s3_1_msg="I3-1 (ì§„í–‰ì¤‘)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> I3-1 (ì§„í–‰ì¤‘)")
                    except Exception:
                        pass
                
                self.after(0, lambda bid=batch_id: messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {bid}"))
            
            self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])

        except Exception as e:
            self.append_log(f"âŒ ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            self.after(0, lambda err=str(e): messagebox.showerror("ì˜¤ë¥˜", f"ë°°ì¹˜ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{err}"))

    def _run_create_batch(self):
        key = self.api_key_var.get().strip()
        src = self.src_file_var.get().strip()
        
        model_name = self.model_var.get().strip() or "gpt-5-mini"
        reasoning_effort = self.effort_var.get().strip() or "low"
        resize_mode = self.resize_mode_var.get().strip() or "B"
        
        # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œì— ë”°ë¥¸ target_width ê²°ì •
        target_width = None
        if resize_mode == "B":
            target_width = 512
        elif resize_mode == "C":
            target_width = 448
        # resize_mode == "A"ì´ë©´ target_widthëŠ” None (ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨)
        
        try:
            client = OpenAI(api_key=key)
            self.append_log(f"ì—‘ì…€ ë¡œë“œ ì¤‘... {os.path.basename(src)}")
            df = pd.read_excel(src)
            
            if "IMG_S1_ëˆ„ë¼" not in df.columns:
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼(IMG_S1_ëˆ„ë¼)ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ìºì‹± ëª¨ë“œ í™•ì¸ ë° ë¡œê·¸
            if CACHE_MODE_CORE:
                self.append_log(f"[INFO] ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ëª¨ë“œ í™œì„±í™” (IMG_analysis_core_Casche.py)")
            else:
                self.append_log(f"[INFO] âš ï¸ ì¼ë°˜ ëª¨ë“œ (IMG_analysis_core.py) - ìºì‹± ìµœì í™” ë¯¸ì ìš©")
            
            self.append_log(f"ì„¤ì •: ëª¨ë¸={model_name}, effort={reasoning_effort}, ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ={resize_mode}")

            # ë¨¼ì € ì „ì²´ ëŒ€ìƒ ìš”ì²­ ìˆ˜ë¥¼ ê³„ì‚° (ë²„í‚· ìˆ˜ ê²°ì •ìš©)
            target_rows = 0
            result_cols = [
                "view_point", "subject_position", "subject_size", "lighting_condition",
                "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
                "bg_layout_hint_en"
            ]
            
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get():
                    has_result = False
                    val = str(row.get("view_point", "")).strip()
                    if val and val != "nan" and val:
                        has_result = True
                    if has_result:
                        continue
                
                # 'bad' ë¼ë²¨ ì²´í¬ (ê¸°ë³¸ê°’: True)
                if self.skip_bad_label_var.get():
                    human_label = str(row.get("IMG_S1_íœ´ë¨¼ë¼ë²¨", "")).strip().lower()
                    ai_label = str(row.get("IMG_S1_AIë¼ë²¨", "")).strip().lower()
                    
                    if human_label == "bad" or ai_label == "bad":
                        continue
                
                # ëˆ„ë¼ ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
                thumbnail_path = str(row.get("IMG_S1_ëˆ„ë¼", "")).strip()
                if not thumbnail_path or thumbnail_path == "nan":
                    continue
                
                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not os.path.exists(thumbnail_path):
                    continue
                
                target_rows += 1

            # ë²„í‚· ìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚° (ëª¨ë“  ìš”ì²­ì— ë™ì¼í•˜ê²Œ ì ìš©)
            if CACHE_MODE_CORE and target_rows > 0:
                # [ë²„í‚· ìˆ˜ ê³„ì‚° ì „ëµ - ì£¼ì˜: OpenAI ê³µì‹ ê¸°ì¤€ì´ ì•„ë‹Œ ì¶”ì •ì¹˜ì…ë‹ˆë‹¤]
                # [í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™” ì „ëµ - í‚¤ ê³ ì •]
                # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼: ë²„í‚· ë¶„ì‚° ì‹œ ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ì•„ì§ (10% ìˆ˜ì¤€)
                # í•´ê²°ì±…: prompt_cache_keyë¥¼ í•˜ë‚˜ë¡œ ê³ ì •í•˜ì—¬ ëª¨ë“  ìš”ì²­ì´ ê°™ì€ ìºì‹œ í’€ ê³µìœ 
                # Batch APIëŠ” 24ì‹œê°„ì— ê±¸ì³ ì²˜ë¦¬ë˜ë¯€ë¡œ overflow ìš°ë ¤ëŠ” ë‚®ìŒ
                PROMPT_CACHE_BUCKETS = 1
                
                self.append_log(f"[INFO] í”„ë¡¬í”„íŠ¸ ìºì‹±: í‚¤ ê³ ì • ì „ëµ ì‚¬ìš© (ëª¨ë“  ìš”ì²­ì´ 'img_analysis_v1' í‚¤ ê³µìœ )")
                self.append_log(f"[INFO] ì˜ˆìƒ ìš”ì²­ ìˆ˜: {target_rows}ê°œ, ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ ì˜ˆìƒ")
            else:
                PROMPT_CACHE_BUCKETS = 1

            jsonl_lines = []
            skipped_cnt = 0
            seen_custom_ids = set()
            duplicate_count = 0
            
            for idx, row in df.iterrows():
                # ìŠ¤í‚µ ë¡œì§
                if self.skip_exist_var.get():
                    has_result = False
                    val = str(row.get("view_point", "")).strip()
                    if val and val != "nan" and val:
                        has_result = True
                    if has_result:
                        skipped_cnt += 1
                        continue
                
                # 'bad' ë¼ë²¨ ì²´í¬ (ê¸°ë³¸ê°’: True)
                if self.skip_bad_label_var.get():
                    human_label = str(row.get("IMG_S1_íœ´ë¨¼ë¼ë²¨", "")).strip().lower()
                    ai_label = str(row.get("IMG_S1_AIë¼ë²¨", "")).strip().lower()
                    
                    if human_label == "bad" or ai_label == "bad":
                        self.append_log(f"[Row {idx+1}] 'bad' ë¼ë²¨ì´ ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤. (íœ´ë¨¼ë¼ë²¨: {row.get('IMG_S1_íœ´ë¨¼ë¼ë²¨', '')}, AIë¼ë²¨: {row.get('IMG_S1_AIë¼ë²¨', '')})")
                        skipped_cnt += 1
                        continue
                
                # ëˆ„ë¼ ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
                thumbnail_path = str(row.get("IMG_S1_ëˆ„ë¼", "")).strip()
                if not thumbnail_path or thumbnail_path == "nan":
                    skipped_cnt += 1
                    continue
                
                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not os.path.exists(thumbnail_path):
                    self.append_log(f"[Row {idx+1}] ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {thumbnail_path}")
                    skipped_cnt += 1
                    continue
                
                try:
                    # ìºì‹± ìµœì í™” ëª¨ë“œ: build_analysis_batch_payload ì‚¬ìš©
                    if CACHE_MODE_CORE and build_analysis_batch_payload:
                        request_obj = build_analysis_batch_payload(
                            row_index=idx,
                            image_path=thumbnail_path,
                            model_name=model_name,
                            reasoning_effort=reasoning_effort,
                            use_cache_optimization=True
                        )
                        
                        if request_obj and "body" in request_obj:
                            custom_id = request_obj.get("custom_id", f"row_{idx}")
                            
                            # ì¤‘ë³µ custom_id ì²´í¬
                            if custom_id in seen_custom_ids:
                                duplicate_count += 1
                                continue
                            seen_custom_ids.add(custom_id)
                            
                            # prompt_cache_key: í‚¤ ê³ ì • ì „ëµ (ëª¨ë“  ìš”ì²­ì´ ë™ì¼í•œ í‚¤ ì‚¬ìš©)
                            request_obj["body"]["prompt_cache_key"] = "img_analysis_v1"
                            
                            # prompt_cache_retention: ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                            # Extended retention ì§€ì› ëª¨ë¸: gpt-5.1, gpt-5.1-codex, gpt-5.1-codex-mini, gpt-5.1-chat-latest, gpt-5, gpt-5-codex, gpt-4.1
                            # gpt-5-mini, gpt-5-nanoëŠ” prompt_cache_retention íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
                            if model_name in ["gpt-5.1", "gpt-5.1-codex", "gpt-5.1-codex-mini", "gpt-5.1-chat-latest", "gpt-5", "gpt-5-codex", "gpt-4.1"]:
                                request_obj["body"]["prompt_cache_retention"] = "extended"  # 24ì‹œê°„ retention
                            elif model_name not in ["gpt-5-mini", "gpt-5-nano"]:
                                # ê¸°íƒ€ ëª¨ë¸ì€ in-memory ì‚¬ìš© (5~10ë¶„ inactivity, ìµœëŒ€ 1ì‹œê°„)
                                request_obj["body"]["prompt_cache_retention"] = "in_memory"
                            
                            # text.format: JSON ì¶œë ¥ ê°•ì œ (Structured Outputs)
                            request_obj["body"]["text"] = {"format": {"type": "json_object"}}
                    else:
                        # ì¼ë°˜ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ ìœ ì§€
                        messages = build_analysis_messages(thumbnail_path)
                        
                        body = {
                            "model": model_name,
                            "messages": messages,
                        }
                        
                        # gpt-5 ê³„ì—´ì€ reasoning_effort ì‚¬ìš©
                        is_reasoning = any(x in model_name for x in ["gpt-5", "o1", "o3"])
                        if is_reasoning and reasoning_effort != "none":
                            body["reasoning_effort"] = reasoning_effort

                        request_obj = {
                            "custom_id": f"row_{idx}",
                            "method": "POST",
                            "url": "/v1/chat/completions",
                            "body": body
                        }
                    
                    jsonl_lines.append(json.dumps(request_obj, ensure_ascii=False))
                except Exception as e:
                    self.append_log(f"[Row {idx+1}] ìŠ¤í‚µ: {e}")
                    skipped_cnt += 1
                    continue
            
            if duplicate_count > 0:
                self.append_log(f"[WARN] âš ï¸ ì¤‘ë³µ ìš”ì²­ {duplicate_count}ê°œê°€ ê°ì§€ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if not jsonl_lines:
                self.append_log("ìƒì„±í•  ìš”ì²­ ì—†ìŒ.")
                return

            base, _ = os.path.splitext(src)
            jsonl_path = f"{base}_img_analysis_batch_input.jsonl"
            with open(jsonl_path, "w", encoding="utf-8") as f:
                f.write("\n".join(jsonl_lines))
            
            self.append_log(f"JSONL ìƒì„± ì™„ë£Œ: {len(jsonl_lines)}ê±´ (ìŠ¤í‚µ {skipped_cnt}ê±´)")
            
            # JSONL íŒŒì¼ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ì €ì¥ (í†µí•© ê¸°ëŠ¥ì—ì„œë„)
            self.after(0, lambda: self.jsonl_file_var.set(jsonl_path))
            
            # íŒŒì¼ í¬ê¸° ë° ìš”ì²­ ìˆ˜ í™•ì¸
            jsonl_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
            info = {
                'num_requests': len(jsonl_lines),
                'file_size_mb': jsonl_size_mb
            }
            self.append_log(f"[INFO] JSONL íŒŒì¼ í¬ê¸°: {jsonl_size_mb:.2f} MB, ìš”ì²­ ìˆ˜: {info['num_requests']}ê°œ")
            
            # ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ : 180MB ì´ìƒì´ë©´ ë¶„í•  ì²˜ë¦¬ (OpenAI Batch API ì œí•œ: 200MB)
            # ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨ (500ê°œ ì œí•œ ì œê±°)
            MAX_FILE_SIZE_MB = 180
            
            if jsonl_size_mb > MAX_FILE_SIZE_MB:
                self.append_log(f"[INFO] íŒŒì¼ í¬ê¸°ê°€ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤: {jsonl_size_mb:.2f}MB > {MAX_FILE_SIZE_MB}MB")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client_with_timeout = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch_ids = self._create_batch_chunks(
                    client=client_with_timeout,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    effort=reasoning_effort,
                    max_size_mb=MAX_FILE_SIZE_MB,
                    max_requests=999999,  # ìš”ì²­ ìˆ˜ ì œí•œ ê±°ì˜ ì œê±° (ìš©ëŸ‰ì´ ìš°ì„ )
                )
                self.append_log(f"âœ… ì´ {len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(batch_ids)}")
                self.after(0, lambda: messagebox.showinfo("ì„±ê³µ", f"{len(batch_ids)}ê°œì˜ ë°°ì¹˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n{', '.join(batch_ids)}"))
            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë°°ì¹˜ ìƒì„±
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
                import httpx
                timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=jsonl_path,
                    excel_path=src,
                    model_name=model_name,
                    reasoning_effort=reasoning_effort,
                )

                batch_id = batch.id
                self.append_log(f"âœ… ë°°ì¹˜ ì‹œì‘! ID: {batch_id}, status={batch.status}")

                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=src,
                    jsonl_path=jsonl_path,
                    model=model_name,
                    effort=reasoning_effort,
                    status=batch.status,
                    output_file_id=None,
                )

                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— I3-1 ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡ - img ìƒíƒœë§Œ ì—…ë°ì´íŠ¸ (text ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                try:
                    root_name = get_root_filename(src)
                    JobManager.update_status(root_name, img_s3_1_msg="I3-1 (ì§„í–‰ì¤‘)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> I3-1 (ì§„í–‰ì¤‘)")
                except Exception:
                    pass
                self.after(0, lambda bid=batch_id: messagebox.showinfo("ì„±ê³µ", f"ë°°ì¹˜ ì‹œì‘ë¨: {bid}"))
            
            self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])

        except Exception as e:
            self.append_log(f"ì—ëŸ¬: {e}")
            import traceback
            self.append_log(traceback.format_exc())
            self.after(0, lambda err=str(e): messagebox.showerror("ì—ëŸ¬", err))
    
    def _create_batch_from_jsonl(self, client, jsonl_path, excel_path, model_name, reasoning_effort):
        """JSONL íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë°°ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        if not os.path.exists(jsonl_path):
            raise FileNotFoundError(f"ì…ë ¥ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")

        with open(jsonl_path, "rb") as f:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•´ 10ë¶„ìœ¼ë¡œ ì„¤ì •
            up_file = client.files.create(file=f, purpose="batch", timeout=600)

        # ìºì‹± ëª¨ë“œì— ë”°ë¼ endpoint ê²°ì •
        endpoint = "/v1/responses" if CACHE_MODE_CORE else "/v1/chat/completions"

        batch = client.batches.create(
            input_file_id=up_file.id,
            endpoint=endpoint,
            completion_window="24h"
        )
        return batch
    
    def _create_batch_chunks(self, client, jsonl_path, excel_path, model_name, effort, max_size_mb=180, max_requests=999999):
        """
        í° JSONL íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ batch_group_idë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        """
        import uuid
        
        # ë°°ì¹˜ ê·¸ë£¹ ID ìƒì„± (ê°™ì€ ì—‘ì…€ì—ì„œ ë¶„í• ëœ ë°°ì¹˜ë“¤ì„ ë¬¶ìŒ)
        batch_group_id = f"group_{uuid.uuid4().hex[:8]}"
        
        # JSONL íŒŒì¼ ì½ê¸°
        requests = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    requests.append(json.loads(line))
        
        total_requests = len(requests)
        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚° (ìš©ëŸ‰ ê¸°ì¤€ë§Œ ì‚¬ìš©, ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
        original_file_size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)
        estimated_total_chunks = max(1, int(original_file_size_mb / max_size_mb) + 1)
        
        self.append_log(f"[INFO] ì´ {total_requests}ê°œ ìš”ì²­ì„ ë¶„í• í•©ë‹ˆë‹¤... (ì˜ˆìƒ: ì•½ {estimated_total_chunks}ê°œ ì²­í¬, ê·¸ë£¹ ID: {batch_group_id})")
        
        # base ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜ (ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹œ ì‚¬ìš©)
        base, ext = os.path.splitext(jsonl_path)
        
        batch_ids = []
        chunk_num = 0
        chunk_files_created = []  # ìƒì„±ëœ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì •ë¦¬ìš©)
        failed_chunk_files = []  # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ (ì¬ì‹œë„ìš©)
        
        i = 0
        while i < total_requests:
            chunk_num += 1
            chunk_requests = []
            chunk_size_bytes = 0  # ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì •í™•íˆ ê³„ì‚°
            
            # ì²­í¬ ìƒì„± (ìš©ëŸ‰ ê¸°ì¤€ ìš°ì„ , ìš”ì²­ ìˆ˜ëŠ” ìš©ëŸ‰ ì œí•œ ë‚´ì—ì„œ ê°€ëŠ¥í•œ ë§Œí¼ í¬í•¨)
            while i < total_requests:
                req_json = json.dumps(requests[i], ensure_ascii=False)
                req_size_bytes = len(req_json.encode('utf-8')) + 1  # +1 for newline
                
                # ìš©ëŸ‰ì´ ìš°ì„ : ë‹¤ìŒ ìš”ì²­ì„ ì¶”ê°€í•˜ë©´ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ ë§ˆì§„ 5% í¬í•¨)
                if chunk_size_bytes + req_size_bytes > max_size_mb * 1024 * 1024 * 0.95:
                    break
                
                chunk_requests.append(requests[i])
                chunk_size_bytes += req_size_bytes
                i += 1
            
            if not chunk_requests:
                break
            
            # ì²­í¬ JSONL íŒŒì¼ ìƒì„±
            chunk_jsonl_path = f"{base}_chunk{chunk_num:03d}{ext}"
            chunk_files_created.append(chunk_jsonl_path)
            
            with open(chunk_jsonl_path, "w", encoding="utf-8") as f:
                for req in chunk_requests:
                    f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            chunk_size_mb = os.path.getsize(chunk_jsonl_path) / (1024 * 1024)
            self.append_log(f"[INFO] ì²­í¬ {chunk_num}: {len(chunk_requests)}ê°œ ìš”ì²­, {chunk_size_mb:.2f} MB")
            
            # ë°°ì¹˜ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 3
            retry_count = 0
            batch_created = False
            
            while retry_count < max_retries and not batch_created:
                try:
                    self.append_log(f"[INFO] ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹œë„ ì¤‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                    batch = self._create_batch_from_jsonl(
                        client=client,
                        jsonl_path=chunk_jsonl_path,
                        excel_path=excel_path,
                        model_name=model_name,
                        reasoning_effort=effort,
                    )
                    
                    batch_id = batch.id
                    batch_ids.append(batch_id)
                    self.append_log(f"âœ… ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_id}")
                    
                    batch_created = True
                    
                    # ì‘ì—… ì´ë ¥ ê¸°ë¡ (ê·¸ë£¹ ì •ë³´ í¬í•¨)
                    upsert_batch_job(
                        batch_id=batch_id,
                        src_excel=excel_path,
                        jsonl_path=chunk_jsonl_path,
                        model=model_name,
                        effort=effort,
                        status=batch.status,
                        output_file_id=None,
                        batch_group_id=batch_group_id,  # ê·¸ë£¹ ID ì¶”ê°€
                        chunk_index=chunk_num,  # ì²­í¬ ë²ˆí˜¸
                        total_chunks=chunk_num,  # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì²­í¬ ìˆ˜ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨)
                    )
                except Exception as e:
                    error_str = str(e).lower()
                    is_token_limit_error = "enqueued token limit" in error_str or "token limit reached" in error_str
                    
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                        self.append_log(f"âš ï¸ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count}/{max_retries}): {e}")
                        if is_token_limit_error:
                            self.append_log(f"[WARN] í† í° ì œí•œ ì˜¤ë¥˜ ê°ì§€. ë” ê¸´ ëŒ€ê¸° ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            wait_time = max(wait_time, 30)  # í† í° ì œí•œ ì˜¤ë¥˜ëŠ” ìµœì†Œ 30ì´ˆ ëŒ€ê¸°
                        self.append_log(f"[INFO] {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        import time
                        time.sleep(wait_time)
                    else:
                        self.append_log(f"âŒ ì²­í¬ {chunk_num} ë°°ì¹˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}")
                        if is_token_limit_error:
                            self.append_log(f"[INFO] í† í° ì œí•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì¼ë¶€ ë°°ì¹˜ê°€ ì™„ë£Œëœ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                            self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼: {chunk_jsonl_path}")
                            self.append_log(f"[INFO] ë‚˜ì¤‘ì— '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì´ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        import traceback
                        self.append_log(traceback.format_exc())
                        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìœ ì§€ (ìˆ˜ë™ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                        failed_chunk_files.append({
                            "chunk_num": chunk_num,
                            "chunk_file": chunk_jsonl_path,
                            "error": str(e),
                            "is_token_limit": is_token_limit_error,
                            "excel_path": excel_path,
                            "model_name": model_name,
                            "effort": effort,
                            "batch_group_id": batch_group_id,
                        })
                        # í•˜ì§€ë§Œ ë°°ì¹˜ IDëŠ” ì¶”ê°€ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ batch_idsì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ
        
        # ëª¨ë“  ì²­í¬ ìƒì„± ì™„ë£Œ í›„, total_chunksë¥¼ ì‹¤ì œ ìƒì„±ëœ ë°°ì¹˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        actual_total_chunks = len(batch_ids)
        if actual_total_chunks > 0:
            self.append_log(f"[INFO] ì´ {actual_total_chunks}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ. ì‘ì—… ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
            jobs = load_batch_jobs()
            updated_count = 0
            for j in jobs:
                if j.get("batch_group_id") == batch_group_id:
                    j["total_chunks"] = actual_total_chunks
                    updated_count += 1
            if updated_count > 0:
                save_batch_jobs(jobs)
                self.append_log(f"[INFO] {updated_count}ê°œ ì‘ì—…ì˜ total_chunksë¥¼ {actual_total_chunks}ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.append_log(f"âš ï¸ ìƒì„±ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì„±ê³µí•œ ë°°ì¹˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²­í¬ íŒŒì¼ ì •ë¦¬ ì˜µì…˜ ì œê³µ (í˜„ì¬ëŠ” ìœ ì§€)
        # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ì€ ìˆ˜ë™ ì¬ì‹œë„ë¥¼ ìœ„í•´ ìœ ì§€
        if actual_total_chunks < chunk_num:
            failed_chunks = chunk_num - actual_total_chunks
            self.append_log(f"âš ï¸ {failed_chunks}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²­í¬ íŒŒì¼ì€ ìœ ì§€ë©ë‹ˆë‹¤.")
            
            # ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡ì„ ëª…í™•íˆ í‘œì‹œ
            if failed_chunk_files:
                self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ íŒŒì¼ ëª©ë¡:")
                for failed_info in failed_chunk_files:
                    self.append_log(f"  - ì²­í¬ {failed_info['chunk_num']}: {os.path.basename(failed_info['chunk_file'])}")
                    if failed_info['is_token_limit']:
                        self.append_log(f"    â†’ í† í° ì œí•œ ì˜¤ë¥˜. ì¼ë¶€ ë°°ì¹˜ ì™„ë£Œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
                
                # ì‹¤íŒ¨ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
                failed_info_path = f"{base}_failed_chunks.json"
                try:
                    with open(failed_info_path, "w", encoding="utf-8") as f:
                        json.dump(failed_chunk_files, f, ensure_ascii=False, indent=2)
                    self.append_log(f"[INFO] ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ ì €ì¥: {os.path.basename(failed_info_path)}")
                    self.append_log(f"[INFO] ë‚˜ì¤‘ì— ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤íŒ¨í•œ ì²­í¬ë§Œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    # GUIì— ìë™ìœ¼ë¡œ ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ì•Œë¦¼
                    self.after(0, lambda: self._handle_failed_chunks(failed_info_path, failed_chunk_files))
                except Exception as e:
                    self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
        try:
            root_name = get_root_filename(excel_path)
            JobManager.update_status(root_name, img_s3_1_msg="I3-1 (ì§„í–‰ì¤‘)")
            self.append_log(f"[INFO] ëŸ°ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸: {root_name} -> I3-1 (ì§„í–‰ì¤‘)")
        except Exception:
            pass
        
        return batch_ids

    # ----------------------------------------------------
    # Tab 2: Manage (List & Trash)
    # ----------------------------------------------------
    def _init_tab_manage(self):
        container = ttk.Frame(self.tab_manage, padding=10)
        container.pack(fill='both', expand=True)

        sub_tabs = ttk.Notebook(container)
        sub_tabs.pack(fill='both', expand=True)
        
        self.sub_active = ttk.Frame(sub_tabs, padding=10)
        self.sub_archive = ttk.Frame(sub_tabs, padding=10)
        
        sub_tabs.add(self.sub_active, text=" â–¶ ì§„í–‰ì¤‘ / ì™„ë£Œ (Active) ")
        sub_tabs.add(self.sub_archive, text=" ğŸ—‘ íœ´ì§€í†µ (Archive) ")
        
        # --- Active Tab UI ---
        f_ctrl = ttk.Frame(self.sub_active)
        f_ctrl.pack(fill='x', pady=(0, 10))
        
        ttk.Button(f_ctrl, text="ğŸ”„ ì„ íƒ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_ctrl, text="ğŸ“¥ ì„ íƒ ì¼ê´„ ë³‘í•©", command=self._merge_selected, style="Primary.TButton").pack(side='left', padx=10)
        ttk.Button(f_ctrl, text="ğŸ—‘ íœ´ì§€í†µ ì´ë™", command=self._archive_selected, style="Danger.TButton").pack(side='right', padx=2)
        
        # ì»¬ëŸ¼ ì •ì˜: batch_id | excel_name | memo | status | created | completed | model | effort | counts | group
        cols = ("batch_id", "excel_name", "memo", "status", "created", "completed", "model", "effort", "counts", "group")
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš© (íŠ¸ë¦¬ ì•„ì´ì½˜ + ì»¬ëŸ¼ í—¤ë”)
        self.tree_active = ttk.Treeview(self.sub_active, columns=cols, show='tree headings', height=12, selectmode='extended')
        
        self.tree_active.tag_configure('odd', background=COLOR_WHITE)
        self.tree_active.tag_configure('even', background='#F2F7FF')
        self.tree_active.tag_configure('group', background='#E8F4FD')
        self.tree_active.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_active.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_active.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_active.heading("memo", text="ë©”ëª¨")
        self.tree_active.heading("status", text="ìƒíƒœ")
        self.tree_active.heading("created", text="ìƒì„±ì¼ì‹œ")
        self.tree_active.heading("completed", text="ì™„ë£Œì¼ì‹œ")
        self.tree_active.heading("model", text="ëª¨ë¸")
        self.tree_active.heading("effort", text="Effort")
        self.tree_active.heading("counts", text="ì™„ë£Œ/ì „ì²´")
        self.tree_active.heading("group", text="ê·¸ë£¹")
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree_active.column("batch_id", width=180, anchor="w")
        self.tree_active.column("excel_name", width=200, anchor="w")
        self.tree_active.column("memo", width=150, anchor="w")
        self.tree_active.column("status", width=80, anchor="center")
        self.tree_active.column("created", width=120, anchor="center")
        self.tree_active.column("completed", width=120, anchor="center")
        self.tree_active.column("model", width=80, anchor="center")
        self.tree_active.column("effort", width=60, anchor="center")
        self.tree_active.column("counts", width=80, anchor="center")
        self.tree_active.column("group", width=80, anchor="center")
        
        self.tree_active.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ê·¸ë£¹ ì ‘ê¸°/í¼ì¹˜ê¸° ë²„íŠ¼
        f_group_ctrl = ttk.Frame(self.sub_active)
        f_group_ctrl.pack(fill='x', padx=5, pady=(0, 5))
        ttk.Button(f_group_ctrl, text="ğŸ“‚ ëª¨ë“  ê·¸ë£¹ í¼ì¹˜ê¸°", command=lambda: self._expand_all_groups(self.tree_active)).pack(side='left', padx=2)
        ttk.Button(f_group_ctrl, text="ğŸ“ ëª¨ë“  ê·¸ë£¹ ì ‘ê¸°", command=lambda: self._collapse_all_groups(self.tree_active)).pack(side='left', padx=2)
        
        # ìš°í´ë¦­ ë©”ë‰´
        self.menu_active = Menu(self, tearoff=0)
        self.menu_active.add_command(label="ìƒíƒœ ê°±ì‹ ", command=lambda: self._refresh_selected(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ê²°ê³¼ ë³‘í•©", command=self._merge_selected)
        self.menu_active.add_separator()
        self.menu_active.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_active))
        self.menu_active.add_separator()
        self.menu_active.add_command(label="íœ´ì§€í†µìœ¼ë¡œ ì´ë™", command=self._archive_selected)
        
        self.tree_active.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_active, self.menu_active))
        self.tree_active.bind("<Double-1>", self._on_tree_double_click)

        # --- Archive Tab UI ---
        f_arch_ctrl = ttk.Frame(self.sub_archive)
        f_arch_ctrl.pack(fill='x', pady=(0, 10))
        ttk.Button(f_arch_ctrl, text="â™»ï¸ ì„ íƒ ë³µêµ¬", command=self._restore_selected, style="Primary.TButton").pack(side='left')
        ttk.Button(f_arch_ctrl, text="ğŸ”¥ ì„ íƒ ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected, style="Danger.TButton").pack(side='right')
        
        # ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ„í•´ show='tree headings' ì‚¬ìš©
        self.tree_arch = ttk.Treeview(self.sub_archive, columns=cols, show='tree headings', height=12, selectmode='extended')
        self.tree_arch.tag_configure('odd', background=COLOR_WHITE)
        self.tree_arch.tag_configure('even', background='#FFF2F2')
        self.tree_arch.tag_configure('group_header', background='#C8E6C9', font=("ë§‘ì€ ê³ ë”•", 9, "bold"))
        
        # ì»¬ëŸ¼ í—¤ë” í•œê¸€í™”
        self.tree_arch.heading("batch_id", text="ë°°ì¹˜ ID")
        self.tree_arch.heading("excel_name", text="ì—‘ì…€ëª…")
        self.tree_arch.heading("memo", text="ë©”ëª¨")
        self.tree_arch.heading("status", text="ìƒíƒœ")
        self.tree_arch.heading("created", text="ìƒì„±ì¼ì‹œ")
        self.tree_arch.heading("completed", text="ì™„ë£Œì¼ì‹œ")
        self.tree_arch.heading("model", text="ëª¨ë¸")
        self.tree_arch.heading("effort", text="Effort")
        self.tree_arch.heading("counts", text="ì™„ë£Œ/ì „ì²´")
        self.tree_arch.heading("group", text="ê·¸ë£¹")
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree_arch.column("batch_id", width=180, anchor="w")
        self.tree_arch.column("excel_name", width=200, anchor="w")
        self.tree_arch.column("memo", width=150, anchor="w")
        self.tree_arch.column("status", width=80, anchor="center")
        self.tree_arch.column("created", width=120, anchor="center")
        self.tree_arch.column("completed", width=120, anchor="center")
        self.tree_arch.column("model", width=80, anchor="center")
        self.tree_arch.column("effort", width=60, anchor="center")
        self.tree_arch.column("counts", width=80, anchor="center")
        self.tree_arch.column("group", width=80, anchor="center")
        
        self.tree_arch.pack(fill='both', expand=True)
        
        # Archive ìš°í´ë¦­ ë©”ë‰´
        self.menu_arch = Menu(self, tearoff=0)
        self.menu_arch.add_command(label="ë³µêµ¬", command=self._restore_selected)
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="ë©”ëª¨ í¸ì§‘", command=lambda: self._edit_memo(self.tree_arch))
        self.menu_arch.add_separator()
        self.menu_arch.add_command(label="ì˜êµ¬ ì‚­ì œ", command=self._hard_delete_selected)
        self.tree_arch.bind("<Button-3>", lambda event: self._show_context_menu(event, self.tree_arch, self.menu_arch))
        
        self._load_jobs_all()
        self._load_archive_list()

    def _show_context_menu(self, event, tree, menu):
        item = tree.identify_row(event.y)
        if item:
            if item not in tree.selection():
                tree.selection_set(item)
            menu.post(event.x_root, event.y_root)

    def _expand_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ í¼ì¹©ë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=True)
    
    def _collapse_all_groups(self, tree):
        """ëª¨ë“  ê·¸ë£¹ í—¤ë”ë¥¼ ì ‘ìŠµë‹ˆë‹¤."""
        for item in tree.get_children():
            vals = tree.item(item)['values']
            if not vals or not vals[0]:  # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
                tree.item(item, open=False)
    
    def _get_selected_ids(self, tree):
        """
        ì„ íƒëœ í•­ëª©ì—ì„œ ë°°ì¹˜ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê·¸ë£¹ í—¤ë”ë¥¼ ì„ íƒí•˜ë©´ ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        selection = tree.selection()
        ids = []
        
        for item in selection:
            vals = tree.item(item)['values']
            batch_id = vals[0] if vals else ""
            
            # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš° (batch_idê°€ ë¹„ì–´ìˆìŒ)
            if not batch_id:
                # ê·¸ë£¹ í—¤ë”ì˜ ìì‹ ë…¸ë“œë“¤(ë°°ì¹˜ë“¤)ì„ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
                children = tree.get_children(item)
                for child in children:
                    child_vals = tree.item(child)['values']
                    if child_vals and child_vals[0]:
                        ids.append(child_vals[0])
            else:
                # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°
                ids.append(batch_id)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(ids))
    
    def _edit_memo(self, tree):
        """ì„ íƒëœ ë°°ì¹˜ì˜ ë©”ëª¨ë¥¼ í¸ì§‘í•©ë‹ˆë‹¤."""
        selection = tree.selection()
        if not selection:
            messagebox.showinfo("ì•ˆë‚´", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        item = selection[0]
        vals = tree.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        if not batch_id:
            messagebox.showinfo("ì•ˆë‚´", "ë°°ì¹˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # í˜„ì¬ ë©”ëª¨ ê°€ì ¸ì˜¤ê¸°
        jobs = load_batch_jobs()
        current_memo = ""
        for j in jobs:
            if j["batch_id"] == batch_id:
                current_memo = j.get("memo", "") or ""
                break
        
        # ë©”ëª¨ í¸ì§‘ ë‹¤ì´ì–¼ë¡œê·¸
        dialog = tk.Toplevel(self)
        dialog.title("ë©”ëª¨ í¸ì§‘")
        dialog.geometry("500x200")
        dialog.transient(self)
        dialog.grab_set()
        
        # ë°°ì¹˜ ID í‘œì‹œ
        tk.Label(dialog, text=f"ë°°ì¹˜ ID: {batch_id[:30]}...", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(10, 5))
        
        # ë©”ëª¨ ì…ë ¥ í•„ë“œ
        tk.Label(dialog, text="ë©”ëª¨:", font=("ë§‘ì€ ê³ ë”•", 9), anchor="w").pack(fill="x", padx=10, pady=(5, 0))
        memo_entry = tk.Text(dialog, height=5, width=60, font=("ë§‘ì€ ê³ ë”•", 9))
        memo_entry.pack(fill="both", expand=True, padx=10, pady=5)
        memo_entry.insert("1.0", current_memo)
        memo_entry.focus()
        
        # ë²„íŠ¼
        btn_frame = tk.Frame(dialog)
        btn_frame.pack(fill="x", padx=10, pady=10)
        
        def save_memo():
            new_memo = memo_entry.get("1.0", "end-1c").strip()
            upsert_batch_job(batch_id, memo=new_memo)
            self.append_log(f"[INFO] ë°°ì¹˜ {batch_id[:20]}... ë©”ëª¨ ì—…ë°ì´íŠ¸: {new_memo[:50]}...")
            self._load_jobs_all()
            self._load_archive_list()
            dialog.destroy()
            messagebox.showinfo("ì™„ë£Œ", "ë©”ëª¨ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        tk.Button(btn_frame, text="ì €ì¥", command=save_memo, bg="#4CAF50", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)
        tk.Button(btn_frame, text="ì·¨ì†Œ", command=dialog.destroy, bg="#f44336", fg="white", font=("ë§‘ì€ ê³ ë”•", 9), width=10).pack(side="right", padx=5)

    def _load_jobs_all(self):
        if not hasattr(self, 'tree_active'): return
        for i in self.tree_active.get_children(): self.tree_active.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            # ê·¸ë£¹ ë‚´ ë°°ì¹˜ë“¤ì„ ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs))
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0]
            created_at = first_job.get("created_at", "")
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    date_str = dt.strftime("%m-%d %H:%M")
                except:
                    date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
            else:
                date_str = "-"
            
            # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
            group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
            excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
            memo = first_job.get("memo", "") or "-"
            group_node = self.tree_active.insert("", "end", 
                text=group_header_text,
                values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                tags=('group_header',),
                open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
            )
            
            # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
            for j in group_jobs:
                cnt = "-"
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                group_display = f"ì²­í¬ {chunk_info}"
                excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                memo = j.get("memo", "") or "-"
                tag = 'group'
                self.tree_active.insert(group_node, "end", 
                    text=f"  â””â”€ {j['batch_id'][:20]}...",
                    values=(
                        j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, 
                        j.get("model"), j.get("effort", "-"), cnt, group_display
                    ), 
                    tags=(tag,))
                idx += 1
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_active.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), 
                    c_at, f_at, 
                    j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))
            idx += 1

    def _load_archive_list(self):
        if not hasattr(self, 'tree_arch'): return
        for i in self.tree_arch.get_children(): self.tree_arch.delete(i)
        jobs = load_batch_jobs()
        idx = 0
        
        # ê·¸ë£¹ë³„ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
        grouped_jobs = {}
        ungrouped_jobs = []
        for j in jobs:
            if not j.get("archived", False): continue
            group_id = j.get("batch_group_id")
            if group_id:
                if group_id not in grouped_jobs:
                    grouped_jobs[group_id] = []
                grouped_jobs[group_id].append(j)
            else:
                ungrouped_jobs.append(j)
        
        # ê·¸ë£¹ë³„ ë°°ì¹˜ í‘œì‹œ (ê³„ì¸µ êµ¬ì¡°)
        for group_id, group_jobs in sorted(grouped_jobs.items()):
            group_jobs.sort(key=lambda x: x.get("chunk_index", 0))
            total_chunks = group_jobs[0].get("total_chunks", len(group_jobs)) if group_jobs else len(group_jobs)
            
            # ê·¸ë£¹ ìƒíƒœ ì§‘ê³„
            statuses = {}
            total_completed = 0
            total_requests = 0
            for j in group_jobs:
                status = j.get("status", "unknown")
                statuses[status] = statuses.get(status, 0) + 1
                if "request_counts" in j and j["request_counts"]:
                    rc = j["request_counts"]
                    total_completed += rc.get('completed', 0)
                    total_requests += rc.get('total', 0)
            
            # ê·¸ë£¹ í—¤ë” ìƒì„± (ìš”ì•½ ì •ë³´ - ê°„ì†Œí™”)
            completed_count = statuses.get("completed", 0) + statuses.get("merged", 0)
            status_summary = f"ì™„ë£Œ: {completed_count}/{total_chunks}"
            if total_requests > 0:
                status_summary += f" | ìš”ì²­: {total_completed}/{total_requests}"
            
            # ê·¸ë£¹ ìƒì„± ë‚ ì§œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒì„± ì‹œê°„ ì‚¬ìš© - valuesì—ë§Œ í¬í•¨)
            first_job = group_jobs[0] if group_jobs else None
            if first_job:
                created_at = first_job.get("created_at", "")
                if created_at:
                    try:
                        dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                        date_str = dt.strftime("%m-%d %H:%M")
                    except:
                        date_str = created_at[:16].replace("T", " ") if len(created_at) >= 16 else created_at[:10]
                else:
                    date_str = "-"
                
                # ê·¸ë£¹ í—¤ë” (ë¶€ëª¨ ë…¸ë“œ) - ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸
                group_header_text = f"ğŸ“¦ {group_id[:12]} | {status_summary}"
                excel_name = get_excel_name_from_path(first_job.get("src_excel", ""))
                memo = first_job.get("memo", "") or "-"
                group_node = self.tree_arch.insert("", "end", 
                    text=group_header_text,
                    values=("", excel_name, memo, "", date_str, "", first_job.get("model", "-"), first_job.get("effort", "-"), "", f"ê·¸ë£¹ {total_chunks}ê°œ"),
                    tags=('group_header',),
                    open=False  # ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœ
                )
                
                # ê°œë³„ ë°°ì¹˜ (ìì‹ ë…¸ë“œ)
                for j in group_jobs:
                    cnt = "-"
                    if "request_counts" in j and j["request_counts"]:
                        rc = j["request_counts"]
                        cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
                    c_at = (j.get("created_at") or "")[:16].replace("T", " ")
                    f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
                    chunk_info = f"{j.get('chunk_index', 0)}/{total_chunks}"
                    group_display = f"ì²­í¬ {chunk_info}"
                    excel_name = get_excel_name_from_path(j.get("src_excel", ""))
                    memo = j.get("memo", "") or "-"
                    tag = 'group'
                    self.tree_arch.insert(group_node, "end", 
                        text=f"  â””â”€ {j['batch_id'][:20]}...",
                        values=(
                            j["batch_id"], excel_name, memo, j.get("status"), c_at, f_at, 
                            j.get("model"), j.get("effort", "-"), cnt, group_display
                        ), 
                        tags=(tag,))
                    idx += 1
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ í‘œì‹œ
        for j in ungrouped_jobs:
            cnt = "-"
            if "request_counts" in j and j["request_counts"]:
                rc = j["request_counts"]
                cnt = f"{rc.get('completed',0)}/{rc.get('total',0)}"
            c_at = (j.get("created_at") or "")[:16].replace("T", " ")
            f_at = (j.get("completed_at") or "")[:16].replace("T", " ")
            excel_name = get_excel_name_from_path(j.get("src_excel", ""))
            memo = j.get("memo", "") or "-"
            tag = 'even' if idx % 2 == 0 else 'odd'
            self.tree_arch.insert("", "end", 
                text=j["batch_id"][:30],
                values=(
                    j["batch_id"], excel_name, memo, j.get("status"), 
                    c_at, f_at, 
                    j.get("model"), j.get("effort", "-"), cnt, "-"
                ), 
                tags=(tag,))
            idx += 1

    # --- Batch Actions ---
    def _refresh_selected(self, tree):
        ids = self._get_selected_ids(tree)
        if not ids: return
        
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        jobs = load_batch_jobs()
        completed = [bid for bid in ids if next((x for x in jobs if x["batch_id"] == bid), {}).get("status") in ["completed", "merged"]]
        
        if completed:
            if messagebox.askyesno("í™•ì¸", f"{len(completed)}ê±´ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\nì œì™¸í•˜ê³  ë¯¸ì™„ë£Œ ê±´ë§Œ ê°±ì‹ í• ê¹Œìš”?"):
                ids = [i for i in ids if i not in completed]
        
        if not ids:
            messagebox.showinfo("ì·¨ì†Œ", "ê°±ì‹ í•  ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        t = threading.Thread(target=self._run_refresh_ids, args=(ids,))
        t.daemon = True
        t.start()

    def _run_refresh_ids(self, ids):
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return
        
        self.append_log(f"ì„ íƒëœ {len(ids)}ê±´ ê°±ì‹  ì¤‘...")
        success_cnt = 0
        fail_cnt = 0
        
        for bid in ids:
            try:
                remote = client.batches.retrieve(bid)
                rc = None
                if remote.request_counts:
                    rc = {"total": remote.request_counts.total, "completed": remote.request_counts.completed, "failed": remote.request_counts.failed}
                
                # expired ìƒíƒœë„ ê°±ì‹  ê°€ëŠ¥ (output_file_id í™•ì¸ì„ ìœ„í•´)
                # output_file_id ì¶”ì¶œ: ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
                output_file_id = getattr(remote, "output_file_id", None)
                if not output_file_id:
                    # output_file ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                    output_file = getattr(remote, "output_file", None)
                    if output_file:
                        if isinstance(output_file, str):
                            output_file_id = output_file
                        else:
                            output_file_id = getattr(output_file, "id", None) or getattr(output_file, "file_id", None)
                
                # model_dump()ë¥¼ í†µí•œ ì¶”ê°€ í™•ì¸ (ê°±ì‹  ì‹œì—ë„ ì ìš©)
                if not output_file_id and remote.status == "completed":
                    try:
                        if hasattr(remote, "model_dump"):
                            dump = remote.model_dump()
                            if "output_file_id" in dump and dump["output_file_id"]:
                                output_file_id = dump["output_file_id"]
                            elif "output_file" in dump:
                                of = dump["output_file"]
                                if isinstance(of, str) and of:
                                    output_file_id = of
                                elif isinstance(of, dict) and "id" in of:
                                    output_file_id = of["id"]
                    except Exception as e:
                        # ë””ë²„ê¹…: remote ê°ì²´ì˜ ëª¨ë“  ì†ì„± í™•ì¸
                        attrs = [attr for attr in dir(remote) if not attr.startswith('_')]
                        self.append_log(f"  [DEBUG] {bid}: model_dump ì‹¤íŒ¨, remote ì†ì„±: {', '.join(attrs[:15])}, ì˜¤ë¥˜: {e}")
                
                upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id, request_counts=rc)
                
                if remote.status == "expired" and output_file_id:
                    self.append_log(f"â„¹ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ì§€ë§Œ output_file_idê°€ ìˆìŠµë‹ˆë‹¤. (ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")
                elif remote.status == "completed":
                    if output_file_id:
                        self.append_log(f"âœ… {bid}: {remote.status} (output_file_id: {output_file_id})")
                    else:
                        self.append_log(f"âš ï¸ {bid}: {remote.status} (output_file_id ì—†ìŒ - ë””ë²„ê¹… í•„ìš”)")
                else:
                    self.append_log(f"âœ… {bid}: {remote.status}")
                success_cnt += 1
            except Exception as e:
                self.append_log(f"âŒ {bid} ê°±ì‹  ì‹¤íŒ¨: {e}")
                fail_cnt += 1
        
        self.after(0, lambda: [self._load_jobs_all(), self._load_archive_list()])
        if fail_cnt > 0:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}, ì‹¤íŒ¨: {fail_cnt})")
        else:
            self.append_log(f"ê°±ì‹  ì™„ë£Œ (ì„±ê³µ: {success_cnt}ê±´)")

    def _merge_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if not ids: return
        
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        jobs = load_batch_jobs()
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        group_ids = set()
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if job:
                group_id = job.get("batch_group_id")
                if group_id:
                    group_ids.add(group_id)
        
        # ê°™ì€ ê·¸ë£¹ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í¬í•¨
        all_target_ids = set(ids)
        for group_id in group_ids:
            if group_id:
                # completed ë˜ëŠ” expired ìƒíƒœì¸ ë°°ì¹˜ í¬í•¨ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
                group_batches = [j for j in jobs if j.get("batch_group_id") == group_id and j.get("status") in ["completed", "expired"]]
                for j in group_batches:
                    all_target_ids.add(j["batch_id"])
        
        if len(all_target_ids) > len(ids):
            group_info = f"\n\nê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ {len(all_target_ids) - len(ids)}ê°œê°€ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤."
        else:
            group_info = ""
        
        # completed, expired ë˜ëŠ” merged ìƒíƒœì¸ ë°°ì¹˜ ëª¨ë‘ ì„ íƒ ê°€ëŠ¥ (expired ìƒíƒœì—ì„œë„ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
        targets = [bid for bid in all_target_ids if next((j for j in jobs if j["batch_id"] == bid), {}).get("status") in ["completed", "expired", "merged"]]
        if not targets:
            messagebox.showinfo("ì•Œë¦¼", "ë³‘í•©í•  ìˆ˜ ìˆëŠ” 'completed', 'expired' ë˜ëŠ” 'merged' ìƒíƒœì˜ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        msg = f"ì„ íƒí•œ {len(targets)}ê±´ì„ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ?{group_info}"
        if messagebox.askyesno("ë³‘í•©", msg):
            t = threading.Thread(target=self._run_merge_multi, args=(targets,))
            t.daemon = True
            t.start()

    def _run_merge_multi(self, ids):
        """
        ì„ íƒëœ Batch ë“¤ì— ëŒ€í•´ ê²°ê³¼ JSONL ë‹¤ìš´ë¡œë“œ + ì—‘ì…€ ë³‘í•©ì„ ìˆ˜í–‰.
        ê°™ì€ ê·¸ë£¹ì˜ ë°°ì¹˜ë“¤ì€ í•˜ë‚˜ì˜ ì—‘ì…€ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.
        """
        key = self.api_key_var.get().strip()
        if not key:
            self.append_log("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            client = OpenAI(api_key=key)
        except Exception as e:
            self.append_log(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return
        
        jobs = load_batch_jobs()
        
        # ê·¸ë£¹ë³„ë¡œ ë°°ì¹˜ ë¶„ë¥˜
        groups_to_merge = {}  # {group_id: [batch_ids]}
        ungrouped_batches = []  # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤
        
        for bid in ids:
            job = next((j for j in jobs if j["batch_id"] == bid), None)
            if not job:
                continue
            
            group_id = job.get("batch_group_id")
            if group_id:
                if group_id not in groups_to_merge:
                    groups_to_merge[group_id] = []
                groups_to_merge[group_id].append(bid)
            else:
                ungrouped_batches.append(bid)
        
        success_cnt = 0
        total_cost = 0.0
        success_folders = set()  # ì„±ê³µí•œ íŒŒì¼ë“¤ì´ ì €ì¥ëœ í´ë”ë“¤ ì¶”ì 
        
        # ê·¸ë£¹ë³„ ë³‘í•© ì²˜ë¦¬
        for group_id, batch_ids in groups_to_merge.items():
            self.append_log(f"--- ê·¸ë£¹ ë³‘í•© ì‹œì‘: {group_id} ({len(batch_ids)}ê°œ ë°°ì¹˜) ---")
            try:
                # ê·¸ë£¹ ë‚´ ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì›ë³¸ ì—‘ì…€ ê²½ë¡œ ì‚¬ìš©
                first_job = next((j for j in jobs if j["batch_id"] == batch_ids[0]), None)
                if not first_job:
                    continue
                
                src_path = first_job.get("src_excel") or ""
                if not src_path or not os.path.exists(src_path):
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì›ë³¸ ì—‘ì…€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
                all_results_map = {}  # {custom_id: result_data}
                total_group_in = 0
                total_group_out = 0
                total_group_cost = 0.0
                total_group_cached = 0
                total_group_requests = 0
                total_group_cache_hits = 0
                model_name = first_job.get("model", "gpt-5-mini")
                
                # ì²­í¬ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ (chunk_indexê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ)
                def get_chunk_index(bid):
                    job = next((j for j in jobs if j["batch_id"] == bid), None)
                    if job:
                        idx = job.get("chunk_index")
                        return idx if idx is not None else 999999
                    return 999999
                
                batch_ids_sorted = sorted(batch_ids, key=get_chunk_index)
                
                for bid in batch_ids_sorted:
                    self.append_log(f"  [ê·¸ë£¹] ë°°ì¹˜ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    try:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if not local_job:
                            continue
                        
                        # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                        if local_job.get("status") == "merged":
                            self.append_log(f"  â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        # Batch ìƒíƒœ ë° ê²°ê³¼ íŒŒì¼ ID ì¡°íšŒ
                        remote = client.batches.retrieve(bid)
                        # output_file_id ì¶”ì¶œ: ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
                        output_file_id = getattr(remote, "output_file_id", None)
                        if not output_file_id:
                            # output_file ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                            output_file = getattr(remote, "output_file", None)
                            if output_file:
                                if isinstance(output_file, str):
                                    output_file_id = output_file
                                else:
                                    output_file_id = getattr(output_file, "id", None) or getattr(output_file, "file_id", None)
                        
                        # model_dump()ë¥¼ í†µí•œ ì¶”ê°€ í™•ì¸
                        if not output_file_id and remote.status == "completed":
                            try:
                                if hasattr(remote, "model_dump"):
                                    dump = remote.model_dump()
                                    if "output_file_id" in dump and dump["output_file_id"]:
                                        output_file_id = dump["output_file_id"]
                                        self.append_log(f"  [DEBUG] {bid}: model_dump()ì—ì„œ output_file_id ë°œê²¬: {output_file_id}")
                                    elif "output_file" in dump:
                                        of = dump["output_file"]
                                        if isinstance(of, str) and of:
                                            output_file_id = of
                                        elif isinstance(of, dict) and "id" in of:
                                            output_file_id = of["id"]
                                        if output_file_id:
                                            self.append_log(f"  [DEBUG] {bid}: model_dump()ì—ì„œ output_fileì—ì„œ ì¶”ì¶œ: {output_file_id}")
                            except Exception as e:
                                # ë””ë²„ê¹…: output_file_idê°€ ì—†ì„ ë•Œ remote ê°ì²´ ì†ì„± í™•ì¸
                                attrs = [attr for attr in dir(remote) if not attr.startswith('_')]
                                self.append_log(f"  [DEBUG] {bid}: output_file_id ì—†ìŒ. remote ì†ì„±: {', '.join(attrs[:10])}, model_dump ì‹¤íŒ¨: {e}")
                        
                        # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                        if remote.status == "completed":
                            if not output_file_id:
                                self.append_log(f"  âš ï¸ {bid}: ì™„ë£Œ ìƒíƒœì§€ë§Œ output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                                upsert_batch_job(bid, status=remote.status, output_file_id=None)
                                continue
                        elif remote.status == "expired":
                            if not output_file_id:
                                self.append_log(f"  âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                                upsert_batch_job(bid, status=remote.status, output_file_id=None)
                                continue
                            else:
                                self.append_log(f"  â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                        else:
                            self.append_log(f"  âš ï¸ {bid}: ì•„ì§ completed ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                            upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id)
                            continue
                        
                        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                        base, _ = os.path.splitext(src_path)
                        out_jsonl = f"{base}_img_analysis_batch_output_{bid}.jsonl"
                        
                        try:
                            content = client.files.content(output_file_id).content
                        except AttributeError:
                            file_content = client.files.content(output_file_id)
                            if hasattr(file_content, "read"):
                                content = file_content.read()
                            elif hasattr(file_content, "iter_bytes"):
                                chunks = []
                                for ch in file_content.iter_bytes():
                                    chunks.append(ch)
                                content = b"".join(chunks)
                            else:
                                content = file_content
                        
                        with open(out_jsonl, "wb") as f:
                            f.write(content)
                        
                        upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id, output_jsonl=out_jsonl)
                        
                        # JSONL íŒŒì¼ ì½ì–´ì„œ ê²°ê³¼ ìˆ˜ì§‘
                        batch_in_tok = 0
                        batch_out_tok = 0
                        batch_reasoning_tok = 0
                        batch_cached_tok = 0
                        batch_total_requests = 0
                        batch_cache_hits = 0
                        with open(out_jsonl, "r", encoding="utf-8") as f:
                            for line in f:
                                if not line.strip(): continue
                                data = json.loads(line)
                                cid = data.get("custom_id")
                                
                                # /v1/responses API í˜•ì‹ ì²˜ë¦¬
                                response_body = data.get("response", {}).get("body", {})
                                usage = response_body.get("usage", {})
                                input_tokens = usage.get("input_tokens", 0)
                                batch_in_tok += input_tokens
                                batch_out_tok += usage.get("output_tokens", 0)
                                output_tokens_details = usage.get("output_tokens_details", {})
                                batch_reasoning_tok += output_tokens_details.get("reasoning_tokens", 0)
                                
                                # ìºì‹± í†µê³„ ìˆ˜ì§‘
                                input_tokens_details = usage.get("input_tokens_details", {})
                                cached_tokens = input_tokens_details.get("cached_tokens", 0)
                                batch_cached_tok += cached_tokens
                                batch_total_requests += 1
                                if cached_tokens > 0:
                                    batch_cache_hits += 1
                                
                                try:
                                    # /v1/responses API: output ë°°ì—´ì—ì„œ message íƒ€ì… ì°¾ê¸°
                                    output_array = response_body.get("output", [])
                                    content_str = None
                                    
                                    for output_item in output_array:
                                        if output_item.get("type") == "message":
                                            content_array = output_item.get("content", [])
                                            for content_item in content_array:
                                                if content_item.get("type") == "output_text":
                                                    content_str = content_item.get("text", "").strip()
                                                    break
                                            if content_str:
                                                break
                                    
                                    # Fallback: ê¸°ì¡´ choices í˜•ì‹ (í˜¸í™˜ì„±)
                                    if not content_str:
                                        content_str = data.get("response", {}).get("body", {}).get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                                    
                                    if content_str:
                                        result_data = json.loads(content_str)
                                        all_results_map[cid] = result_data
                                    else:
                                        if cid:
                                            all_results_map[cid] = {}
                                except Exception as e:
                                    if cid:
                                        all_results_map[cid] = {}
                                    self.append_log(f"  [WARN] {cid} íŒŒì‹± ì‹¤íŒ¨: {e}")
                        
                        total_group_in += batch_in_tok
                        total_group_out += batch_out_tok
                        total_group_cached += batch_cached_tok
                        total_group_requests += batch_total_requests
                        total_group_cache_hits += batch_cache_hits
                        
                        # ìºì‹± í†µê³„ ì¶œë ¥
                        cache_hit_rate = (batch_cache_hits / batch_total_requests * 100) if batch_total_requests > 0 else 0
                        cache_savings_pct = (batch_cached_tok / batch_in_tok * 100) if batch_in_tok > 0 else 0
                        self.append_log(f"  [ìºì‹±] {bid}: ìš”ì²­ {batch_total_requests}ê±´, íˆíŠ¸ {batch_cache_hits}ê±´ ({cache_hit_rate:.1f}%), ìºì‹œ í† í° {batch_cached_tok:,} ({cache_savings_pct:.1f}%)")
                        
                        # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                        pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                        cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                        # output_tokensì— reasoning_tokens í¬í•¨ë¨ (ë³„ë„ ê³„ì‚° ë¶ˆí•„ìš”)
                        cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                        total_group_cost += cost_in + cost_out
                        
                        # ìºì‹œë¡œ ì ˆê°ëœ ë¹„ìš© ê³„ì‚° (ìºì‹œëœ í† í°ì€ ë¹„ìš©ì´ 0)
                        cache_savings = (batch_cached_tok / 1_000_000) * pricing["input"] * 0.5
                        if cache_savings > 0:
                            self.append_log(f"  [ë¹„ìš©ì ˆê°] {bid}: ìºì‹±ìœ¼ë¡œ ${cache_savings:.4f} ì ˆê°")
                        
                    except Exception as e:
                        self.append_log(f"  âŒ {bid} ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if not all_results_map:
                    self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ë³‘í•©í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ê·¸ë£¹ì˜ ì „ì²´ ì²­í¬ ìˆ˜ í™•ì¸ ë° ê²€ì¦
                expected_total_chunks = first_job.get("total_chunks")
                if expected_total_chunks:
                    downloaded_batch_ids = []
                    for bid in batch_ids_sorted:
                        local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                        if local_job and local_job.get("status") in ["completed", "expired"]:
                            base, _ = os.path.splitext(src_path)
                            out_jsonl = local_job.get("output_jsonl") or f"{base}_img_analysis_batch_output_{bid}.jsonl"
                            if os.path.exists(out_jsonl):
                                downloaded_batch_ids.append(bid)
                    
                    if len(downloaded_batch_ids) < expected_total_chunks:
                        missing = expected_total_chunks - len(downloaded_batch_ids)
                        self.append_log(f"âš ï¸ ê·¸ë£¹ {group_id}: ì˜ˆìƒ {expected_total_chunks}ê°œ ì¤‘ {len(downloaded_batch_ids)}ê°œë§Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({missing}ê°œ ëˆ„ë½ ê°€ëŠ¥)")
                
                # í†µí•© ê²°ê³¼ë¥¼ ì—‘ì…€ì— ë³‘í•©
                df = pd.read_excel(src_path)
                result_cols = [
                    "view_point", "subject_position", "subject_size", "lighting_condition",
                    "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
                    "bg_layout_hint_en"
                ]
                for col in result_cols:
                    if col not in df.columns:
                        df[col] = ""
                    df[col] = df[col].astype(str)
                
                cnt = 0
                for cid, result_data in all_results_map.items():
                    try:
                        idx = int(cid.split("_")[1])
                        if 0 <= idx < len(df):
                            for col in result_cols:
                                if col in result_data:
                                    val = result_data[col]
                                    if col == "is_flat_lay":
                                        df.at[idx, col] = str(val).lower() if isinstance(val, bool) else str(val)
                                    else:
                                        df.at[idx, col] = str(val)
                            cnt += 1
                    except:
                        pass
                
                # ì¤‘ê°„ íŒŒì¼ ì €ì¥
                base, ext = os.path.splitext(src_path)
                out_excel = f"{base}_img_analysis_batch_done{ext}"
                if not safe_save_excel(df, out_excel):
                    self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")
                    continue
                
                # I3 ë²„ì „ íŒŒì¼ë¡œ ì €ì¥
                try:
                    final_out_path = get_i3_output_path(src_path)
                    df_done = pd.read_excel(out_excel)
                    if safe_save_excel(df_done, final_out_path):
                        # ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
                        if out_excel != final_out_path and os.path.exists(out_excel):
                            try:
                                os.remove(out_excel)
                                self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                            except Exception as e:
                                self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                    else:
                        final_out_path = out_excel
                except Exception as e:
                    self.append_log(f"[WARN] I3 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                    final_out_path = out_excel
                
                # ê·¸ë£¹ ë‚´ ëª¨ë“  ë°°ì¹˜ë¥¼ merged ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                for bid in batch_ids_sorted:
                    upsert_batch_job(bid, out_excel=final_out_path, status="merged")
                
                # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸
                try:
                    root_name = get_root_filename(src_path)
                    JobManager.update_status(root_name, img_s3_1_msg="I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)")
                    self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)")
                except Exception as e:
                    self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")
                
                # ê·¸ë£¹ ì „ì²´ ìºì‹± í†µê³„ ì¶œë ¥
                group_cache_hit_rate = (total_group_cache_hits / total_group_requests * 100) if total_group_requests > 0 else 0
                group_cache_savings_pct = (total_group_cached / total_group_in * 100) if total_group_in > 0 else 0
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                group_cache_savings = (total_group_cached / 1_000_000) * pricing["input"] * 0.5
                
                self.append_log(f"  [ê·¸ë£¹] ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(final_out_path)}")
                self.append_log(f"  [ê·¸ë£¹ ìºì‹± í†µê³„] ìš”ì²­ {total_group_requests:,}ê±´, íˆíŠ¸ {total_group_cache_hits:,}ê±´ ({group_cache_hit_rate:.1f}%), ìºì‹œ í† í° {total_group_cached:,} ({group_cache_savings_pct:.1f}%)")
                if group_cache_savings > 0:
                    self.append_log(f"  [ê·¸ë£¹ ë¹„ìš©ì ˆê°] ìºì‹±ìœ¼ë¡œ ì´ ${group_cache_savings:.4f} ì ˆê°")
                
                success_cnt += 1
                total_cost += total_group_cost
                success_folders.add(os.path.dirname(final_out_path))
                
            except Exception as e:
                self.append_log(f"âŒ ê·¸ë£¹ {group_id} ë³‘í•© ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
                continue
        
        # ê·¸ë£¹ ì—†ëŠ” ë°°ì¹˜ë“¤ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        for bid in ungrouped_batches:
            self.append_log(f"--- ë³‘í•© ì‹œì‘: {bid} ---")
            try:
                jobs = load_batch_jobs()
                local_job = next((j for j in jobs if j["batch_id"] == bid), None)
                
                if not local_job:
                    self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì‘ì—… ì´ë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # ì´ë¯¸ ë³‘í•©ëœ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
                is_already_merged = local_job.get("status") == "merged"
                if is_already_merged:
                    self.append_log(f"â­ï¸ {bid}: ì´ë¯¸ ë³‘í•© ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤.")
                    continue

                # Batch ìƒíƒœ ë° ê²°ê³¼ íŒŒì¼ ID ì¡°íšŒ
                remote = client.batches.retrieve(bid)
                # output_file_id ì¶”ì¶œ: ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
                output_file_id = getattr(remote, "output_file_id", None)
                if not output_file_id:
                    # output_file ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                    output_file = getattr(remote, "output_file", None)
                    if output_file:
                        if isinstance(output_file, str):
                            output_file_id = output_file
                        else:
                            output_file_id = getattr(output_file, "id", None) or getattr(output_file, "file_id", None)
                
                # model_dump()ë¥¼ í†µí•œ ì¶”ê°€ í™•ì¸
                if not output_file_id and remote.status == "completed":
                    try:
                        if hasattr(remote, "model_dump"):
                            dump = remote.model_dump()
                            if "output_file_id" in dump and dump["output_file_id"]:
                                output_file_id = dump["output_file_id"]
                                self.append_log(f"  [DEBUG] {bid}: model_dump()ì—ì„œ output_file_id ë°œê²¬: {output_file_id}")
                            elif "output_file" in dump:
                                of = dump["output_file"]
                                if isinstance(of, str) and of:
                                    output_file_id = of
                                elif isinstance(of, dict) and "id" in of:
                                    output_file_id = of["id"]
                                if output_file_id:
                                    self.append_log(f"  [DEBUG] {bid}: model_dump()ì—ì„œ output_fileì—ì„œ ì¶”ì¶œ: {output_file_id}")
                    except Exception as e:
                        # ë””ë²„ê¹…: output_file_idê°€ ì—†ì„ ë•Œ remote ê°ì²´ ì†ì„± í™•ì¸
                        attrs = [attr for attr in dir(remote) if not attr.startswith('_')]
                        self.append_log(f"  [DEBUG] {bid}: output_file_id ì—†ìŒ. remote ì†ì„±: {', '.join(attrs[:10])}, model_dump ì‹¤íŒ¨: {e}")
                
                # completed ë˜ëŠ” expired ìƒíƒœì—ì„œ output_file_idê°€ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
                if remote.status == "completed":
                    if not output_file_id:
                        self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: ì™„ë£Œ ìƒíƒœì§€ë§Œ output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        upsert_batch_job(bid, status=remote.status, output_file_id=None)
                        continue
                elif remote.status == "expired":
                    if not output_file_id:
                        self.append_log(f"âš ï¸ {bid}: ë§Œë£Œëœ ë°°ì¹˜ì´ë©° output_file_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
                        upsert_batch_job(bid, status=remote.status, output_file_id=None)
                        continue
                    else:
                        self.append_log(f"â„¹ï¸ {bid}: ë°°ì¹˜ê°€ ë§Œë£Œë˜ì—ˆì§€ë§Œ output_file_idê°€ ìˆì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                else:
                    self.append_log(f"âš ï¸ {bid}: ì•„ì§ completed ë˜ëŠ” expired ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤. (status={remote.status})")
                    upsert_batch_job(bid, status=remote.status, output_file_id=output_file_id)
                    continue
                
                # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                try:
                    content = client.files.content(output_file_id).content
                except AttributeError:
                    file_content = client.files.content(output_file_id)
                    if hasattr(file_content, "read"):
                        content = file_content.read()
                    elif hasattr(file_content, "iter_bytes"):
                        chunks = []
                        for ch in file_content.iter_bytes():
                            chunks.append(ch)
                        content = b"".join(chunks)
                    else:
                        content = file_content
                
                if local_job and local_job.get("src_excel"):
                    src_path = local_job["src_excel"]
                    base, _ = os.path.splitext(src_path)
                    out_jsonl = f"{base}_img_analysis_batch_output.jsonl"
                else:
                    out_jsonl = f"output_{bid}.jsonl"
                    src_path = None

                with open(out_jsonl, "wb") as f:
                    f.write(content)
                
                # JSONL íŒŒì‹± ë° ì—‘ì…€ ë³‘í•©
                results_map = {}
                batch_in_tok = 0
                batch_out_tok = 0
                batch_reasoning_tok = 0
                batch_cached_tok = 0
                batch_total_requests = 0
                batch_cache_hits = 0
                
                with open(out_jsonl, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        
                        # /v1/responses API í˜•ì‹ ì²˜ë¦¬
                        response_body = data.get("response", {}).get("body", {})
                        usage = response_body.get("usage", {})
                        input_tokens = usage.get("input_tokens", 0) or usage.get("prompt_tokens", 0)  # í˜¸í™˜ì„±
                        batch_in_tok += input_tokens
                        batch_out_tok += usage.get("output_tokens", 0) or usage.get("completion_tokens", 0)  # í˜¸í™˜ì„±
                        output_tokens_details = usage.get("output_tokens_details", {})
                        batch_reasoning_tok += output_tokens_details.get("reasoning_tokens", 0)
                        
                        # ìºì‹± í†µê³„ ìˆ˜ì§‘
                        input_tokens_details = usage.get("input_tokens_details", {})
                        cached_tokens = input_tokens_details.get("cached_tokens", 0)
                        batch_cached_tok += cached_tokens
                        batch_total_requests += 1
                        if cached_tokens > 0:
                            batch_cache_hits += 1
                        
                        try:
                            # /v1/responses API: output ë°°ì—´ì—ì„œ message íƒ€ì… ì°¾ê¸°
                            output_array = response_body.get("output", [])
                            content_str = None
                            
                            for output_item in output_array:
                                if output_item.get("type") == "message":
                                    content_array = output_item.get("content", [])
                                    for content_item in content_array:
                                        if content_item.get("type") == "output_text":
                                            content_str = content_item.get("text", "").strip()
                                            break
                                    if content_str:
                                        break
                            
                            # Fallback: ê¸°ì¡´ choices í˜•ì‹ (í˜¸í™˜ì„±)
                            if not content_str:
                                content_str = data.get("response", {}).get("body", {}).get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                            
                            if content_str:
                                result_data = json.loads(content_str)
                                results_map[cid] = result_data
                            else:
                                results_map[cid] = {}
                        except Exception as e:
                            results_map[cid] = {}
                            self.append_log(f"  [WARN] {cid} íŒŒì‹± ì‹¤íŒ¨: {e}")

                # ìºì‹± í†µê³„ ì¶œë ¥
                cache_hit_rate = (batch_cache_hits / batch_total_requests * 100) if batch_total_requests > 0 else 0
                cache_savings_pct = (batch_cached_tok / batch_in_tok * 100) if batch_in_tok > 0 else 0
                self.append_log(f"  [ìºì‹±] {bid}: ìš”ì²­ {batch_total_requests}ê±´, íˆíŠ¸ {batch_cache_hits}ê±´ ({cache_hit_rate:.1f}%), ìºì‹œ í† í° {batch_cached_tok:,} ({cache_savings_pct:.1f}%)")
                
                # ë¹„ìš© ê³„ì‚° (50% í• ì¸)
                model_name = local_job.get("model", "gpt-5-mini") if local_job else "gpt-5-mini"
                pricing = MODEL_PRICING_USD_PER_MTOK.get(model_name, {"input": 0, "output": 0})
                cost_in = (batch_in_tok / 1_000_000) * pricing["input"] * 0.5
                cost_out = (batch_out_tok / 1_000_000) * pricing["output"] * 0.5
                cost_total = cost_in + cost_out
                total_cost += cost_total
                
                # ìºì‹œë¡œ ì ˆê°ëœ ë¹„ìš© ê³„ì‚° (ìºì‹œëœ í† í°ì€ ë¹„ìš©ì´ 0)
                cache_savings = (batch_cached_tok / 1_000_000) * pricing["input"] * 0.5
                if cache_savings > 0:
                    self.append_log(f"  [ë¹„ìš©ì ˆê°] {bid}: ìºì‹±ìœ¼ë¡œ ${cache_savings:.4f} ì ˆê°")

                if src_path and os.path.exists(src_path):
                    df = pd.read_excel(src_path)
                    result_cols = [
                        "view_point", "subject_position", "subject_size", "lighting_condition",
                        "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
                        "bg_layout_hint_en"
                    ]
                    for col in result_cols:
                        if col not in df.columns:
                            df[col] = ""
                        df[col] = df[col].astype(str)
                    
                    cnt = 0
                    for cid, result_data in results_map.items():
                        try:
                            idx = int(cid.split("_")[1])
                            if 0 <= idx < len(df):
                                for col in result_cols:
                                    if col in result_data:
                                        val = result_data[col]
                                        # is_flat_layëŠ” booleanì´ë¯€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
                                        if col == "is_flat_lay":
                                            df.at[idx, col] = str(val).lower() if isinstance(val, bool) else str(val)
                                        else:
                                            df.at[idx, col] = str(val)
                                cnt += 1
                        except:
                            pass

                    # ì¤‘ê°„ íŒŒì¼ ì €ì¥
                    base, ext = os.path.splitext(src_path)
                    out_excel = f"{base}_img_analysis_batch_done{ext}"
                    if not safe_save_excel(df, out_excel):
                        self.append_log(f"[WARN] ê¸°ë³¸ ì™„ë£Œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out_excel}")
                        continue

                    # I3 ë²„ì „ íŒŒì¼ë¡œ ì €ì¥
                    try:
                        final_out_path = get_i3_output_path(src_path)
                        df_done = pd.read_excel(out_excel)
                        if safe_save_excel(df_done, final_out_path):
                            # ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
                            if out_excel != final_out_path and os.path.exists(out_excel):
                                try:
                                    os.remove(out_excel)
                                    self.append_log(f"[INFO] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {os.path.basename(out_excel)}")
                                except Exception as e:
                                    self.append_log(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                        else:
                            final_out_path = out_excel
                    except Exception as e:
                        self.append_log(f"[WARN] I3 ë²„ì „ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                        final_out_path = out_excel

                    upsert_batch_job(bid, out_excel=final_out_path, status="merged")

                    # ë©”ì¸ ëŸ°ì²˜ í˜„í™©íŒì— I3-1 ì™„ë£Œ ìƒíƒœ ê¸°ë¡ - img ìƒíƒœë§Œ I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)ë¡œ ì—…ë°ì´íŠ¸ (text ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
                    try:
                        root_name = get_root_filename(src_path)
                        JobManager.update_status(root_name, img_s3_1_msg="I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)")
                        self.append_log(f"[INFO] ëŸ°ì²˜ í˜„í™©íŒ ì—…ë°ì´íŠ¸: {root_name} -> I3-1(ì¸ë„¤ì¼ë¶„ì„ì™„ë£Œ)")
                    except Exception as e:
                        self.append_log(f"[WARN] í˜„í™©íŒ ì—°ë™ ì‹¤íŒ¨: {e}")

                    self.append_log(f"âœ… ë³‘í•© ì™„ë£Œ ({cnt}ê±´): {os.path.basename(final_out_path)}")
                    success_cnt += 1
                    # ì„±ê³µí•œ íŒŒì¼ì˜ í´ë” ê²½ë¡œ ì €ì¥
                    success_folders.add(os.path.dirname(final_out_path))
                else:
                    self.append_log(f"âš ï¸ ì›ë³¸ ì—†ìŒ. JSONLë§Œ ì €ì¥.")
            except Exception as e:
                self.append_log(f"âŒ {bid} ë³‘í•© ì‹¤íŒ¨: {e}")
        
        self.append_log(f"=== ì¼ê´„ ë³‘í•© ë (ì„±ê³µ: {success_cnt}, ë¹„ìš©: ${total_cost:.4f}) ===")
        self._load_jobs_all()
        
        # ì„±ê³µí•œ íŒŒì¼ì´ ìˆìœ¼ë©´ í´ë” ì—´ê¸°
        if success_folders:
            try:
                # ì²« ë²ˆì§¸ ì„±ê³µí•œ í´ë” ì—´ê¸° (ì—¬ëŸ¬ ê°œë©´ ì²« ë²ˆì§¸ë§Œ)
                folder_path = list(success_folders)[0]
                if platform.system() == "Windows":
                    os.startfile(folder_path)
                elif platform.system() == "Darwin":  # macOS
                    subprocess.run(["open", folder_path])
                else:  # Linux
                    subprocess.run(["xdg-open", folder_path])
                self.append_log(f"ğŸ“‚ ê²°ê³¼ í´ë” ì—´ê¸°: {folder_path}")
            except Exception as e:
                self.append_log(f"[WARN] í´ë” ì—´ê¸° ì‹¤íŒ¨: {e}")
        
        messagebox.showinfo("ì™„ë£Œ", f"{success_cnt}ê±´ ë³‘í•© ì™„ë£Œ.\nì´ ë¹„ìš©: ${total_cost:.4f}\n\nê²°ê³¼ íŒŒì¼ì´ ì €ì¥ëœ í´ë”ë¥¼ ì—´ì—ˆìŠµë‹ˆë‹¤.")

    def _archive_selected(self):
        ids = self._get_selected_ids(self.tree_active)
        if ids and messagebox.askyesno("ë³´ê´€", f"{len(ids)}ê±´ íœ´ì§€í†µ ì´ë™?"):
            archive_batch_job(ids, True)
            self._load_jobs_all()
            self._load_archive_list()

    def _restore_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ë³µêµ¬", f"{len(ids)}ê±´ ë³µêµ¬?"):
            archive_batch_job(ids, False)
            self._load_jobs_all()
            self._load_archive_list()

    def _hard_delete_selected(self):
        ids = self._get_selected_ids(self.tree_arch)
        if ids and messagebox.askyesno("ì‚­ì œ", "ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            hard_delete_batch_job(ids)
            self._load_archive_list()

    def _on_tree_double_click(self, event):
        """ë”ë¸”í´ë¦­ ì‹œ: ê·¸ë£¹ í—¤ë”ë©´ ì ‘ê¸°/í¼ì¹˜ê¸°, ë°°ì¹˜ë©´ ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™"""
        sel = self.tree_active.selection()
        if not sel: return
        
        item = sel[0]
        vals = self.tree_active.item(item)['values']
        batch_id = vals[0] if vals else ""
        
        # ê·¸ë£¹ í—¤ë”ì¸ ê²½ìš°: ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
        if not batch_id:
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            children = self.tree_active.get_children(item)
            if children:
                # ìì‹ì´ ìˆìœ¼ë©´ ì ‘ê¸°/í¼ì¹˜ê¸° í† ê¸€
                if self.tree_active.item(item, 'open'):
                    self.tree_active.item(item, open=False)
                else:
                    self.tree_active.item(item, open=True)
        else:
            # ê°œë³„ ë°°ì¹˜ì¸ ê²½ìš°: ë³‘í•© íƒ­ìœ¼ë¡œ ì´ë™
            self.batch_id_var.set(batch_id)
            self.main_tabs.select(self.tab_merge)

    # ----------------------------------------------------
    # Tab 3: Merge (Manual)
    # ----------------------------------------------------
    def _init_tab_merge(self):
        container = ttk.Frame(self.tab_merge, padding=20)
        container.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì„¹ì…˜
        f_retry = ttk.LabelFrame(container, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", padding=15)
        f_retry.pack(fill='x', pady=(0, 15))
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        f_list = ttk.Frame(f_retry)
        f_list.pack(fill='both', expand=True, pady=(0, 10))
        ttk.Label(f_list, text="ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡:", font=("ë§‘ì€ ê³ ë”•", 9, "bold")).pack(anchor='w')
        
        # Treeviewë¡œ ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ í‘œì‹œ
        list_frame = ttk.Frame(f_list)
        list_frame.pack(fill='both', expand=True, pady=5)
        
        scrollbar = ttk.Scrollbar(list_frame)
        scrollbar.pack(side='right', fill='y')
        
        self.failed_chunks_tree = ttk.Treeview(list_frame, columns=("chunk_num", "file_name", "error_type"), 
                                                show='headings', height=4, yscrollcommand=scrollbar.set)
        scrollbar.config(command=self.failed_chunks_tree.yview)
        
        self.failed_chunks_tree.heading("chunk_num", text="ì²­í¬ ë²ˆí˜¸")
        self.failed_chunks_tree.heading("file_name", text="íŒŒì¼ëª…")
        self.failed_chunks_tree.heading("error_type", text="ì˜¤ë¥˜ ìœ í˜•")
        
        self.failed_chunks_tree.column("chunk_num", width=80, anchor="center")
        self.failed_chunks_tree.column("file_name", width=300, anchor="w")
        self.failed_chunks_tree.column("error_type", width=150, anchor="center")
        
        self.failed_chunks_tree.pack(fill='both', expand=True)
        
        # ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ ì…ë ¥
        f_file = ttk.Frame(f_retry)
        f_file.pack(fill='x', pady=(10, 0))
        ttk.Label(f_file, text="ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼:", font=("ë§‘ì€ ê³ ë”•", 9)).pack(side='left')
        self.failed_chunks_file_var = tk.StringVar()
        ttk.Entry(f_file, textvariable=self.failed_chunks_file_var, width=50, font=("Consolas", 9)).pack(side='left', padx=5, fill='x', expand=True)
        btn_select = ttk.Button(f_file, text="ğŸ“‚ ì°¾ê¸°", command=self._select_failed_chunks_file)
        btn_select.pack(side='left', padx=5)
        btn_retry = ttk.Button(f_file, text="ğŸ”„ ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„", command=self._retry_failed_chunks, style="Success.TButton")
        btn_retry.pack(side='left', padx=5)
        
        f_in = ttk.LabelFrame(container, text="ê°œë³„ ì‘ì—…", padding=15)
        f_in.pack(fill='x', pady=(0, 15))
        ttk.Label(f_in, text="Batch ID:").pack(side='left')
        ttk.Entry(f_in, textvariable=self.batch_id_var, width=45, font=("Consolas", 10)).pack(side='left', padx=10)
        
        f_btn = ttk.Frame(container)
        f_btn.pack(fill='x', pady=20)
        ttk.Button(f_btn, text="1. ê²°ê³¼ ë³‘í•©", command=self._start_merge, style="Primary.TButton").pack(fill='x', pady=5)

    def _start_merge(self):
        key = self.api_key_var.get().strip()
        if not key:
            messagebox.showwarning("ì˜¤ë¥˜", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        bid = self.batch_id_var.get().strip()
        if not bid:
            messagebox.showwarning("ì˜¤ë¥˜", "Batch IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        t = threading.Thread(target=self._run_merge)
        t.daemon = True
        t.start()

    def _run_merge(self):
        bid = self.batch_id_var.get().strip()
        if bid: self._run_merge_multi([bid])
    
    def _handle_failed_chunks(self, failed_info_path, failed_chunk_files):
        """ì‹¤íŒ¨í•œ ì²­í¬ê°€ ìˆì„ ë•Œ GUIì— ìë™ìœ¼ë¡œ í‘œì‹œí•˜ê³  ì•Œë¦¼"""
        # ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ê²½ë¡œ ìë™ ì„¤ì •
        self.failed_chunks_file_var.set(failed_info_path)
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ëª©ë¡ì„ Treeviewì— í‘œì‹œ
        for item in self.failed_chunks_tree.get_children():
            self.failed_chunks_tree.delete(item)
        
        for failed_info in failed_chunk_files:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
            file_name = os.path.basename(chunk_file)
            
            self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        
        # ì•Œë¦¼ ë©”ì‹œì§€ ë° íƒ­ ì „í™˜
        failed_count = len(failed_chunk_files)
        token_limit_count = sum(1 for f in failed_chunk_files if f.get("is_token_limit", False))
        
        msg = f"âš ï¸ {failed_count}ê°œ ì²­í¬ì˜ ë°°ì¹˜ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\n"
        if token_limit_count > 0:
            msg += f"â€¢ í† í° ì œí•œ ì˜¤ë¥˜: {token_limit_count}ê°œ\n"
        msg += f"â€¢ ì‹¤íŒ¨ ì •ë³´ê°€ ìë™ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        msg += f"â€¢ '3. ê°œë³„ ë³‘í•© (ìˆ˜ë™)' íƒ­ì—ì„œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        messagebox.showwarning("ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨", msg)
        
        # ì¬ì‹œë„ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
        self.main_tabs.select(self.tab_merge)
    
    def _select_failed_chunks_file(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ"""
        path = filedialog.askopenfilename(
            title="ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ ì„ íƒ",
            filetypes=[("JSON íŒŒì¼", "*.json"), ("ëª¨ë“  íŒŒì¼", "*.*")]
        )
        if path:
            self.failed_chunks_file_var.set(path)
            # íŒŒì¼ì„ ì„ íƒí•˜ë©´ ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(path)
    
    def _load_failed_chunks_from_file(self, file_path):
        """ì‹¤íŒ¨ ì •ë³´ JSON íŒŒì¼ì„ ì½ì–´ì„œ ëª©ë¡ì— í‘œì‹œ"""
        if not os.path.exists(file_path):
            return
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ê¸°ì¡´ ëª©ë¡ ì‚­ì œ
            for item in self.failed_chunks_tree.get_children():
                self.failed_chunks_tree.delete(item)
            
            # ëª©ë¡ì— ì¶”ê°€
            for failed_info in failed_chunks:
                chunk_num = failed_info.get("chunk_num", 0)
                chunk_file = failed_info.get("chunk_file", "")
                error_type = "í† í° ì œí•œ" if failed_info.get("is_token_limit", False) else "ì¼ë°˜ ì˜¤ë¥˜"
                file_name = os.path.basename(chunk_file)
                
                self.failed_chunks_tree.insert("", "end", values=(chunk_num, file_name, error_type))
        except Exception as e:
            self.append_log(f"[WARN] ì‹¤íŒ¨ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    def _retry_failed_chunks(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„"""
        failed_file = self.failed_chunks_file_var.get().strip()
        if not failed_file:
            messagebox.showwarning("ê²½ê³ ", "ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ JSON íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        if not os.path.exists(failed_file):
            messagebox.showerror("ì˜¤ë¥˜", f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{failed_file}")
            return
        
        if not self.api_key_var.get():
            messagebox.showwarning("ê²½ê³ ", "API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        try:
            with open(failed_file, "r", encoding="utf-8") as f:
                failed_chunks = json.load(f)
            
            # ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            self._load_failed_chunks_from_file(failed_file)
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨:\n{e}")
            return
        
        if not failed_chunks:
            messagebox.showinfo("ì•Œë¦¼", "ì¬ì‹œë„í•  ì‹¤íŒ¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if messagebox.askyesno("í™•ì¸", f"{len(failed_chunks)}ê°œ ì‹¤íŒ¨í•œ ì²­í¬ë¥¼ ì¬ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            t = threading.Thread(target=self._run_retry_failed_chunks, args=(failed_chunks,))
            t.daemon = True
            t.start()
    
    def _run_retry_failed_chunks(self, failed_chunks):
        """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„ ì‹¤í–‰"""
        key = self.api_key_var.get().strip()
        import httpx
        timeout = httpx.Timeout(600.0, connect=60.0)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        client = OpenAI(api_key=key, timeout=timeout, max_retries=3)
        
        self.append_log(f"[RETRY] ì‹¤íŒ¨í•œ ì²­í¬ {len(failed_chunks)}ê°œ ì¬ì‹œë„ ì‹œì‘...")
        
        retry_batch_ids = []
        for failed_info in failed_chunks:
            chunk_num = failed_info.get("chunk_num", 0)
            chunk_file = failed_info.get("chunk_file", "")
            excel_path = failed_info.get("excel_path", "")
            model_name = failed_info.get("model_name", DEFAULT_MODEL)
            effort = failed_info.get("effort", "low")
            batch_group_id = failed_info.get("batch_group_id", "")
            
            if not os.path.exists(chunk_file):
                self.append_log(f"âš ï¸ ì²­í¬ {chunk_num}: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_file}")
                continue
            
            self.append_log(f"[RETRY] ì²­í¬ {chunk_num} ì¬ì‹œë„ ì¤‘... ({os.path.basename(chunk_file)})")
            
            try:
                batch = self._create_batch_from_jsonl(
                    client=client,
                    jsonl_path=chunk_file,
                    excel_path=excel_path,
                    model_name=model_name,
                    reasoning_effort=effort,
                )
                
                batch_id = batch.id
                retry_batch_ids.append(batch_id)
                self.append_log(f"âœ… ì²­í¬ {chunk_num} ì¬ì‹œë„ ì„±ê³µ: {batch_id}")
                
                # ì‘ì—… ì´ë ¥ ê¸°ë¡
                upsert_batch_job(
                    batch_id=batch_id,
                    src_excel=excel_path,
                    jsonl_path=chunk_file,
                    model=model_name,
                    effort=effort,
                    status=batch.status,
                    output_file_id=None,
                    batch_group_id=batch_group_id,
                    chunk_index=chunk_num,
                )
                
            except Exception as e:
                self.append_log(f"âŒ ì²­í¬ {chunk_num} ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                import traceback
                self.append_log(traceback.format_exc())
        
        if retry_batch_ids:
            self.append_log(f"âœ… ì¬ì‹œë„ ì™„ë£Œ: {len(retry_batch_ids)}ê°œ ë°°ì¹˜ ìƒì„±ë¨")
            # ë°°ì¹˜ ëª©ë¡ ê°±ì‹  ë° ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
            self.after(0, lambda: [
                self._load_jobs_all(),
                self._load_archive_list(),
                self.main_tabs.select(self.tab_manage),  # ë°°ì¹˜ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ìë™ ì „í™˜
                messagebox.showinfo("ì™„ë£Œ", f"{len(retry_batch_ids)}ê°œ ì²­í¬ ì¬ì‹œë„ ì„±ê³µ:\n{', '.join(retry_batch_ids[:5])}{'...' if len(retry_batch_ids) > 5 else ''}\n\në°°ì¹˜ ê´€ë¦¬ íƒ­ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.")
            ])
        else:
            self.append_log(f"âš ï¸ ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.after(0, lambda: messagebox.showwarning("ê²½ê³ ", "ì¬ì‹œë„ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤."))


if __name__ == "__main__":
    app = ImageAnalysisBatchGUI()
    app.mainloop()

