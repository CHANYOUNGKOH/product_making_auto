"""
IMG_analysis_core_Casche.py

ì¸ë„¤ì¼ ì´ë¯¸ì§€ ë¶„ì„: ì´ë¯¸ì§€ â†’ ì¹´ë©”ë¼ ê°ë„, ë ˆì´ì•„ì›ƒ, ì¡°ëª… ë¶„ì„ JSON ìƒì„± ì½”ì–´ ëª¨ë“ˆ (ìºì‹± ìµœì í™” ë²„ì „)

- OpenAI Vision APIë¥¼ í˜¸ì¶œí•´ì„œ
  view_point, subject_position, subject_size, lighting_condition, color_tone,
  shadow_presence, background_simplicity, is_flat_lay, bg_layout_hint_en
  9ê°œ í•„ë“œë¥¼ ê°€ì§„ JSONì„ ë°˜í™˜í•œë‹¤.
- ğŸš€ í”„ë¡¬í”„íŠ¸ ìºì‹± ìµœì í™”: OpenAI Prompt Caching ê°€ì´ë“œì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì¬êµ¬ì„±
  * ì •ì  ì½˜í…ì¸ (ì—­í• , ì œì•½, ê·œì¹™)ë¥¼ system í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * ë™ì  ì½˜í…ì¸ (ì´ë¯¸ì§€)ë¥¼ user í”„ë¡¬í”„íŠ¸ì— ë°°ì¹˜
  * í”„ë¡¬í”„íŠ¸ í”„ë¦¬í”½ìŠ¤ê°€ ëª¨ë“  ìš”ì²­ì—ì„œ ë™ì¼í•˜ë„ë¡ êµ¬ì„±
"""

from __future__ import annotations

import os
import json
import base64
from io import BytesIO
from typing import Any, Dict, Optional, Union
from PIL import Image

from openai import OpenAI

# === ê¸°ë³¸ ì„¤ì • ===

# í•„ìš”í•˜ë©´ ì›í•˜ëŠ” íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½í•´ì„œ ì“°ë©´ ë¨
API_KEY_FILE = ".openai_api_key_img_analysis"

# ì‹¤ì œ ì‚¬ìš© ì‹œ, OpenAI ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸í•œ ëª¨ë¸ëª…ìœ¼ë¡œ êµì²´
DEFAULT_MODEL = "gpt-5-mini"  # Vision API ì§€ì› ëª¨ë¸ ì‚¬ìš©

# ëª¨ë¸ë³„ ê°€ê²© (USD per Million Tokens)
MODEL_PRICING_USD_PER_MTOK = {
    "gpt-5": {"input": 1.25, "output": 10.0},
    "gpt-5-mini": {"input": 0.25, "output": 2.00},
    "gpt-5-nano": {"input": 0.05, "output": 0.40},
    "gpt-4o": {"input": 2.50, "output": 10.00},
    "gpt-4o-mini": {"input": 0.15, "output": 0.60},
}

# ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì„¤ì • (í˜¸í™˜ì„± ìœ ì§€ìš©, ì‹¤ì œë¡œëŠ” resize_mode íŒŒë¼ë¯¸í„° ì‚¬ìš©)
INPUT_SIZE = 1000  # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°
OUTPUT_SIZE = 512  # API ì „ì†¡ìš© ë¦¬ì‚¬ì´ì¦ˆ í¬ê¸° (ê¸°ë³¸ê°’)


# === API Key ìœ í‹¸ ===

def load_api_key_from_file(path: str = API_KEY_FILE) -> Optional[str]:
    """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ì½ëŠ”ë‹¤. ì—†ìœ¼ë©´ None."""
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            key = f.read().strip()
        return key or None
    except Exception:
        return None


def save_api_key_to_file(api_key: str, path: str = API_KEY_FILE) -> None:
    """API í‚¤ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì €ì¥í•œë‹¤."""
    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(api_key.strip())


def get_openai_client(api_key: Optional[str] = None) -> OpenAI:
    """
    OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•œë‹¤.
    - api_keyê°€ Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì‚¬ìš©.
    """
    if api_key is None:
        # íŒŒì¼ì— ì €ì¥ëœ í‚¤ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        file_key = load_api_key_from_file()
        if file_key:
            api_key = file_key
    if api_key is None:
        # ë§ˆì§€ë§‰ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜
        api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        raise RuntimeError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                           "API_KEY_FILE ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    return OpenAI(api_key=api_key)


# === ì´ë¯¸ì§€ ì²˜ë¦¬ ìœ í‹¸ ===

def image_to_base64_data_url(image_path: str, max_width: int = None, log_func=None) -> str:
    """
    ì´ë¯¸ì§€ íŒŒì¼ì„ ê°€ë¡œ ê¸°ì¤€ ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ í›„ base64 data URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        max_width: ìµœëŒ€ ê°€ë¡œ (px). Noneì´ë©´ ë¦¬ì‚¬ì´ì§• ì•ˆ í•¨. ê°€ë¡œ ê¸°ì¤€ ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ (í¬ë¡­/íŒ¨ë”© ê¸ˆì§€).
        log_func: ë¡œê·¸ ì¶œë ¥ í•¨ìˆ˜ (ì„ íƒì‚¬í•­)
    
    Returns:
        "data:image/png;base64,..." ë˜ëŠ” "data:image/jpeg;base64,..." í˜•ì‹ì˜ ë¬¸ìì—´
    """
    import mimetypes
    
    mime, _ = mimetypes.guess_type(image_path)
    if mime is None:
        mime = "image/jpeg"
    
    # ë¦¬ì‚¬ì´ì§•ì´ í•„ìš”í•˜ë©´ PIL ì‚¬ìš©
    if max_width is not None:
        try:
            with Image.open(image_path) as img:
                # ì›ë³¸ í¬ê¸° í™•ì¸
                original_width, original_height = img.size
                need_resize = False
                new_width = original_width
                new_height = original_height
                
                # ê°€ë¡œ ê¸°ì¤€ ë¦¬ì‚¬ì´ì¦ˆ (ë¹„ìœ¨ ìœ ì§€, í¬ë¡­/íŒ¨ë”© ê¸ˆì§€)
                if original_width > max_width:
                    # ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ê°€ë¡œ ê¸°ì¤€ìœ¼ë¡œ ë¦¬ì‚¬ì´ì§•
                    ratio = max_width / original_width
                    new_width = max_width
                    new_height = round(original_height * ratio)
                    need_resize = True
                    
                    # ë””ë²„ê¹…: ë¦¬ì‚¬ì´ì¦ˆ ì •ë³´ ë¡œê·¸
                    if log_func:
                        reduction_pct = (1 - (new_width * new_height) / (original_width * original_height)) * 100
                        log_func(f"[ë¦¬ì‚¬ì´ì¦ˆ ì ìš©] ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: original=({original_width}, {original_height}) -> new=({new_width}, {new_height}) (ê°€ë¡œ {max_width}px)")
                else:
                    # ì´ë¯¸ì§€ê°€ ì´ë¯¸ max_width ì´í•˜ì´ë©´ ë¦¬ì‚¬ì´ì¦ˆ ë¶ˆí•„ìš”
                    if log_func:
                        log_func(f"[ë¦¬ì‚¬ì´ì¦ˆ] ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ë¶ˆí•„ìš”: original=({original_width}, {original_height}) (ì´ë¯¸ ê°€ë¡œ {max_width}px ì´í•˜)")
                
                if need_resize:
                    # RGB ëª¨ë“œë¡œ ë³€í™˜ (RGBA ë“±ë„ ì§€ì›)
                    if img.mode not in ('RGB', 'L'):
                        img = img.convert('RGB')
                    
                    # ê³ í’ˆì§ˆ ë¦¬ìƒ˜í”Œë§ (Lanczos ê¶Œì¥)
                    img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                    
                    # JPEGë¡œ ì €ì¥ (ë©”ëª¨ë¦¬, íˆ¬ëª…ë„ê°€ ìˆìœ¼ë©´ PNG ì‚¬ìš©)
                    output = BytesIO()
                    if img.mode == 'RGBA' or img.mode == 'LA':
                        img_resized.save(output, format='PNG', optimize=True)
                        mime = "image/png"
                    else:
                        img_resized.save(output, format='JPEG', quality=85, optimize=True)
                        mime = "image/jpeg"
                    output.seek(0)
                    
                    # base64 ì¸ì½”ë”©
                    b64 = base64.b64encode(output.read()).decode("ascii")
                    
                    return f"data:{mime};base64,{b64}"
        except Exception as e:
            # ë¦¬ì‚¬ì´ì§• ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
            error_msg = f"[WARN] ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• ì‹¤íŒ¨ ({os.path.basename(image_path)}): {e}, ì›ë³¸ ì‚¬ìš©"
            if log_func:
                log_func(error_msg)
            else:
                print(error_msg)
    
    # ë¦¬ì‚¬ì´ì§• ì—†ì´ ì›ë³¸ ì‚¬ìš©
    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("ascii")
    return f"data:{mime};base64,{b64}"


# === System Prompt ===
# âš ï¸ ì¤‘ìš”: í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”ë¥¼ ìœ„í•´ 1024 í† í° ì´ìƒì´ì–´ì•¼ í•¨
ANALYSIS_SYSTEM_PROMPT = """You are a commercial photography analyst specializing in e-commerce product image analysis. Your expertise includes camera angles, lighting conditions, composition, and technical photography aspects.

Analyze the given product thumbnail image and output one JSON object describing its camera angle, layout, and lighting. The result will be used to generate a background that must match the image's perspective and lighting for product compositing. Accuracy in perspective and lighting matching is critical for seamless product integration.

[Core Rules and Principles]

- Output ONLY one JSON object. No markdown, no comments, no explanations, no additional text.
- All values must be in English. Use standard English terminology for photography and lighting.
- Describe the scene objectively based solely on what you observe. Do NOT guess product purpose, brand, or functionality.
- Focus exclusively on technical photography aspects: camera angle, lighting direction and quality, composition, shadows, and background characteristics.
- Do NOT analyze product features, materials, functionality, or any product-specific attributes.
- Maintain consistency: similar images should produce similar analysis results.
- Be precise: small differences in camera angle or lighting should be reflected accurately in the output.

[Field Definitions and Allowed Values]

1) view_point (choose exactly one):
   - "front_view": Camera positioned at eye level (approximately 0-15 degrees from horizontal), facing the product directly. The product appears as if viewed straight-on. Common for product catalog shots.
   - "top_down": Camera positioned directly above (approximately 85-90 degrees from horizontal), looking straight down. The product is viewed from above. This is the classic flat lay perspective.
   - "angled": Camera positioned at an angle between 30-60 degrees from horizontal. The product is viewed from above but at an angle, showing both top and side surfaces. This is the most common e-commerce product photography angle.
   - "side_view": Camera viewing from the side, showing the product's profile. The camera is positioned perpendicular to the product's main axis, typically at eye level or slightly above.
   - "close_up": Very close camera position where the product fills most or all of the frame. The camera angle can vary, but the key characteristic is the extreme proximity to the subject.

2) subject_position (choose one):
   - "center": Product is centered in the frame
   - "slightly_left": Product is slightly to the left of center
   - "slightly_right": Product is slightly to the right of center
   - "top_center": Product is at the top center of the frame
   - "bottom_center": Product is at the bottom center of the frame

3) subject_size (choose one):
   - "very_small": Product occupies less than 20% of frame
   - "small": Product occupies 20-40% of frame
   - "medium": Product occupies 40-60% of frame
   - "large": Product occupies 60-80% of frame
   - "very_large": Product occupies more than 80% of frame

4) lighting_condition:
   - Short descriptive phrase (typically 3-8 words) describing the lighting quality, direction, and characteristics.
   - Include: light quality (soft/hard/diffuse), direction (from top/left/right/front), source type (studio/natural/artificial), and any notable characteristics.
   - Examples:
     * "soft diffuse studio lighting"
     * "soft diffuse studio lighting with subtle top-left emphasis"
     * "hard directional light from left"
     * "natural daylight from window"
     * "warm artificial lighting"
     * "cool fluorescent lighting"
     * "soft ambient lighting with subtle shadows"
     * "soft diffuse studio lighting with slight specular highlights"
     * "even soft studio lighting with minimal shadows"

5) color_tone:
   - "warm": Yellow, orange, red tones dominant
   - "cool": Blue, cyan, green tones dominant
   - "neutral": Balanced, no strong color cast

6) shadow_presence (choose one):
   - "none": No visible shadows
   - "soft": Soft, diffused shadows with gradual edges
   - "grounded": Shadows that connect the product to the surface
   - "floating": Shadows that make the product appear to float
   - "strong": Hard, dark shadows with sharp edges

7) background_simplicity:
   - "simple": Minimal background, few elements, clean
   - "complex": Busy background, multiple elements, detailed

8) is_flat_lay:
   - true: Camera is directly top-down (90 degrees, looking straight down)
   - false: Any other camera angle

9) bg_layout_hint_en:
   - One concise sentence (typically 15-25 words) describing ONLY an empty background that matches the view_point, lighting, and surface characteristics observed in the image.
   - CRITICAL: Do NOT mention the specific product, product type, or any product-related elements.
   - CRITICAL: If view_point is "front_view", you MUST include "floor", "tabletop", or "surface" to provide spatial context and anchor the object.
   - Required elements: surface type/material, perspective (matching view_point), lighting condition, and emphasis on empty space.
   - Surface types: white studio background, wooden tabletop, marble surface, concrete floor, fabric surface, paper surface, etc.
   - Examples:
     * "Empty clean white studio background seen from eye level with soft diffuse lighting"
     * "Top-down view of an empty clean white tabletop with soft diffuse studio lighting and subtle soft shadows"
     * "Empty marble surface seen from an angled perspective with warm artificial lighting"
     * "Empty concrete floor seen from front view with cool fluorescent lighting"
     * "Empty clean white seamless studio surface seen from an angled perspective with soft diffuse lighting and subtle soft shadows on the surface"
     * "Empty clean white tabletop surface seen from eye level with soft diffuse studio lighting and even illumination"

[JSON Output Format]

{
  "view_point": "",
  "subject_position": "",
  "subject_size": "",
  "lighting_condition": "",
  "color_tone": "",
  "shadow_presence": "",
  "background_simplicity": "",
  "is_flat_lay": true,
  "bg_layout_hint_en": ""
}

[Important Notes]

- The analysis must be purely technical and objective.
- Focus on what you can see, not what you infer about the product.
- bg_layout_hint_en is critical for background generation - be precise and descriptive.
- All fields are required - do not omit any field.

[Analysis Examples]

Example 1 - Top-down flat lay:
Input: Product image showing a top-down view of items on a white table.
Output:
{
  "view_point": "top_down",
  "subject_position": "center",
  "subject_size": "large",
  "lighting_condition": "soft diffuse studio lighting",
  "color_tone": "neutral",
  "shadow_presence": "soft",
  "background_simplicity": "simple",
  "is_flat_lay": true,
  "bg_layout_hint_en": "Top-down view of an empty clean white tabletop with soft diffuse studio lighting and subtle soft shadows"
}

Example 2 - Angled product shot:
Input: Product image showing an item at a 45-degree angle on a white surface.
Output:
{
  "view_point": "angled",
  "subject_position": "slightly_right",
  "subject_size": "large",
  "lighting_condition": "soft diffuse studio lighting with subtle top-left emphasis",
  "color_tone": "neutral",
  "shadow_presence": "grounded",
  "background_simplicity": "simple",
  "is_flat_lay": false,
  "bg_layout_hint_en": "Empty clean white studio surface seen from an angled perspective with soft diffuse lighting and subtle grounded shadows"
}

Example 3 - Front view product:
Input: Product image showing a front-facing view of an item on a surface.
Output:
{
  "view_point": "front_view",
  "subject_position": "center",
  "subject_size": "medium",
  "lighting_condition": "soft diffuse studio lighting",
  "color_tone": "neutral",
  "shadow_presence": "soft",
  "background_simplicity": "simple",
  "is_flat_lay": false,
  "bg_layout_hint_en": "Empty clean white tabletop surface seen from eye level with soft diffuse studio lighting and even illumination"
}

Example 4 - Side profile view:
Input: Product image showing a side profile of an item.
Output:
{
  "view_point": "side_view",
  "subject_position": "center",
  "subject_size": "large",
  "lighting_condition": "soft diffuse studio lighting",
  "color_tone": "neutral",
  "shadow_presence": "grounded",
  "background_simplicity": "simple",
  "is_flat_lay": false,
  "bg_layout_hint_en": "Empty clean white studio background seen from a side/profile perspective with a flat tabletop surface, soft diffuse lighting and subtle soft shadows"
}

[Edge Cases and Special Considerations]

1. Ambiguous camera angles:
   - If the angle is between 30-60 degrees, use "angled"
   - If it's closer to 90 degrees (looking down), use "top_down" and set is_flat_lay to true
   - If it's closer to 0 degrees (eye level), use "front_view"

2. Multiple products in frame:
   - Analyze the primary/most prominent product
   - Use its position and size for subject_position and subject_size

3. Mixed lighting:
   - Describe the dominant lighting condition
   - If multiple light sources are visible, describe the overall effect

4. Unusual backgrounds:
   - Even if the background is complex, describe it accurately
   - The bg_layout_hint_en should still describe an empty background matching the perspective

5. Shadow interpretation:
   - "grounded" shadows clearly connect the product to the surface
   - "floating" shadows create separation between product and surface
   - "soft" shadows have gradual edges and moderate darkness
   - "strong" shadows are dark with sharp, defined edges

[Quality Assurance Checklist]

Before outputting, verify:
- All 9 fields are filled with appropriate values
- view_point accurately matches the actual camera angle observed in the image
- is_flat_lay is true only for true top-down (85-90 degree) views, false for all other angles
- bg_layout_hint_en does NOT mention the product itself, product type, or any product-related elements
- bg_layout_hint_en includes perspective (matching view_point), surface type, and lighting information
- All text values are in English using standard photography terminology
- JSON format is valid (no trailing commas, proper quotes, correct boolean values)
- subject_position and subject_size accurately reflect the product's placement and size in the frame
- lighting_condition describes the actual lighting observed, not assumptions

[Common Mistakes to Avoid]

1. Do NOT set is_flat_lay to true for angled views (even if the angle is steep)
2. Do NOT include product names or product types in bg_layout_hint_en
3. Do NOT guess lighting conditions - base them on what you actually see
4. Do NOT use "close_up" for view_point unless the product truly fills most of the frame
5. Do NOT mix camera angle descriptions - choose the single most accurate view_point
6. Do NOT omit required fields - all 9 fields must be present in the output
7. Do NOT use markdown formatting - output pure JSON only
8. Do NOT add explanatory text outside the JSON object

[Technical Photography Guidelines]

Camera Angle Determination:
- Observe the horizon line and surface orientation to determine the exact angle
- Top-down (90 degrees): Surface appears as a flat plane with no visible depth
- Angled (30-60 degrees): Surface shows depth and perspective, both top and side visible
- Front view (0-15 degrees): Surface appears as a vertical or near-vertical plane
- Side view: Product's profile is the primary visible aspect

Lighting Analysis:
- Observe shadow direction to determine light source position
- Observe shadow softness to determine light quality (hard vs soft)
- Observe color temperature to determine warm vs cool vs neutral
- Observe shadow presence to determine if shadows exist and their characteristics

Subject Analysis:
- Measure subject position relative to frame center (use rule of thirds as reference)
- Estimate subject size as percentage of total frame area
- Consider both width and height when determining size category

Background Analysis:
- Assess complexity: simple backgrounds have minimal elements, complex backgrounds have multiple visible elements
- Note surface material and texture if visible
- Consider depth of field effects on background blur

Output the JSON object now."""


# === ë©”ì‹œì§€ ìƒì„± ===

def build_analysis_messages(
    image_path: str,
    use_cache_optimization: bool = False,
    max_width: int = None
) -> list[Dict[str, Any]]:
    """
    ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë°›ì•„ì„œ
    OpenAI vision APIìš© messages ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“ ë‹¤.
    
    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        use_cache_optimization: ìºì‹± ìµœì í™” ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False, í˜¸í™˜ì„± ìœ ì§€)
        max_width: ìµœëŒ€ ê°€ë¡œ (px). Noneì´ë©´ ë¦¬ì‚¬ì´ì§• ì•ˆ í•¨. ê°€ë¡œ ê¸°ì¤€ ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ (í¬ë¡­/íŒ¨ë”© ê¸ˆì§€).
    """
    # ì´ë¯¸ì§€ë¥¼ base64 data URLë¡œ ë³€í™˜
    data_url = image_to_base64_data_url(image_path, max_width=max_width)
    
    user_text = "Analyze this product thumbnail image and output the JSON object describing camera angle, layout, and lighting according to the rules."
    
    if use_cache_optimization:
        # ìºì‹± ìµœì í™” ëª¨ë“œ: Responses API í˜•ì‹
        user_content = [
            {
                "type": "input_text",
                "text": user_text
            },
            {
                "type": "input_image",
                "image_url": data_url  # /v1/responses APIì—ì„œëŠ” image_urlì´ ì§ì ‘ ë¬¸ìì—´ì´ì–´ì•¼ í•¨
            }
        ]
    else:
        # ì¼ë°˜ ëª¨ë“œ: Chat Completions API í˜•ì‹
        user_content = [
            {
                "type": "text",
                "text": user_text
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": data_url
                }
            }
        ]

    messages = [
        {"role": "system", "content": ANALYSIS_SYSTEM_PROMPT},
        {"role": "user", "content": user_content},
    ]
    return messages

def build_analysis_batch_payload(
    row_index: int,
    image_path: str,
    model_name: str,
    reasoning_effort: str,
    use_cache_optimization: bool = True,
    max_width: int = None,
    log_func=None
) -> Dict[str, Any]:
    """
    Batch APIìš© ìš”ì²­ payloadë¥¼ ìƒì„±í•œë‹¤.
    
    Args:
        row_index: í–‰ ì¸ë±ìŠ¤ (custom_id ìƒì„±ìš©)
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        model_name: ëª¨ë¸ëª…
        reasoning_effort: reasoning effort (gpt-5 ê³„ì—´ìš©)
        use_cache_optimization: ìºì‹± ìµœì í™” ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
        max_width: ìµœëŒ€ ê°€ë¡œ (px). Noneì´ë©´ ë¦¬ì‚¬ì´ì§• ì•ˆ í•¨. ê°€ë¡œ ê¸°ì¤€ ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ (í¬ë¡­/íŒ¨ë”© ê¸ˆì§€).
        log_func: ë¡œê·¸ ì¶œë ¥ í•¨ìˆ˜ (ì„ íƒì‚¬í•­)
    
    Returns:
        Batch APIìš© request ê°ì²´
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    
    # ì´ë¯¸ì§€ë¥¼ base64 data URLë¡œ ë³€í™˜
    data_url = image_to_base64_data_url(image_path, max_width=max_width, log_func=log_func)
    
    user_text = "Analyze this product thumbnail image and output the JSON object describing camera angle, layout, and lighting according to the rules."
    
    if use_cache_optimization:
        # System ë©”ì‹œì§€ (í…ìŠ¤íŠ¸ë§Œ, ì •ì )
        system_content = [{"type": "input_text", "text": ANALYSIS_SYSTEM_PROMPT}]
        
        # User ë©”ì‹œì§€ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€, ë™ì )
        user_content = [
            {
                "type": "input_text",
                "text": user_text
            },
            {
                "type": "input_image",
                "image_url": data_url  # /v1/responses APIì—ì„œëŠ” image_urlì´ ì§ì ‘ ë¬¸ìì—´ì´ì–´ì•¼ í•¨
            }
        ]
        
        # Body êµ¬ì„± (Responses API)
        body = {
            "model": model_name,
            "input": [
                {
                    "role": "system",
                    "content": system_content,
                },
                {
                    "role": "user",
                    "content": user_content,
                }
            ],
        }
        
        # reasoning.effort (Responses API)
        is_reasoning = any(x in model_name for x in ["gpt-5", "o1", "o3"])
        if is_reasoning and reasoning_effort != "none":
            body["reasoning"] = {"effort": reasoning_effort}
        
        # text.format: JSON ì¶œë ¥ ê°•ì œ (Structured Outputs)
        body["text"] = {"format": {"type": "json_object"}}
        
        url = "/v1/responses"
    else:
        # ì¼ë°˜ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ ìœ ì§€
        user_content = [
            {
                "type": "text",
                "text": user_text
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": data_url
                }
            }
        ]
        
        body = {
            "model": model_name,
            "messages": [
                {"role": "system", "content": ANALYSIS_SYSTEM_PROMPT},
                {"role": "user", "content": user_content},
            ],
        }
        
        is_reasoning = any(x in model_name for x in ["gpt-5", "o1", "o3"])
        if is_reasoning and reasoning_effort != "none":
            body["reasoning_effort"] = reasoning_effort
        
        url = "/v1/chat/completions"

    request_obj = {
        "custom_id": f"row_{row_index}",
        "method": "POST",
        "url": url,
        "body": body
    }
    return request_obj


# === OpenAI í˜¸ì¶œ ë˜í¼ ===

def call_image_analysis_api(
    image_path: str,
    model: str = DEFAULT_MODEL,
    api_key: Optional[str] = None,
    reasoning_effort: Optional[str] = None,
    use_cache_optimization: bool = False,
) -> tuple[Dict[str, Any], Any]:
    """
    ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„
    OpenAI Vision APIë¥¼ í˜¸ì¶œí•˜ê³ , íŒŒì‹±ëœ JSON(dict)ê³¼ response ê°ì²´ë¥¼ ë°˜í™˜í•œë‹¤.

    Returns:
        (result_dict, response): ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì™€ API ì‘ë‹µ ê°ì²´
    
    ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¨ë‹¤.
    
    Note: gpt-5 ê³„ì—´ ëª¨ë¸ì€ reasoning_effort íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    Note: use_cache_optimizationì´ Trueì´ë©´ Responses APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ìºì‹± ìµœì í™”).
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    
    client = get_openai_client(api_key=api_key)
    
    if use_cache_optimization:
        # ìºì‹± ìµœì í™” ëª¨ë“œ: Chat Completions API ì‚¬ìš© (Vision APIëŠ” ì´ë¯¸ì§€ í¬í•¨)
        # system/user ë¶„ë¦¬ëœ ë©”ì‹œì§€ êµ¬ì¡° ì‚¬ìš©
        messages = build_analysis_messages(image_path, use_cache_optimization=False)
        
        # API íŒŒë¼ë¯¸í„° êµ¬ì„±
        params = {
            "model": model,
            "messages": messages,
        }
        
        # gpt-5 ê³„ì—´ì€ reasoning_effort ì‚¬ìš©
        if reasoning_effort and reasoning_effort != "none":
            params["reasoning_effort"] = reasoning_effort
        
        # prompt_cache_key ë° prompt_cache_retention (ì´ë¯¸ì§€ê°€ ë‹¤ë¥´ë¯€ë¡œ íš¨ê³¼ëŠ” ì œí•œì )
        # í•˜ì§€ë§Œ system í”„ë¡¬í”„íŠ¸ëŠ” ë™ì¼í•˜ë¯€ë¡œ ì¼ë¶€ ìºì‹± íš¨ê³¼ëŠ” ìˆìŒ
        params["prompt_cache_key"] = "img_analysis_v1"
        # prompt_cache_retention: ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
        # Extended retention ì§€ì› ëª¨ë¸: gpt-5.1, gpt-5.1-codex, gpt-5.1-codex-mini, gpt-5.1-chat-latest, gpt-5, gpt-5-codex, gpt-4.1
        # gpt-5-mini, gpt-5-nanoëŠ” prompt_cache_retention íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
        if model in ["gpt-5.1", "gpt-5.1-codex", "gpt-5.1-codex-mini", "gpt-5.1-chat-latest", "gpt-5", "gpt-5-codex", "gpt-4.1"]:
            params["prompt_cache_retention"] = "extended"  # 24ì‹œê°„ retention
        elif model not in ["gpt-5-mini", "gpt-5-nano"]:
            # ê¸°íƒ€ ëª¨ë¸ì€ in-memory ì‚¬ìš© (5~10ë¶„ inactivity, ìµœëŒ€ 1ì‹œê°„)
            params["prompt_cache_retention"] = "in_memory"
        
        # text.format: JSON ì¶œë ¥ ê°•ì œ (Structured Outputs)
        params["text"] = {"format": {"type": "json_object"}}
        
        response = client.chat.completions.create(**params)
    else:
        # ì¼ë°˜ ëª¨ë“œ: Chat Completions API ì‚¬ìš©
        messages = build_analysis_messages(image_path, use_cache_optimization=False)

        # API íŒŒë¼ë¯¸í„° êµ¬ì„±
        params = {
            "model": model,
            "messages": messages,
        }
        
        # gpt-5 ê³„ì—´ì€ reasoning_effort ì‚¬ìš©
        if reasoning_effort and reasoning_effort != "none":
            params["reasoning_effort"] = reasoning_effort

        response = client.chat.completions.create(**params)

    content = response.choices[0].message.content
    # contentëŠ” ìˆœìˆ˜ JSON ë¬¸ìì—´ì´ì–´ì•¼ í•œë‹¤.
    try:
        data = json.loads(content)
    except json.JSONDecodeError as e:
        raise ValueError(f"ëª¨ë¸ ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}\n\ncontent=\n{content}")

    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
    required_fields = [
        "view_point", "subject_position", "subject_size", "lighting_condition",
        "color_tone", "shadow_presence", "background_simplicity", "is_flat_lay",
        "bg_layout_hint_en"
    ]
    for key in required_fields:
        if key not in data:
            raise ValueError(f"ì‘ë‹µ JSONì— í•„ë“œ '{key}' ê°€ ì—†ìŠµë‹ˆë‹¤. content=\n{content}")

    return data, response


# === í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ ===

if __name__ == "__main__":
    # ê°„ë‹¨í•œ ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
    test_image_path = "test_thumbnail.png"  # í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ê²½ë¡œ
    
    if os.path.exists(test_image_path):
        try:
            result, response = call_image_analysis_api(test_image_path)
            print(json.dumps(result, ensure_ascii=False, indent=2))
            print(f"\n[Usage] {response.usage}")
        except Exception as e:
            print("[ERROR]", e)
    else:
        print(f"[INFO] í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {test_image_path}")

